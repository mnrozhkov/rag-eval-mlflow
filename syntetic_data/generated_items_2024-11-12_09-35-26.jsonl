{"document": {"content": "As machine learning systems become ubiquitous, there has been a surge of\ninterest in interpretable machine learning: systems that provide explanation\nfor their outputs. These explanations are often used to qualitatively assess\nother criteria such as safety or non-discrimination. However, despite the\ninterest in interpretability, there is very little consensus on what\ninterpretable machine learning is and how it should be measured. In this\nposition paper, we first define interpretability and describe when\ninterpretability is needed (and when it is not). Next, we suggest a taxonomy\nfor rigorous evaluation and expose open questions towards a more rigorous\nscience of interpretable machine learning.", "collection_id": "bc278bf69a69a946a49b92d0b3173439115bf84c912768682bc5c57172920219"}, "question": "What is the main goal of interpretable machine learning systems?\n", "answer": "The main goal of interpretable machine learning systems is to provide explanations for their outputs, which can be used to qualitatively assess other criteria such as safety or non-discrimination."}
{"document": {"content": "We consider the problem of adapting neural paragraph-level question answering\nmodels to the case where entire documents are given as input. Our proposed\nsolution trains models to produce well calibrated confidence scores for their\nresults on individual paragraphs. We sample multiple paragraphs from the\ndocuments during training, and use a shared-normalization training objective\nthat encourages the model to produce globally correct output. We combine this\nmethod with a state-of-the-art pipeline for training models on document QA\ndata. Experiments demonstrate strong performance on several document QA\ndatasets. Overall, we are able to achieve a score of 71.3 F1 on the web portion\nof TriviaQA, a large improvement from the 56.7 F1 of the previous best system.", "collection_id": "b012960d3f23d066f6842d7f7680cbec5efef05f654fd96013737c9029eb8afc"}, "question": "What F1 score was achieved on the web portion of TriviaQA using the proposed solution for adapting neural paragraph-level question answering models?\n\n", "answer": "The proposed solution achieved a score of 71.3 F1 on the web portion of TriviaQA, significantly improving upon the previous best system's score of 56.7 F1."}
{"document": {"content": "Transformers are powerful sequence models, but require time and memory that\ngrows quadratically with the sequence length. In this paper we introduce sparse\nfactorizations of the attention matrix which reduce this to $O(n \\sqrt{n})$. We\nalso introduce a) a variation on architecture and initialization to train\ndeeper networks, b) the recomputation of attention matrices to save memory, and\nc) fast attention kernels for training. We call networks with these changes\nSparse Transformers, and show they can model sequences tens of thousands of\ntimesteps long using hundreds of layers. We use the same architecture to model\nimages, audio, and text from raw bytes, setting a new state of the art for\ndensity modeling of Enwik8, CIFAR-10, and ImageNet-64. We generate\nunconditional samples that demonstrate global coherence and great diversity,\nand show it is possible in principle to use self-attention to model sequences\nof length one million or more.", "collection_id": "37f83ff3eb1da6383c8e2d0d3d3c0be0f7b8ad4be46c7bd92e35788055c84888"}, "question": "What is the time and memory complexity reduction achieved by sparse factorizations of the attention matrix in Transformers?\n", "answer": "The sparse factorizations of the attention matrix reduce the time and memory complexity from quadratic growth with the sequence length to $O(n \\sqrt{n})$."}
{"document": {"content": "When answering a question, people often draw upon their rich world knowledge\nin addition to the particular context. Recent work has focused primarily on\nanswering questions given some relevant document or context, and required very\nlittle general background. To investigate question answering with prior\nknowledge, we present CommonsenseQA: a challenging new dataset for commonsense\nquestion answering. To capture common sense beyond associations, we extract\nfrom ConceptNet (Speer et al., 2017) multiple target concepts that have the\nsame semantic relation to a single source concept. Crowd-workers are asked to\nauthor multiple-choice questions that mention the source concept and\ndiscriminate in turn between each of the target concepts. This encourages\nworkers to create questions with complex semantics that often require prior\nknowledge. We create 12,247 questions through this procedure and demonstrate\nthe difficulty of our task with a large number of strong baselines. Our best\nbaseline is based on BERT-large (Devlin et al., 2018) and obtains 56% accuracy,\nwell below human performance, which is 89%.", "collection_id": "b0973987d59d2134580374204f977b0fdc217e09d9f2aa63aa17f9353ba45651"}, "question": "What is the human performance accuracy on the CommonsenseQA dataset?\n", "answer": "The human performance accuracy on the CommonsenseQA dataset is 89%."}
{"document": {"content": "The intriguing phenomenon of adversarial examples has attracted significant\nattention in machine learning and what might be more surprising to the\ncommunity is the existence of universal adversarial perturbations (UAPs), i.e.\na single perturbation to fool the target DNN for most images. With the focus on\nUAP against deep classifiers, this survey summarizes the recent progress on\nuniversal adversarial attacks, discussing the challenges from both the attack\nand defense sides, as well as the reason for the existence of UAP. We aim to\nextend this work as a dynamic survey that will regularly update its content to\nfollow new works regarding UAP or universal attack in a wide range of domains,\nsuch as image, audio, video, text, etc. Relevant updates will be discussed at:\nhttps://bit.ly/2SbQlLG. We welcome authors of future works in this field to\ncontact us for including your new finding.", "collection_id": "61d67d3ec066bae0d2e5051c9898a64fded7bdee877fef238845fad3c17c6a7a"}, "question": "What is a universal adversarial perturbation (UAP) in machine learning?\n", "answer": "A universal adversarial perturbation (UAP) is a single perturbation that can fool a target deep neural network (DNN) for most images, meaning it is a single alteration that can cause the DNN to misclassify a wide range of images."}
{"document": {"content": "In the future, powerful AI systems may be deployed in high-stakes settings,\nwhere a single failure could be catastrophic. One technique for improving AI\nsafety in high-stakes settings is adversarial training, which uses an adversary\nto generate examples to train on in order to achieve better worst-case\nperformance.\n  In this work, we used a safe language generation task (``avoid injuries'') as\na testbed for achieving high reliability through adversarial training. We\ncreated a series of adversarial training techniques -- including a tool that\nassists human adversaries -- to find and eliminate failures in a classifier\nthat filters text completions suggested by a generator. In our task, we\ndetermined that we can set very conservative classifier thresholds without\nsignificantly impacting the quality of the filtered outputs. We found that\nadversarial training increased robustness to the adversarial attacks that we\ntrained on -- doubling the time for our contractors to find adversarial\nexamples both with our tool (from 13 to 26 minutes) and without (from 20 to 44\nminutes) -- without affecting in-distribution performance.\n  We hope to see further work in the high-stakes reliability setting, including\nmore powerful tools for enhancing human adversaries and better ways to measure\nhigh levels of reliability, until we can confidently rule out the possibility\nof catastrophic deployment-time failures of powerful models.", "collection_id": "aec5c90a7e3898453f1ca8a3c526b8567c7fa8c25fa4b902dd4f7870a3e5a4e1"}, "question": "What technique is used to improve AI safety in high-stakes settings by generating examples to train on for better worst-case performance?\n\n", "answer": "The technique used to improve AI safety in high-stakes settings is called adversarial training. It uses an adversary to generate examples to train on in order to achieve better worst-case performance. This method helps to find and eliminate failures in AI systems, making them more reliable and robust in critical situations."}
{"document": {"content": "Chat models, such as ChatGPT, have shown impressive capabilities and have\nbeen rapidly adopted across numerous domains. However, these models are only\naccessible through a restricted API, creating barriers for new research and\nprogress in the field. We propose a pipeline that can automatically generate a\nhigh-quality multi-turn chat corpus by leveraging ChatGPT to engage in a\nconversation with itself. Subsequently, we employ parameter-efficient tuning to\nenhance LLaMA, an open-source large language model. The resulting model, named\nBaize, demonstrates good performance in multi-turn dialogues with guardrails\nthat minimize potential risks. Furthermore, we propose a new technique called\nSelf-Distill with Feedback, to further improve the performance of the Baize\nmodels with feedback from ChatGPT. The Baize models and data are released for\nresearch purposes only at https://github.com/project-baize/baize-chatbot. An\nonline demo is also available at\nhttps://huggingface.co/spaces/project-baize/chat-with-baize.", "collection_id": "bea5ce52b3e9452ab278f3ad7ce2df8243ab6c02772a98e7600272e4004f5c3c"}, "question": "What is the name of the model developed by enhancing LLaMA through parameter-efficient tuning?\n", "answer": "The model developed by enhancing LLaMA through parameter-efficient tuning is named Baize. It demonstrates good performance in multi-turn dialogues with guardrails that minimize potential risks."}
{"document": {"content": "We present a framework for using transformer networks as universal computers\nby programming them with specific weights and placing them in a loop. Our input\nsequence acts as a punchcard, consisting of instructions and memory for data\nread/writes. We demonstrate that a constant number of encoder layers can\nemulate basic computing blocks, including embedding edit operations, non-linear\nfunctions, function calls, program counters, and conditional branches. Using\nthese building blocks, we emulate a small instruction-set computer. This allows\nus to map iterative algorithms to programs that can be executed by a looped,\n13-layer transformer. We show how this transformer, instructed by its input,\ncan emulate a basic calculator, a basic linear algebra library, and in-context\nlearning algorithms that employ backpropagation. Our work highlights the\nversatility of the attention mechanism, and demonstrates that even shallow\ntransformers can execute full-fledged, general-purpose programs.", "collection_id": "e672ff9d3b4e94f4c62f1976dc8f3a1ec79caed60a4554c9b520b5373701f35c"}, "question": "How many encoder layers are required to emulate basic computing blocks in a transformer network?\n", "answer": "A constant number of encoder layers are required to emulate basic computing blocks in a transformer network. Specifically, the researchers were able to demonstrate this capability using a 13-layer transformer."}
{"document": {"content": "Pretrained neural language models (LMs) are prone to generating racist,\nsexist, or otherwise toxic language which hinders their safe deployment. We\ninvestigate the extent to which pretrained LMs can be prompted to generate\ntoxic language, and the effectiveness of controllable text generation\nalgorithms at preventing such toxic degeneration. We create and release\nRealToxicityPrompts, a dataset of 100K naturally occurring, sentence-level\nprompts derived from a large corpus of English web text, paired with toxicity\nscores from a widely-used toxicity classifier. Using RealToxicityPrompts, we\nfind that pretrained LMs can degenerate into toxic text even from seemingly\ninnocuous prompts. We empirically assess several controllable generation\nmethods, and find that while data- or compute-intensive methods (e.g., adaptive\npretraining on non-toxic data) are more effective at steering away from\ntoxicity than simpler solutions (e.g., banning \"bad\" words), no current method\nis failsafe against neural toxic degeneration. To pinpoint the potential cause\nof such persistent toxic degeneration, we analyze two web text corpora used to\npretrain several LMs (including GPT-2; Radford et. al, 2019), and find a\nsignificant amount of offensive, factually unreliable, and otherwise toxic\ncontent. Our work provides a test bed for evaluating toxic generations by LMs\nand stresses the need for better data selection processes for pretraining.", "collection_id": "cdaffec29b8a63152e71e03e836a850564728507647a6759495bde393b9c5c0b"}, "question": "What is the name of the dataset created to assess the effectiveness of controllable text generation algorithms at preventing toxic language generation?\n\n", "answer": "The dataset is called RealToxicityPrompts, which consists of 100K naturally occurring, sentence-level prompts derived from a large corpus of English web text, paired with toxicity scores from a widely-used toxicity classifier."}
{"document": {"content": "This paper presents a new Unified pre-trained Language Model (UniLM) that can\nbe fine-tuned for both natural language understanding and generation tasks. The\nmodel is pre-trained using three types of language modeling tasks:\nunidirectional, bidirectional, and sequence-to-sequence prediction. The unified\nmodeling is achieved by employing a shared Transformer network and utilizing\nspecific self-attention masks to control what context the prediction conditions\non. UniLM compares favorably with BERT on the GLUE benchmark, and the SQuAD 2.0\nand CoQA question answering tasks. Moreover, UniLM achieves new\nstate-of-the-art results on five natural language generation datasets,\nincluding improving the CNN/DailyMail abstractive summarization ROUGE-L to\n40.51 (2.04 absolute improvement), the Gigaword abstractive summarization\nROUGE-L to 35.75 (0.86 absolute improvement), the CoQA generative question\nanswering F1 score to 82.5 (37.1 absolute improvement), the SQuAD question\ngeneration BLEU-4 to 22.12 (3.75 absolute improvement), and the DSTC7\ndocument-grounded dialog response generation NIST-4 to 2.67 (human performance\nis 2.65). The code and pre-trained models are available at\nhttps://github.com/microsoft/unilm.", "collection_id": "de1f0e028af0e29aac157783803f2d1c7a8e108622ef665c3a7348f503db5fa5"}, "question": "What is the performance of UniLM on the CNN/DailyMail abstractive summarization ROUGE-L?\n", "answer": "UniLM achieves a ROUGE-L score of 40.51 on the CNN/DailyMail abstractive summarization task, which is a 2.04 absolute improvement over the previous state-of-the-art result."}
{"document": {"content": "Despite their overwhelming capacity to overfit, deep learning architectures\ntend to generalize relatively well to unseen data, allowing them to be deployed\nin practice. However, explaining why this is the case is still an open area of\nresearch. One standing hypothesis that is gaining popularity, e.g. Hochreiter &\nSchmidhuber (1997); Keskar et al. (2017), is that the flatness of minima of the\nloss function found by stochastic gradient based methods results in good\ngeneralization. This paper argues that most notions of flatness are problematic\nfor deep models and can not be directly applied to explain generalization.\nSpecifically, when focusing on deep networks with rectifier units, we can\nexploit the particular geometry of parameter space induced by the inherent\nsymmetries that these architectures exhibit to build equivalent models\ncorresponding to arbitrarily sharper minima. Furthermore, if we allow to\nreparametrize a function, the geometry of its parameters can change drastically\nwithout affecting its generalization properties.", "collection_id": "c31131fd5f1cb9fac4ba2807f32c58f01bbcd7ff843a2b949a9cd614df934f44"}, "question": "Why do deep learning architectures tend to generalize well to unseen data?\n", "answer": "Deep learning architectures tend to generalize well to unseen data, but the reason for this is still an open area of research. One popular hypothesis is that the flatness of minima of the loss function found by stochastic gradient-based methods results in good generalization. However, this notion of flatness is problematic for deep models and may not be directly applicable to explain generalization."}
{"document": {"content": "Language technologies contribute to promoting multilingualism and linguistic\ndiversity around the world. However, only a very small number of the over 7000\nlanguages of the world are represented in the rapidly evolving language\ntechnologies and applications. In this paper we look at the relation between\nthe types of languages, resources, and their representation in NLP conferences\nto understand the trajectory that different languages have followed over time.\nOur quantitative investigation underlines the disparity between languages,\nespecially in terms of their resources, and calls into question the \"language\nagnostic\" status of current models and systems. Through this paper, we attempt\nto convince the ACL community to prioritise the resolution of the predicaments\nhighlighted here, so that no language is left behind.", "collection_id": "5962b71cf5b4c3851218eb66b9c7209259498355b6725e14c6eef940dcd71d1c"}, "question": "How many languages are there in the world?\n", "answer": "There are over 7000 languages in the world, but only a small number of them are represented in language technologies and applications."}
{"document": {"content": "We propose Machines Talking To Machines (M2M), a framework combining\nautomation and crowdsourcing to rapidly bootstrap end-to-end dialogue agents\nfor goal-oriented dialogues in arbitrary domains. M2M scales to new tasks with\njust a task schema and an API client from the dialogue system developer, but it\nis also customizable to cater to task-specific interactions. Compared to the\nWizard-of-Oz approach for data collection, M2M achieves greater diversity and\ncoverage of salient dialogue flows while maintaining the naturalness of\nindividual utterances. In the first phase, a simulated user bot and a\ndomain-agnostic system bot converse to exhaustively generate dialogue\n\"outlines\", i.e. sequences of template utterances and their semantic parses. In\nthe second phase, crowd workers provide contextual rewrites of the dialogues to\nmake the utterances more natural while preserving their meaning. The entire\nprocess can finish within a few hours. We propose a new corpus of 3,000\ndialogues spanning 2 domains collected with M2M, and present comparisons with\npopular dialogue datasets on the quality and diversity of the surface forms and\ndialogue flows.", "collection_id": "516f27dd80bdeab42e02a519a9c8ad6276c910abeadccfce88037faf99a7050f"}, "question": "How long does the Machines Talking To Machines (M2M) process take to finish?\n", "answer": "The entire M2M process, from generating dialogue outlines to making the utterances more natural through crowd workers' contextual rewrites, can finish within a few hours."}
{"document": {"content": "We present the results and main findings of SemEval-2020 Task 12 on\nMultilingual Offensive Language Identification in Social Media (OffensEval\n2020). The task involves three subtasks corresponding to the hierarchical\ntaxonomy of the OLID schema (Zampieri et al., 2019a) from OffensEval 2019. The\ntask featured five languages: English, Arabic, Danish, Greek, and Turkish for\nSubtask A. In addition, English also featured Subtasks B and C. OffensEval 2020\nwas one of the most popular tasks at SemEval-2020 attracting a large number of\nparticipants across all subtasks and also across all languages. A total of 528\nteams signed up to participate in the task, 145 teams submitted systems during\nthe evaluation period, and 70 submitted system description papers.", "collection_id": "5fd2a26318e318ee64ecdff2361306611f0678346b46cad6709945b21f7a6616"}, "question": "How many teams submitted system description papers in OffensEval 2020?\n", "answer": "In OffensEval 2020, a total of 70 teams submitted system description papers."}
{"document": {"content": "All AI models are susceptible to learning biases in data that they are\ntrained on. For generative dialogue models, being trained on real human\nconversations containing unbalanced gender and race/ethnicity references can\nlead to models that display learned biases, which we define here broadly as any\nmeasurable differences in the distributions of words or semantic content of\nconversations based on demographic groups. We measure the strength of such\nbiases by producing artificial conversations between two copies of a dialogue\nmodel, conditioning one conversational partner to state a name commonly\nassociated with a certain gender and/or race/ethnicity. We find that larger\ncapacity models tend to exhibit more gender bias and greater stereotyping of\noccupations by gender. We show that several methods of tuning these dialogue\nmodels, specifically name scrambling, controlled generation, and unlikelihood\ntraining, are effective in reducing bias in conversation, including on a\ndownstream conversational task. Name scrambling is also effective in lowering\ndifferences in token usage across conversations where partners have names\nassociated with different genders or races/ethnicities.", "collection_id": "d9b048423a266f79705bceefaf7c436243f57d495086b6ccc2fd03b70132e0c0"}, "question": "What methods are effective in reducing bias in conversation for dialogue models?\n\n", "answer": "Several methods have been found to be effective in reducing bias in conversation for dialogue models, including name scrambling, controlled generation, and unlikelihood training. Name scrambling is also effective in lowering differences in token usage across conversations where partners have names associated with different genders or races/ethnicities."}
{"document": {"content": "This paper presents the first consumer-scale next-word prediction (NWP) model\ntrained with Federated Learning (FL) while leveraging the Differentially\nPrivate Federated Averaging (DP-FedAvg) technique. There has been prior work on\nbuilding practical FL infrastructure, including work demonstrating the\nfeasibility of training language models on mobile devices using such\ninfrastructure. It has also been shown (in simulations on a public corpus) that\nit is possible to train NWP models with user-level differential privacy using\nthe DP-FedAvg algorithm. Nevertheless, training production-quality NWP models\nwith DP-FedAvg in a real-world production environment on a heterogeneous fleet\nof mobile phones requires addressing numerous challenges. For instance, the\ncoordinating central server has to keep track of the devices available at the\nstart of each round and sample devices uniformly at random from them, while\nensuring \\emph{secrecy of the sample}, etc. Unlike all prior privacy-focused FL\nwork of which we are aware, for the first time we demonstrate the deployment of\na differentially private mechanism for the training of a production neural\nnetwork in FL, as well as the instrumentation of the production training\ninfrastructure to perform an end-to-end empirical measurement of unintended\nmemorization.", "collection_id": "2cb926b03f70b1076b20f7f961e94a7d32ec0568883c4fe79dfb93ce6d3492bf"}, "question": "What technique is used to train the first consumer-scale next-word prediction model with Federated Learning while maintaining user-level differential privacy?\n\n", "answer": "The Differentially Private Federated Averaging (DP-FedAvg) technique is used to train the first consumer-scale next-word prediction model with Federated Learning while maintaining user-level differential privacy. This technique allows for the training of production-quality next-word prediction models with user-level differential privacy, addressing the challenges of training on a heterogeneous fleet of mobile phones in a real-world production environment."}
{"document": {"content": "Humans and animals have the ability to continually acquire, fine-tune, and\ntransfer knowledge and skills throughout their lifespan. This ability, referred\nto as lifelong learning, is mediated by a rich set of neurocognitive mechanisms\nthat together contribute to the development and specialization of our\nsensorimotor skills as well as to long-term memory consolidation and retrieval.\nConsequently, lifelong learning capabilities are crucial for autonomous agents\ninteracting in the real world and processing continuous streams of information.\nHowever, lifelong learning remains a long-standing challenge for machine\nlearning and neural network models since the continual acquisition of\nincrementally available information from non-stationary data distributions\ngenerally leads to catastrophic forgetting or interference. This limitation\nrepresents a major drawback for state-of-the-art deep neural network models\nthat typically learn representations from stationary batches of training data,\nthus without accounting for situations in which information becomes\nincrementally available over time. In this review, we critically summarize the\nmain challenges linked to lifelong learning for artificial learning systems and\ncompare existing neural network approaches that alleviate, to different\nextents, catastrophic forgetting. We discuss well-established and emerging\nresearch motivated by lifelong learning factors in biological systems such as\nstructural plasticity, memory replay, curriculum and transfer learning,\nintrinsic motivation, and multisensory integration.", "collection_id": "f62d80ad7e46c89b2be9f8e1e652ece5fc1458356a5919a0d404c750d80c1d2d"}, "question": "What is lifelong learning in humans and animals?\n", "answer": "Lifelong learning is the ability to continually acquire, fine-tune, and transfer knowledge and skills throughout one's lifespan, mediated by a rich set of neurocognitive mechanisms that contribute to the development and specialization of sensorimotor skills, as well as long-term memory consolidation and retrieval."}
{"document": {"content": "Some users of social media are spreading racist, sexist, and otherwise\nhateful content. For the purpose of training a hate speech detection system,\nthe reliability of the annotations is crucial, but there is no universally\nagreed-upon definition. We collected potentially hateful messages and asked two\ngroups of internet users to determine whether they were hate speech or not,\nwhether they should be banned or not and to rate their degree of offensiveness.\nOne of the groups was shown a definition prior to completing the survey. We\naimed to assess whether hate speech can be annotated reliably, and the extent\nto which existing definitions are in accordance with subjective ratings. Our\nresults indicate that showing users a definition caused them to partially align\ntheir own opinion with the definition but did not improve reliability, which\nwas very low overall. We conclude that the presence of hate speech should\nperhaps not be considered a binary yes-or-no decision, and raters need more\ndetailed instructions for the annotation.", "collection_id": "e4da3ac0a5d1e6bc4a041dfa2446eb4e8300164ee78e2b5659691410af45ad21"}, "question": "Does having a definition of hate speech improve the reliability of annotations in hate speech detection systems?\n\n", "answer": "Having a definition of hate speech does not improve the reliability of annotations in hate speech detection systems. In a study where two groups of internet users were asked to annotate potentially hateful messages, one group was shown a definition prior to completing the survey. The results showed that while the definition caused users to partially align their opinions with it, the overall reliability of the annotations remained very low. This suggests that simply providing a definition is not enough to ensure reliable annotations, and more detailed instructions may be necessary for accurate hate speech detection."}
{"document": {"content": "Generating step-by-step \"chain-of-thought\" rationales improves language model\nperformance on complex reasoning tasks like mathematics or commonsense\nquestion-answering. However, inducing language model rationale generation\ncurrently requires either constructing massive rationale datasets or\nsacrificing accuracy by using only few-shot inference. We propose a technique\nto iteratively leverage a small number of rationale examples and a large\ndataset without rationales, to bootstrap the ability to perform successively\nmore complex reasoning. This technique, the \"Self-Taught Reasoner\" (STaR),\nrelies on a simple loop: generate rationales to answer many questions, prompted\nwith a few rationale examples; if the generated answers are wrong, try again to\ngenerate a rationale given the correct answer; fine-tune on all the rationales\nthat ultimately yielded correct answers; repeat. We show that STaR\nsignificantly improves performance on multiple datasets compared to a model\nfine-tuned to directly predict final answers, and performs comparably to\nfine-tuning a 30$\\times$ larger state-of-the-art language model on\nCommensenseQA. Thus, STaR lets a model improve itself by learning from its own\ngenerated reasoning.", "collection_id": "611b6e246f34026a77fcd1f72ab57f96eae420eb9b90b7b294231ac6dbe3c874"}, "question": "What is the \"Self-Taught Reasoner\" (STaR) technique used for in language models?\n", "answer": "The \"Self-Taught Reasoner\" (STaR) technique is used to iteratively leverage a small number of rationale examples and a large dataset without rationales to bootstrap the ability to perform successively more complex reasoning in language models, improving their performance on complex reasoning tasks like mathematics or commonsense question-answering."}
{"document": {"content": "Textual content is often the output of a collaborative writing process: We\nstart with an initial draft, ask for suggestions, and repeatedly make changes.\nAgnostic of this process, today's language models are trained to generate only\nthe final result. As a consequence, they lack several abilities crucial for\ncollaborative writing: They are unable to update existing texts, difficult to\ncontrol and incapable of verbally planning or explaining their actions. To\naddress these shortcomings, we introduce PEER, a collaborative language model\nthat is trained to imitate the entire writing process itself: PEER can write\ndrafts, add suggestions, propose edits and provide explanations for its\nactions. Crucially, we train multiple instances of PEER able to infill various\nparts of the writing process, enabling the use of self-training techniques for\nincreasing the quality, amount and diversity of training data. This unlocks\nPEER's full potential by making it applicable in domains for which no edit\nhistories are available and improving its ability to follow instructions, to\nwrite useful comments, and to explain its actions. We show that PEER achieves\nstrong performance across various domains and editing tasks.", "collection_id": "3cf42a1dd420aa7133f30e198c5aea480da6dcf466730671c3434b9f18e6e3ed"}, "question": "What is PEER in the context of language models?\n", "answer": "PEER is a collaborative language model designed to imitate the entire writing process, enabling it to write drafts, add suggestions, propose edits, and provide explanations for its actions."}
{"document": {"content": "Face detection serves as a fundamental research topic for many applications\nlike face recognition. Impressive progress has been made especially with the\nrecent development of convolutional neural networks. However, the issue of\nlarge scale variations, which widely exists in high resolution images/videos,\nhas not been well addressed in the literature. In this paper, we present a\nnovel algorithm called SFace, which efficiently integrates the anchor-based\nmethod and anchor-free method to address the scale issues. A new dataset called\n4K-Face is also introduced to evaluate the performance of face detection with\nextreme large scale variations. The SFace architecture shows promising results\non the new 4K-Face benchmarks. In addition, our method can run at 50 frames per\nsecond (fps) with an accuracy of 80% AP on the standard WIDER FACE dataset,\nwhich outperforms the state-of-art algorithms by almost one order of magnitude\nin speed while achieves comparative performance.", "collection_id": "9e6bb9b3c185297ee3477fda3097195d87779d0d7da36875c6fa5dd760ee464c"}, "question": "What is the speed and accuracy of the SFace algorithm on the WIDER FACE dataset?\n\n", "answer": "The SFace algorithm can run at 50 frames per second (fps) with an accuracy of 80% AP on the standard WIDER FACE dataset. This performance outperforms the state-of-art algorithms by almost one order of magnitude in speed while achieving comparative performance."}
{"document": {"content": "We present TriviaQA, a challenging reading comprehension dataset containing\nover 650K question-answer-evidence triples. TriviaQA includes 95K\nquestion-answer pairs authored by trivia enthusiasts and independently gathered\nevidence documents, six per question on average, that provide high quality\ndistant supervision for answering the questions. We show that, in comparison to\nother recently introduced large-scale datasets, TriviaQA (1) has relatively\ncomplex, compositional questions, (2) has considerable syntactic and lexical\nvariability between questions and corresponding answer-evidence sentences, and\n(3) requires more cross sentence reasoning to find answers. We also present two\nbaseline algorithms: a feature-based classifier and a state-of-the-art neural\nnetwork, that performs well on SQuAD reading comprehension. Neither approach\ncomes close to human performance (23% and 40% vs. 80%), suggesting that\nTriviaQA is a challenging testbed that is worth significant future study. Data\nand code available at -- http://nlp.cs.washington.edu/triviaqa/", "collection_id": "63a5c120269061a78c769c0f35e38e7a63a7d86d221e98310778b0ba2ca8edee"}, "question": "What is the approximate number of question-answer-evidence triples in the TriviaQA dataset?\n", "answer": "The TriviaQA dataset contains over 650,000 question-answer-evidence triples."}
{"document": {"content": "Warning: this paper contains content that may be offensive or upsetting.\n  Language has the power to reinforce stereotypes and project social biases\nonto others. At the core of the challenge is that it is rarely what is stated\nexplicitly, but rather the implied meanings, that frame people's judgments\nabout others. For example, given a statement that \"we shouldn't lower our\nstandards to hire more women,\" most listeners will infer the implicature\nintended by the speaker -- that \"women (candidates) are less qualified.\" Most\nsemantic formalisms, to date, do not capture such pragmatic implications in\nwhich people express social biases and power differentials in language.\n  We introduce Social Bias Frames, a new conceptual formalism that aims to\nmodel the pragmatic frames in which people project social biases and\nstereotypes onto others. In addition, we introduce the Social Bias Inference\nCorpus to support large-scale modelling and evaluation with 150k structured\nannotations of social media posts, covering over 34k implications about a\nthousand demographic groups.\n  We then establish baseline approaches that learn to recover Social Bias\nFrames from unstructured text. We find that while state-of-the-art neural\nmodels are effective at high-level categorization of whether a given statement\nprojects unwanted social bias (80% F1), they are not effective at spelling out\nmore detailed explanations in terms of Social Bias Frames. Our study motivates\nfuture work that combines structured pragmatic inference with commonsense\nreasoning on social implications.", "collection_id": "7bd87597dbc4a1140119dc57cf7ca621b0cce83f1f03d657e644c68684902897"}, "question": "What is the main goal of Social Bias Frames in natural language processing?\n", "answer": "The main goal of Social Bias Frames is to model the pragmatic frames in which people project social biases and stereotypes onto others through language, capturing implied meanings that can reinforce stereotypes and project social biases."}
{"document": {"content": "The perceived toxicity of language can vary based on someone's identity and\nbeliefs, but this variation is often ignored when collecting toxic language\ndatasets, resulting in dataset and model biases. We seek to understand the who,\nwhy, and what behind biases in toxicity annotations. In two online studies with\ndemographically and politically diverse participants, we investigate the effect\nof annotator identities (who) and beliefs (why), drawing from social psychology\nresearch about hate speech, free speech, racist beliefs, political leaning, and\nmore. We disentangle what is annotated as toxic by considering posts with three\ncharacteristics: anti-Black language, African American English (AAE) dialect,\nand vulgarity. Our results show strong associations between annotator identity\nand beliefs and their ratings of toxicity. Notably, more conservative\nannotators and those who scored highly on our scale for racist beliefs were\nless likely to rate anti-Black language as toxic, but more likely to rate AAE\nas toxic. We additionally present a case study illustrating how a popular\ntoxicity detection system's ratings inherently reflect only specific beliefs\nand perspectives. Our findings call for contextualizing toxicity labels in\nsocial variables, which raises immense implications for toxic language\nannotation and detection.", "collection_id": "9596620b6962dd8aee8f34781b2c1a0653872cdb31aa9658ce2f9e1776559708"}, "question": "What are the effects of annotator identities and beliefs on ratings of toxicity in language datasets?\n\n", "answer": "Research has shown that there are strong associations between annotator identity and beliefs and their ratings of toxicity. Specifically, more conservative annotators and those who scored highly on a scale for racist beliefs were less likely to rate anti-Black language as toxic, but more likely to rate African American English (AAE) dialect as toxic. This suggests that the perceived toxicity of language can vary significantly based on an individual's identity and beliefs, and highlights the need to consider these factors when collecting and annotating toxic language datasets."}
{"document": {"content": "Mixture of Experts layers (MoEs) enable efficient scaling of language models\nthrough conditional computation. This paper presents a detailed empirical study\nof how autoregressive MoE language models scale in comparison with dense models\nin a wide range of settings: in- and out-of-domain language modeling, zero- and\nfew-shot priming, and full-shot fine-tuning. With the exception of fine-tuning,\nwe find MoEs to be substantially more compute efficient. At more modest\ntraining budgets, MoEs can match the performance of dense models using $\\sim$4\ntimes less compute. This gap narrows at scale, but our largest MoE model (1.1T\nparameters) consistently outperforms a compute-equivalent dense model (6.7B\nparameters). Overall, this performance gap varies greatly across tasks and\ndomains, suggesting that MoE and dense models generalize differently in ways\nthat are worthy of future study. We make our code and models publicly available\nfor research use.", "collection_id": "2c2f9ef4a27d49c9466abbf53c6fc408f0f65c176b2d357b85aedb1282bfb378"}, "question": "How many parameters does the largest MoE model have?\n", "answer": "The largest MoE model has 1.1 trillion (1.1T) parameters."}
{"document": {"content": "Developing neural network image classification models often requires\nsignificant architecture engineering. In this paper, we study a method to learn\nthe model architectures directly on the dataset of interest. As this approach\nis expensive when the dataset is large, we propose to search for an\narchitectural building block on a small dataset and then transfer the block to\na larger dataset. The key contribution of this work is the design of a new\nsearch space (the \"NASNet search space\") which enables transferability. In our\nexperiments, we search for the best convolutional layer (or \"cell\") on the\nCIFAR-10 dataset and then apply this cell to the ImageNet dataset by stacking\ntogether more copies of this cell, each with their own parameters to design a\nconvolutional architecture, named \"NASNet architecture\". We also introduce a\nnew regularization technique called ScheduledDropPath that significantly\nimproves generalization in the NASNet models. On CIFAR-10 itself, NASNet\nachieves 2.4% error rate, which is state-of-the-art. On ImageNet, NASNet\nachieves, among the published works, state-of-the-art accuracy of 82.7% top-1\nand 96.2% top-5 on ImageNet. Our model is 1.2% better in top-1 accuracy than\nthe best human-invented architectures while having 9 billion fewer FLOPS - a\nreduction of 28% in computational demand from the previous state-of-the-art\nmodel. When evaluated at different levels of computational cost, accuracies of\nNASNets exceed those of the state-of-the-art human-designed models. For\ninstance, a small version of NASNet also achieves 74% top-1 accuracy, which is\n3.1% better than equivalently-sized, state-of-the-art models for mobile\nplatforms. Finally, the learned features by NASNet used with the Faster-RCNN\nframework surpass state-of-the-art by 4.0% achieving 43.1% mAP on the COCO\ndataset.", "collection_id": "0161d440ebcdbb054d04eed18d964c799c717db3a6f15e4b70cce15537aa2ca9"}, "question": "What is the top-1 accuracy achieved by NASNet on ImageNet?\n", "answer": "NASNet achieves a state-of-the-art accuracy of 82.7% top-1 on ImageNet, which is 1.2% better than the best human-invented architectures."}
{"document": {"content": "Language models demonstrate both quantitative improvement and new qualitative\ncapabilities with increasing scale. Despite their potentially transformative\nimpact, these new capabilities are as yet poorly characterized. In order to\ninform future research, prepare for disruptive new model capabilities, and\nameliorate socially harmful effects, it is vital that we understand the present\nand near-future capabilities and limitations of language models. To address\nthis challenge, we introduce the Beyond the Imitation Game benchmark\n(BIG-bench). BIG-bench currently consists of 204 tasks, contributed by 450\nauthors across 132 institutions. Task topics are diverse, drawing problems from\nlinguistics, childhood development, math, common-sense reasoning, biology,\nphysics, social bias, software development, and beyond. BIG-bench focuses on\ntasks that are believed to be beyond the capabilities of current language\nmodels. We evaluate the behavior of OpenAI's GPT models, Google-internal dense\ntransformer architectures, and Switch-style sparse transformers on BIG-bench,\nacross model sizes spanning millions to hundreds of billions of parameters. In\naddition, a team of human expert raters performed all tasks in order to provide\na strong baseline. Findings include: model performance and calibration both\nimprove with scale, but are poor in absolute terms (and when compared with\nrater performance); performance is remarkably similar across model classes,\nthough with benefits from sparsity; tasks that improve gradually and\npredictably commonly involve a large knowledge or memorization component,\nwhereas tasks that exhibit \"breakthrough\" behavior at a critical scale often\ninvolve multiple steps or components, or brittle metrics; social bias typically\nincreases with scale in settings with ambiguous context, but this can be\nimproved with prompting.", "collection_id": "4c27b9bf4aa68cbf5214c6156af941458a18d7b25d737b1b46c43e31487eef82"}, "question": "How many tasks does the Beyond the Imitation Game benchmark (BIG-bench) currently consist of?\n\n", "answer": "The Beyond the Imitation Game benchmark (BIG-bench) currently consists of 204 tasks, which were contributed by 450 authors across 132 institutions. These tasks cover a diverse range of topics, including linguistics, childhood development, math, common-sense reasoning, biology, physics, social bias, software development, and more."}
{"document": {"content": "While large language models (LLMs) have demonstrated impressive capabilities\nacross tasks in language understanding and interactive decision making, their\nabilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g.\naction plan generation) have primarily been studied as separate topics. In this\npaper, we explore the use of LLMs to generate both reasoning traces and\ntask-specific actions in an interleaved manner, allowing for greater synergy\nbetween the two: reasoning traces help the model induce, track, and update\naction plans as well as handle exceptions, while actions allow it to interface\nwith external sources, such as knowledge bases or environments, to gather\nadditional information. We apply our approach, named ReAct, to a diverse set of\nlanguage and decision making tasks and demonstrate its effectiveness over\nstate-of-the-art baselines, as well as improved human interpretability and\ntrustworthiness over methods without reasoning or acting components.\nConcretely, on question answering (HotpotQA) and fact verification (Fever),\nReAct overcomes issues of hallucination and error propagation prevalent in\nchain-of-thought reasoning by interacting with a simple Wikipedia API, and\ngenerates human-like task-solving trajectories that are more interpretable than\nbaselines without reasoning traces. On two interactive decision making\nbenchmarks (ALFWorld and WebShop), ReAct outperforms imitation and\nreinforcement learning methods by an absolute success rate of 34% and 10%\nrespectively, while being prompted with only one or two in-context examples.\nProject site with code: https://react-lm.github.io", "collection_id": "8492206a4700a9315b3e01f28300f185bdd9e42e7bfd932186437f612ef00def"}, "question": "What is the name of the approach that generates both reasoning traces and task-specific actions in an interleaved manner?\n\n", "answer": "The approach is named ReAct. It allows for greater synergy between reasoning and acting, enabling the model to induce, track, and update action plans, handle exceptions, and interface with external sources to gather additional information."}
{"document": {"content": "Building open domain conversational systems that allow users to have engaging\nconversations on topics of their choice is a challenging task. Alexa Prize was\nlaunched in 2016 to tackle the problem of achieving natural, sustained,\ncoherent and engaging open-domain dialogs. In the second iteration of the\ncompetition in 2018, university teams advanced the state of the art by using\ncontext in dialog models, leveraging knowledge graphs for language\nunderstanding, handling complex utterances, building statistical and\nhierarchical dialog managers, and leveraging model-driven signals from user\nresponses. The 2018 competition also included the provision of a suite of tools\nand models to the competitors including the CoBot (conversational bot) toolkit,\ntopic and dialog act detection models, conversation evaluators, and a sensitive\ncontent detection model so that the competing teams could focus on building\nknowledge-rich, coherent and engaging multi-turn dialog systems. This paper\noutlines the advances developed by the university teams as well as the Alexa\nPrize team to achieve the common goal of advancing the science of\nConversational AI. We address several key open-ended problems such as\nconversational speech recognition, open domain natural language understanding,\ncommonsense reasoning, statistical dialog management, and dialog evaluation.\nThese collaborative efforts have driven improved experiences by Alexa users to\nan average rating of 3.61, the median duration of 2 mins 18 seconds, and\naverage turns to 14.6, increases of 14%, 92%, 54% respectively since the launch\nof the 2018 competition. For conversational speech recognition, we have\nimproved our relative Word Error Rate by 55% and our relative Entity Error Rate\nby 34% since the launch of the Alexa Prize. Socialbots improved in quality\nsignificantly more rapidly in 2018, in part due to the release of the CoBot\ntoolkit.", "collection_id": "c0a252384e1dae77243c2e659ea771baf0a4d8b7a27f051a41841177d81f91ec"}, "question": "What year was the Alexa Prize launched to tackle the problem of achieving natural, sustained, coherent and engaging open-domain dialogs?\n\n", "answer": "The Alexa Prize was launched in 2016 to tackle the problem of achieving natural, sustained, coherent and engaging open-domain dialogs."}
{"document": {"content": "Knowledge underpins reasoning. Recent research demonstrates that when\nrelevant knowledge is provided as additional context to commonsense question\nanswering (QA), it can substantially enhance the performance even on top of\nstate-of-the-art. The fundamental challenge is where and how to find such\nknowledge that is high quality and on point with respect to the question;\nknowledge retrieved from knowledge bases are incomplete and knowledge generated\nfrom language models are inconsistent. We present Rainier, or Reinforced\nKnowledge Introspector, that learns to generate contextually relevant knowledge\nin response to given questions. Our approach starts by imitating knowledge\ngenerated by GPT-3, then learns to generate its own knowledge via reinforcement\nlearning where rewards are shaped based on the increased performance on the\nresulting question answering. Rainier demonstrates substantial and consistent\nperformance gains when tested over 9 different commonsense benchmarks:\nincluding 5 datasets that are seen during model training, as well as 4 datasets\nthat are kept unseen. Our work is the first to report that knowledge generated\nby models that are orders of magnitude smaller than GPT-3, even without direct\nsupervision on the knowledge itself, can exceed the quality of commonsense\nknowledge elicited from GPT-3.", "collection_id": "16b251df9bee8aebc319b83ac72baceea5ec1a726e49b93982aa93ae23be7e22"}, "question": "What is Rainier in the context of artificial intelligence?\n", "answer": "Rainier, or Reinforced Knowledge Introspector, is an AI system that generates contextually relevant knowledge in response to given questions, initially by imitating knowledge generated by GPT-3 and then learning to produce its own knowledge through reinforcement learning."}
{"document": {"content": "Despite the recent success of multi-task learning and transfer learning for\nnatural language processing (NLP), few works have systematically studied the\neffect of scaling up the number of tasks during pre-training. Towards this\ngoal, this paper introduces ExMix (Extreme Mixture): a massive collection of\n107 supervised NLP tasks across diverse domains and task-families. Using ExMix,\nwe study the effect of multi-task pre-training at the largest scale to date,\nand analyze co-training transfer amongst common families of tasks. Through this\nanalysis, we show that manually curating an ideal set of tasks for multi-task\npre-training is not straightforward, and that multi-task scaling can vastly\nimprove models on its own. Finally, we propose ExT5: a model pre-trained using\na multi-task objective of self-supervised span denoising and supervised ExMix.\nVia extensive experiments, we show that ExT5 outperforms strong T5 baselines on\nSuperGLUE, GEM, Rainbow, Closed-Book QA tasks, and several tasks outside of\nExMix. ExT5 also significantly improves sample efficiency while pre-training.", "collection_id": "63eb33031ab15d4ecc99cceb0f423f4d56cf14a1ef434803c632aa4403645bfa"}, "question": "What is ExMix in natural language processing?\n", "answer": "ExMix, short for Extreme Mixture, is a massive collection of 107 supervised natural language processing (NLP) tasks across diverse domains and task-families. It was created to study the effect of scaling up the number of tasks during pre-training in NLP."}
{"document": {"content": "Despite showing increasingly human-like conversational abilities,\nstate-of-the-art dialogue models often suffer from factual incorrectness and\nhallucination of knowledge (Roller et al., 2020). In this work we explore the\nuse of neural-retrieval-in-the-loop architectures - recently shown to be\neffective in open-domain QA (Lewis et al., 2020b; Izacard and Grave, 2020) -\nfor knowledge-grounded dialogue, a task that is arguably more challenging as it\nrequires querying based on complex multi-turn dialogue context and generating\nconversationally coherent responses. We study various types of architectures\nwith multiple components - retrievers, rankers, and encoder-decoders - with the\ngoal of maximizing knowledgeability while retaining conversational ability. We\ndemonstrate that our best models obtain state-of-the-art performance on two\nknowledge-grounded conversational tasks. The models exhibit open-domain\nconversational capabilities, generalize effectively to scenarios not within the\ntraining data, and, as verified by human evaluations, substantially reduce the\nwell-known problem of knowledge hallucination in state-of-the-art chatbots.", "collection_id": "bd13cc24772c0dd2745025e202a70beabfe7152b55d4bb0255a8f9063e6686ce"}, "question": "What is a major issue with state-of-the-art dialogue models?\n", "answer": "A major issue with state-of-the-art dialogue models is factual incorrectness and hallucination of knowledge, meaning they often provide false information or make things up."}
{"document": {"content": "In the last year, new models and methods for pretraining and transfer\nlearning have driven striking performance improvements across a range of\nlanguage understanding tasks. The GLUE benchmark, introduced a little over one\nyear ago, offers a single-number metric that summarizes progress on a diverse\nset of such tasks, but performance on the benchmark has recently surpassed the\nlevel of non-expert humans, suggesting limited headroom for further research.\nIn this paper we present SuperGLUE, a new benchmark styled after GLUE with a\nnew set of more difficult language understanding tasks, a software toolkit, and\na public leaderboard. SuperGLUE is available at super.gluebenchmark.com.", "collection_id": "050dbc8c33d3fbfaa89db36ddac0f7d6a0e9cae33ed25c76430abac85e01e2d3"}, "question": "What is the website for accessing SuperGLUE?\n", "answer": "The website for accessing SuperGLUE is super.gluebenchmark.com."}
{"document": {"content": "In this paper, we quantify, analyze and mitigate gender bias exhibited in\nELMo's contextualized word vectors. First, we conduct several intrinsic\nanalyses and find that (1) training data for ELMo contains significantly more\nmale than female entities, (2) the trained ELMo embeddings systematically\nencode gender information and (3) ELMo unequally encodes gender information\nabout male and female entities. Then, we show that a state-of-the-art\ncoreference system that depends on ELMo inherits its bias and demonstrates\nsignificant bias on the WinoBias probing corpus. Finally, we explore two\nmethods to mitigate such gender bias and show that the bias demonstrated on\nWinoBias can be eliminated.", "collection_id": "f33e4345317079f563fee35cbf4bf99c99c25b4932c07f980c5c7592c8303bb0"}, "question": "What are the issues with the training data for ELMo contextualized word vectors?\n", "answer": "The training data for ELMo contains significantly more male than female entities, which contributes to the model's bias. This imbalance leads to the model systematically encoding gender information and doing so unequally for male and female entities."}
{"document": {"content": "Language models (LMs) have recently been shown to generate more factual\nresponses by employing modularity (Zhou et al., 2021) in combination with\nretrieval (Adolphs et al., 2021). We extend the recent approach of Adolphs et\nal. (2021) to include internet search as a module. Our SeeKeR (Search\nengine->Knowledge->Response) method thus applies a single LM to three modular\ntasks in succession: search, generating knowledge, and generating a final\nresponse. We show that, when using SeeKeR as a dialogue model, it outperforms\nthe state-of-the-art model BlenderBot 2 (Chen et al., 2021) on open-domain\nknowledge-grounded conversations for the same number of parameters, in terms of\nconsistency, knowledge and per-turn engagingness. SeeKeR applied to topical\nprompt completions as a standard language model outperforms GPT2 (Radford et\nal., 2019) and GPT3 (Brown et al., 2020) in terms of factuality and topicality,\ndespite GPT3 being a vastly larger model. Our code and models are made publicly\navailable.", "collection_id": "7e0cfdcd5fbf959f6b212d030a77005b5d40a77f5ad855a11913353c0ab22970"}, "question": "What is the SeeKeR method and how does it work?\n", "answer": "The SeeKeR (Search engine->Knowledge->Response) method is a modular approach that employs a single language model to perform three tasks in succession: search, generating knowledge, and generating a final response. This method utilizes internet search as a module to improve the factuality of responses."}
{"document": {"content": "We present NewsQA, a challenging machine comprehension dataset of over\n100,000 human-generated question-answer pairs. Crowdworkers supply questions\nand answers based on a set of over 10,000 news articles from CNN, with answers\nconsisting of spans of text from the corresponding articles. We collect this\ndataset through a four-stage process designed to solicit exploratory questions\nthat require reasoning. A thorough analysis confirms that NewsQA demands\nabilities beyond simple word matching and recognizing textual entailment. We\nmeasure human performance on the dataset and compare it to several strong\nneural models. The performance gap between humans and machines (0.198 in F1)\nindicates that significant progress can be made on NewsQA through future\nresearch. The dataset is freely available at\nhttps://datasets.maluuba.com/NewsQA.", "collection_id": "dc83d417f4497db2cda7b69e0ec33f10c0118eb7844ee2cb6e904d8c65922202"}, "question": "What is the performance gap between humans and machines on the NewsQA dataset?\n", "answer": "The performance gap between humans and machines on the NewsQA dataset is 0.198 in F1 score, indicating that there is still room for improvement through future research."}
{"document": {"content": "Few-shot prompting is a surprisingly powerful way to use Large Language\nModels (LLMs) to solve various tasks. However, this approach struggles as the\ntask complexity increases or when the individual reasoning steps of the task\nthemselves are hard to learn, especially when embedded in more complex tasks.\nTo address this, we propose Decomposed Prompting, a new approach to solve\ncomplex tasks by decomposing them (via prompting) into simpler sub-tasks that\ncan be delegated to a library of prompting-based LLMs dedicated to these\nsub-tasks. This modular structure allows each prompt to be optimized for its\nspecific sub-task, further decomposed if necessary, and even easily replaced\nwith more effective prompts, trained models, or symbolic functions if desired.\nWe show that the flexibility and modularity of Decomposed Prompting allows it\nto outperform prior work on few-shot prompting using GPT3. On symbolic\nreasoning tasks, we can further decompose sub-tasks that are hard for LLMs into\neven simpler solvable sub-tasks. When the complexity comes from the input\nlength, we can recursively decompose the task into the same task but with\nsmaller inputs. We also evaluate our approach on textual multi-step reasoning\ntasks: on long-context multi-hop QA task, we can more effectively teach the\nsub-tasks via our separate sub-tasks prompts; and on open-domain multi-hop QA,\nwe can incorporate a symbolic information retrieval within our decomposition\nframework, leading to improved performance on both tasks. Datasets, Code and\nPrompts available at https://github.com/allenai/DecomP.", "collection_id": "e0e13f3b213c9a4ad72226a003c2ae531d2b3fe9e328e9baf3e56951443ba845"}, "question": "What is Decomposed Prompting in Large Language Models?\n", "answer": "Decomposed Prompting is an approach to solve complex tasks by breaking them down into simpler sub-tasks that can be delegated to a library of prompting-based Large Language Models (LLMs) dedicated to these sub-tasks. This modular structure allows each prompt to be optimized for its specific sub-task, further decomposed if necessary, and even easily replaced with more effective prompts, trained models, or symbolic functions if desired."}
{"document": {"content": "We introduce a method to provide vectorial representations of visual\nclassification tasks which can be used to reason about the nature of those\ntasks and their relations. Given a dataset with ground-truth labels and a loss\nfunction defined over those labels, we process images through a \"probe network\"\nand compute an embedding based on estimates of the Fisher information matrix\nassociated with the probe network parameters. This provides a fixed-dimensional\nembedding of the task that is independent of details such as the number of\nclasses and does not require any understanding of the class label semantics. We\ndemonstrate that this embedding is capable of predicting task similarities that\nmatch our intuition about semantic and taxonomic relations between different\nvisual tasks (e.g., tasks based on classifying different types of plants are\nsimilar) We also demonstrate the practical value of this framework for the\nmeta-task of selecting a pre-trained feature extractor for a new task. We\npresent a simple meta-learning framework for learning a metric on embeddings\nthat is capable of predicting which feature extractors will perform well.\nSelecting a feature extractor with task embedding obtains a performance close\nto the best available feature extractor, while costing substantially less than\nexhaustively training and evaluating on all available feature extractors.", "collection_id": "24977c0c4eff59146f720f5aa7bdb20b0467b9ef907bd3d38159e9cb95ac5897"}, "question": "What is the purpose of processing images through a \"probe network\" in the method for providing vectorial representations of visual classification tasks?\n\n", "answer": "The purpose of processing images through a \"probe network\" is to compute an embedding based on estimates of the Fisher information matrix associated with the probe network parameters. This provides a fixed-dimensional embedding of the task that is independent of details such as the number of classes and does not require any understanding of the class label semantics."}
{"document": {"content": "Large language models (LLMs) have shown exceptional performance on a variety\nof natural language tasks. Yet, their capabilities for HTML understanding --\ni.e., parsing the raw HTML of a webpage, with applications to automation of\nweb-based tasks, crawling, and browser-assisted retrieval -- have not been\nfully explored. We contribute HTML understanding models (fine-tuned LLMs) and\nan in-depth analysis of their capabilities under three tasks: (i) Semantic\nClassification of HTML elements, (ii) Description Generation for HTML inputs,\nand (iii) Autonomous Web Navigation of HTML pages. While previous work has\ndeveloped dedicated architectures and training procedures for HTML\nunderstanding, we show that LLMs pretrained on standard natural language\ncorpora transfer remarkably well to HTML understanding tasks. For instance,\nfine-tuned LLMs are 12% more accurate at semantic classification compared to\nmodels trained exclusively on the task dataset. Moreover, when fine-tuned on\ndata from the MiniWoB benchmark, LLMs successfully complete 50% more tasks\nusing 192x less data compared to the previous best supervised model. Out of the\nLLMs we evaluate, we show evidence that T5-based models are ideal due to their\nbidirectional encoder-decoder architecture. To promote further research on LLMs\nfor HTML understanding, we create and open-source a large-scale HTML dataset\ndistilled and auto-labeled from CommonCrawl.", "collection_id": "3ae7bd5d2b54c3155114a2e5994f36f913b089230d473682ddf11a9119ad4618"}, "question": "What type of large language model architecture is ideal for HTML understanding tasks?\n\n", "answer": "T5-based models are ideal for HTML understanding tasks due to their bidirectional encoder-decoder architecture. This architecture allows them to effectively process and understand the structure and content of HTML pages, making them well-suited for tasks such as semantic classification, description generation, and autonomous web navigation."}
{"document": {"content": "We introduce LLaMA, a collection of foundation language models ranging from\n7B to 65B parameters. We train our models on trillions of tokens, and show that\nit is possible to train state-of-the-art models using publicly available\ndatasets exclusively, without resorting to proprietary and inaccessible\ndatasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks,\nand LLaMA-65B is competitive with the best models, Chinchilla-70B and\nPaLM-540B. We release all our models to the research community.", "collection_id": "395b412d89c13838fefc1c829b373fec7c76e1938c3abdba18306fb1607177c6"}, "question": "What sizes of foundation language models are included in the LLaMA collection?\n", "answer": "The LLaMA collection includes foundation language models ranging from 7B to 65B parameters."}
{"document": {"content": "Abstractive summarization systems today produce fluent and relevant output,\nbut often \"hallucinate\" statements not supported by the source text. We analyze\nthe connection between hallucinations and training data, and find evidence that\nmodels hallucinate because they train on target summaries that are unsupported\nby the source. Based on our findings, we present PINOCCHIO, a new decoding\nmethod that improves the consistency of a transformer-based abstractive\nsummarizer by constraining beam search to avoid hallucinations. Given the model\nstates and outputs at a given step, PINOCCHIO detects likely model\nhallucinations based on various measures of attribution to the source text.\nPINOCCHIO backtracks to find more consistent output, and can opt to produce no\nsummary at all when no consistent generation can be found. In experiments, we\nfind that PINOCCHIO improves the consistency of generation (in terms of F1) by\nan average of~67% on two abstractive summarization datasets.", "collection_id": "310b31454b91fcf92d50b53b6504d8e459df3e271dffaef624eb18ca15212e02"}, "question": "What is the name of the new decoding method that improves the consistency of a transformer-based abstractive summarizer by constraining beam search to avoid hallucinations?\n\n", "answer": "The new decoding method is called PINOCCHIO. It detects likely model hallucinations based on various measures of attribution to the source text and backtracks to find more consistent output, opting to produce no summary at all when no consistent generation can be found."}
{"document": {"content": "We present a new approach for pretraining a bi-directional transformer model\nthat provides significant performance gains across a variety of language\nunderstanding problems. Our model solves a cloze-style word reconstruction\ntask, where each word is ablated and must be predicted given the rest of the\ntext. Experiments demonstrate large performance gains on GLUE and new state of\nthe art results on NER as well as constituency parsing benchmarks, consistent\nwith the concurrently introduced BERT model. We also present a detailed\nanalysis of a number of factors that contribute to effective pretraining,\nincluding data domain and size, model capacity, and variations on the cloze\nobjective.", "collection_id": "00fa35650fef605d223a1129d77b772d8c691b1ac2f7f446766d06b95f26869c"}, "question": "What type of task does the presented bi-directional transformer model solve?\n", "answer": "The model solves a cloze-style word reconstruction task, where each word is ablated and must be predicted given the rest of the text."}
{"document": {"content": "Large language models are trained in two stages: (1) unsupervised pretraining\nfrom raw text, to learn general-purpose representations, and (2) large scale\ninstruction tuning and reinforcement learning, to better align to end tasks and\nuser preferences. We measure the relative importance of these two stages by\ntraining LIMA, a 65B parameter LLaMa language model fine-tuned with the\nstandard supervised loss on only 1,000 carefully curated prompts and responses,\nwithout any reinforcement learning or human preference modeling. LIMA\ndemonstrates remarkably strong performance, learning to follow specific\nresponse formats from only a handful of examples in the training data,\nincluding complex queries that range from planning trip itineraries to\nspeculating about alternate history. Moreover, the model tends to generalize\nwell to unseen tasks that did not appear in the training data. In a controlled\nhuman study, responses from LIMA are either equivalent or strictly preferred to\nGPT-4 in 43% of cases; this statistic is as high as 58% when compared to Bard\nand 65% versus DaVinci003, which was trained with human feedback. Taken\ntogether, these results strongly suggest that almost all knowledge in large\nlanguage models is learned during pretraining, and only limited instruction\ntuning data is necessary to teach models to produce high quality output.", "collection_id": "2922e6fd0d01a62780be32e8f91f655ab51b546e5c6cd337ef766ff17b180475"}, "question": "How many parameters does the LIMA language model have?\n", "answer": "The LIMA language model has 65 billion parameters."}
{"document": {"content": "In this paper, we proposed a sentence encoding-based model for recognizing\ntext entailment. In our approach, the encoding of sentence is a two-stage\nprocess. Firstly, average pooling was used over word-level bidirectional LSTM\n(biLSTM) to generate a first-stage sentence representation. Secondly, attention\nmechanism was employed to replace average pooling on the same sentence for\nbetter representations. Instead of using target sentence to attend words in\nsource sentence, we utilized the sentence's first-stage representation to\nattend words appeared in itself, which is called \"Inner-Attention\" in our paper\n. Experiments conducted on Stanford Natural Language Inference (SNLI) Corpus\nhas proved the effectiveness of \"Inner-Attention\" mechanism. With less number\nof parameters, our model outperformed the existing best sentence encoding-based\napproach by a large margin.", "collection_id": "1d46d7d1992379b26999f2ec410a6e356f7ce095267c3ab0ef1f93344de0b037"}, "question": "What mechanism was employed to replace average pooling for better sentence representations?\n\n", "answer": "The mechanism employed to replace average pooling for better sentence representations is called \"Inner-Attention\", which utilizes the sentence's first-stage representation to attend words that appear in itself."}
{"document": {"content": "Like humans, large language models (LLMs) do not always generate the best\noutput on their first try. Motivated by how humans refine their written text,\nwe introduce Self-Refine, an approach for improving initial outputs from LLMs\nthrough iterative feedback and refinement. The main idea is to generate an\ninitial output using an LLMs; then, the same LLMs provides feedback for its\noutput and uses it to refine itself, iteratively. Self-Refine does not require\nany supervised training data, additional training, or reinforcement learning,\nand instead uses a single LLM as the generator, refiner, and feedback provider.\nWe evaluate Self-Refine across 7 diverse tasks, ranging from dialog response\ngeneration to mathematical reasoning, using state-of-the-art (GPT-3.5, ChatGPT,\nand GPT-4) LLMs. Across all evaluated tasks, outputs generated with Self-Refine\nare preferred by humans and automatic metrics over those generated with the\nsame LLM using conventional one-step generation, improving by ~20% absolute on\naverage in task performance. Our work demonstrates that even state-of-the-art\nLLMs like GPT-4 can be further improved at test time using our simple,\nstandalone approach.", "collection_id": "5c2c1b3bc473474eb935cd4e34a65375fd0fc4033920237ca341c3862289fa87"}, "question": "How much does Self-Refine improve task performance in large language models on average?\n", "answer": "Self-Refine improves task performance in large language models by approximately 20% absolute on average. This improvement is seen across various tasks, including dialog response generation and mathematical reasoning, when compared to conventional one-step generation using the same language model."}
{"document": {"content": "Existing datasets for natural language inference (NLI) have propelled\nresearch on language understanding. We propose a new method for automatically\nderiving NLI datasets from the growing abundance of large-scale question\nanswering datasets. Our approach hinges on learning a sentence transformation\nmodel which converts question-answer pairs into their declarative forms.\nDespite being primarily trained on a single QA dataset, we show that it can be\nsuccessfully applied to a variety of other QA resources. Using this system, we\nautomatically derive a new freely available dataset of over 500k NLI examples\n(QA-NLI), and show that it exhibits a wide range of inference phenomena rarely\nseen in previous NLI datasets.", "collection_id": "0d7c83d4577a963bacca68ffb8a695de3c88209bc52fe1682fe4b164ce3b7f00"}, "question": "How many NLI examples are in the QA-NLI dataset?\n", "answer": "The QA-NLI dataset contains over 500,000 NLI examples, which were automatically derived using a sentence transformation model that converts question-answer pairs into their declarative forms. This dataset is freely available and exhibits a wide range of inference phenomena rarely seen in previous NLI datasets."}
{"document": {"content": "Convolutional Neural Networks (ConvNets) are commonly developed at a fixed\nresource budget, and then scaled up for better accuracy if more resources are\navailable. In this paper, we systematically study model scaling and identify\nthat carefully balancing network depth, width, and resolution can lead to\nbetter performance. Based on this observation, we propose a new scaling method\nthat uniformly scales all dimensions of depth/width/resolution using a simple\nyet highly effective compound coefficient. We demonstrate the effectiveness of\nthis method on scaling up MobileNets and ResNet.\n  To go even further, we use neural architecture search to design a new\nbaseline network and scale it up to obtain a family of models, called\nEfficientNets, which achieve much better accuracy and efficiency than previous\nConvNets. In particular, our EfficientNet-B7 achieves state-of-the-art 84.3%\ntop-1 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on\ninference than the best existing ConvNet. Our EfficientNets also transfer well\nand achieve state-of-the-art accuracy on CIFAR-100 (91.7%), Flowers (98.8%),\nand 3 other transfer learning datasets, with an order of magnitude fewer\nparameters. Source code is at\nhttps://github.com/tensorflow/tpu/tree/master/models/official/efficientnet.", "collection_id": "1b6288d0bb1c44b6f6d2d9c56578c9c12861a0e494d1b8d767852d1440156e37"}, "question": "What is the top-1 accuracy achieved by EfficientNet-B7 on ImageNet?\n", "answer": "EfficientNet-B7 achieves a state-of-the-art 84.3% top-1 accuracy on ImageNet."}
{"document": {"content": "We present KERMIT, a simple insertion-based approach to generative modeling\nfor sequences and sequence pairs. KERMIT models the joint distribution and its\ndecompositions (i.e., marginals and conditionals) using a single neural network\nand, unlike much prior work, does not rely on a prespecified factorization of\nthe data distribution. During training, one can feed KERMIT paired data $(x,\ny)$ to learn the joint distribution $p(x, y)$, and optionally mix in unpaired\ndata $x$ or $y$ to refine the marginals $p(x)$ or $p(y)$. During inference, we\nhave access to the conditionals $p(x \\mid y)$ and $p(y \\mid x)$ in both\ndirections. We can also sample from the joint distribution or the marginals.\nThe model supports both serial fully autoregressive decoding and parallel\npartially autoregressive decoding, with the latter exhibiting an empirically\nlogarithmic runtime. We demonstrate through experiments in machine translation,\nrepresentation learning, and zero-shot cloze question answering that our\nunified approach is capable of matching or exceeding the performance of\ndedicated state-of-the-art systems across a wide range of tasks without the\nneed for problem-specific architectural adaptation.", "collection_id": "08a2cd9a7530a5c9aaa67ff5f9c21dab09fb28b4be99d9281a981250fedf790a"}, "question": "What type of decoding does KERMIT support for sequence and sequence pair modeling?\n", "answer": "KERMIT supports both serial fully autoregressive decoding and parallel partially autoregressive decoding for sequence and sequence pair modeling. The parallel partially autoregressive decoding exhibits an empirically logarithmic runtime, making it efficient for various tasks."}
{"document": {"content": "Progress in machine learning (ML) comes with a cost to the environment, given\nthat training ML models requires significant computational resources, energy\nand materials. In the present article, we aim to quantify the carbon footprint\nof BLOOM, a 176-billion parameter language model, across its life cycle. We\nestimate that BLOOM's final training emitted approximately 24.7 tonnes\nof~\\carboneq~if we consider only the dynamic power consumption, and 50.5 tonnes\nif we account for all processes ranging from equipment manufacturing to\nenergy-based operational consumption. We also study the energy requirements and\ncarbon emissions of its deployment for inference via an API endpoint receiving\nuser queries in real-time. We conclude with a discussion regarding the\ndifficulty of precisely estimating the carbon footprint of ML models and future\nresearch directions that can contribute towards improving carbon emissions\nreporting.", "collection_id": "fa08f601b3eefe3d509023f3b7aed93c420b70d1fa72e1257d8960d6faf5e8a1"}, "question": "What is the estimated carbon footprint of training the BLOOM language model considering all processes from equipment manufacturing to operational consumption?\n\n", "answer": "The estimated carbon footprint of training the BLOOM language model is approximately 50.5 tonnes of CO2 equivalent when considering all processes from equipment manufacturing to operational consumption."}
{"document": {"content": "Conversational modeling is an important task in natural language\nunderstanding and machine intelligence. Although previous approaches exist,\nthey are often restricted to specific domains (e.g., booking an airline ticket)\nand require hand-crafted rules. In this paper, we present a simple approach for\nthis task which uses the recently proposed sequence to sequence framework. Our\nmodel converses by predicting the next sentence given the previous sentence or\nsentences in a conversation. The strength of our model is that it can be\ntrained end-to-end and thus requires much fewer hand-crafted rules. We find\nthat this straightforward model can generate simple conversations given a large\nconversational training dataset. Our preliminary results suggest that, despite\noptimizing the wrong objective function, the model is able to converse well. It\nis able extract knowledge from both a domain specific dataset, and from a\nlarge, noisy, and general domain dataset of movie subtitles. On a\ndomain-specific IT helpdesk dataset, the model can find a solution to a\ntechnical problem via conversations. On a noisy open-domain movie transcript\ndataset, the model can perform simple forms of common sense reasoning. As\nexpected, we also find that the lack of consistency is a common failure mode of\nour model.", "collection_id": "d78401f455abe3c28a285fe3315bd40980dd8b391e34066fdce5927995a951ae"}, "question": "What type of framework is used for conversational modeling in the proposed approach?\n", "answer": "The proposed approach for conversational modeling uses the sequence to sequence framework, which enables the model to converse by predicting the next sentence given the previous sentence or sentences in a conversation. This framework allows the model to be trained end-to-end, reducing the need for hand-crafted rules."}
{"document": {"content": "To create models that are robust across a wide range of test inputs, training\ndatasets should include diverse examples that span numerous phenomena. Dynamic\nadversarial data collection (DADC), where annotators craft examples that\nchallenge continually improving models, holds promise as an approach for\ngenerating such diverse training sets. Prior work has shown that running DADC\nover 1-3 rounds can help models fix some error types, but it does not\nnecessarily lead to better generalization beyond adversarial test data. We\nargue that running DADC over many rounds maximizes its training-time benefits,\nas the different rounds can together cover many of the task-relevant phenomena.\nWe present the first study of longer-term DADC, where we collect 20 rounds of\nNLI examples for a small set of premise paragraphs, with both adversarial and\nnon-adversarial approaches. Models trained on DADC examples make 26% fewer\nerrors on our expert-curated test set compared to models trained on\nnon-adversarial data. Our analysis shows that DADC yields examples that are\nmore difficult, more lexically and syntactically diverse, and contain fewer\nannotation artifacts compared to non-adversarial examples.", "collection_id": "605ae53e1e21d680fad95d00f897983303e7779ee406d20328d51d20ac644dbb"}, "question": "What is the percentage of error reduction in models trained on DADC examples compared to models trained on non-adversarial data?\n\n", "answer": "Models trained on DADC examples make 26% fewer errors on an expert-curated test set compared to models trained on non-adversarial data. This suggests that using DADC can significantly improve the performance of models by reducing the number of errors they make."}
{"document": {"content": "Chain-of-thought prompting has demonstrated remarkable performance on various\nnatural language reasoning tasks. However, it tends to perform poorly on tasks\nwhich requires solving problems harder than the exemplars shown in the prompts.\nTo overcome this challenge of easy-to-hard generalization, we propose a novel\nprompting strategy, least-to-most prompting. The key idea in this strategy is\nto break down a complex problem into a series of simpler subproblems and then\nsolve them in sequence. Solving each subproblem is facilitated by the answers\nto previously solved subproblems. Our experimental results on tasks related to\nsymbolic manipulation, compositional generalization, and math reasoning reveal\nthat least-to-most prompting is capable of generalizing to more difficult\nproblems than those seen in the prompts. A notable finding is that when the\nGPT-3 code-davinci-002 model is used with least-to-most prompting, it can solve\nthe compositional generalization benchmark SCAN in any split (including length\nsplit) with an accuracy of at least 99% using just 14 exemplars, compared to\nonly 16% accuracy with chain-of-thought prompting. This is particularly\nnoteworthy because neural-symbolic models in the literature that specialize in\nsolving SCAN are trained on the entire training set containing over 15,000\nexamples. We have included prompts for all the tasks in the Appendix.", "collection_id": "c64c88d92ec1eee58309f5bfbae8229b59d8d8e39d73c54f7f2ff06556595b70"}, "question": "What accuracy can the GPT-3 code-davinci-002 model achieve on the compositional generalization benchmark SCAN when used with least-to-most prompting?\n", "answer": "The GPT-3 code-davinci-002 model can solve the compositional generalization benchmark SCAN with an accuracy of at least 99% using just 14 exemplars when used with least-to-most prompting."}
{"document": {"content": "Reasoning and inference are central to human and artificial intelligence.\nModeling inference in human language is very challenging. With the availability\nof large annotated data (Bowman et al., 2015), it has recently become feasible\nto train neural network based inference models, which have shown to be very\neffective. In this paper, we present a new state-of-the-art result, achieving\nthe accuracy of 88.6% on the Stanford Natural Language Inference Dataset.\nUnlike the previous top models that use very complicated network architectures,\nwe first demonstrate that carefully designing sequential inference models based\non chain LSTMs can outperform all previous models. Based on this, we further\nshow that by explicitly considering recursive architectures in both local\ninference modeling and inference composition, we achieve additional\nimprovement. Particularly, incorporating syntactic parsing information\ncontributes to our best result---it further improves the performance even when\nadded to the already very strong model.", "collection_id": "4c7654a1e334a2d59c4cd545ad1e79765b10362d25c5eba64f1ba849cae5a978"}, "question": "What accuracy was achieved on the Stanford Natural Language Inference Dataset in the presented study?\n", "answer": "The study achieved an accuracy of 88.6% on the Stanford Natural Language Inference Dataset, setting a new state-of-the-art result."}
{"document": {"content": "Training large deep neural networks on massive datasets is computationally\nvery challenging. There has been recent surge in interest in using large batch\nstochastic optimization methods to tackle this issue. The most prominent\nalgorithm in this line of research is LARS, which by employing layerwise\nadaptive learning rates trains ResNet on ImageNet in a few minutes. However,\nLARS performs poorly for attention models like BERT, indicating that its\nperformance gains are not consistent across tasks. In this paper, we first\nstudy a principled layerwise adaptation strategy to accelerate training of deep\nneural networks using large mini-batches. Using this strategy, we develop a new\nlayerwise adaptive large batch optimization technique called LAMB; we then\nprovide convergence analysis of LAMB as well as LARS, showing convergence to a\nstationary point in general nonconvex settings. Our empirical results\ndemonstrate the superior performance of LAMB across various tasks such as BERT\nand ResNet-50 training with very little hyperparameter tuning. In particular,\nfor BERT training, our optimizer enables use of very large batch sizes of 32868\nwithout any degradation of performance. By increasing the batch size to the\nmemory limit of a TPUv3 Pod, BERT training time can be reduced from 3 days to\njust 76 minutes (Table 1). The LAMB implementation is available at\nhttps://github.com/tensorflow/addons/blob/master/tensorflow_addons/optimizers/lamb.py", "collection_id": "448f23817875a62aad8c88c52ac21eddf55adc402158d255ea318f6058cb7f07"}, "question": "How long does BERT training take with the LAMB optimizer and a large batch size on a TPUv3 Pod?\n", "answer": "With the LAMB optimizer and a large batch size on a TPUv3 Pod, BERT training time can be reduced to just 76 minutes, down from the original 3 days."}
{"document": {"content": "Large-scale Pre-Trained Language Models (PTLMs) capture knowledge from\nmassive human-written data which contains latent societal biases and toxic\ncontents. In this paper, we leverage the primary task of PTLMs, i.e., language\nmodeling, and propose a new metric to quantify manifested implicit\nrepresentational harms in PTLMs towards 13 marginalized demographics. Using\nthis metric, we conducted an empirical analysis of 24 widely used PTLMs. Our\nanalysis provides insights into the correlation between the proposed metric in\nthis work and other related metrics for representational harm. We observe that\nour metric correlates with most of the gender-specific metrics in the\nliterature. Through extensive experiments, we explore the connections between\nPTLMs architectures and representational harms across two dimensions: depth and\nwidth of the networks. We found that prioritizing depth over width, mitigates\nrepresentational harms in some PTLMs. Our code and data can be found at\nhttps://github.com/microsoft/SafeNLP.", "collection_id": "ad83f350c19c173538a91e55d3d6cf951207eee8f8225be07903e41457094059"}, "question": "What is the link to access the code and data for the research on Pre-Trained Language Models and representational harms?\n", "answer": "The code and data for the research can be accessed at https://github.com/microsoft/SafeNLP."}
{"document": {"content": "With the recent wave of progress in artificial intelligence (AI) has come a\ngrowing awareness of the large-scale impacts of AI systems, and recognition\nthat existing regulations and norms in industry and academia are insufficient\nto ensure responsible AI development. In order for AI developers to earn trust\nfrom system users, customers, civil society, governments, and other\nstakeholders that they are building AI responsibly, they will need to make\nverifiable claims to which they can be held accountable. Those outside of a\ngiven organization also need effective means of scrutinizing such claims. This\nreport suggests various steps that different stakeholders can take to improve\nthe verifiability of claims made about AI systems and their associated\ndevelopment processes, with a focus on providing evidence about the safety,\nsecurity, fairness, and privacy protection of AI systems. We analyze ten\nmechanisms for this purpose--spanning institutions, software, and hardware--and\nmake recommendations aimed at implementing, exploring, or improving those\nmechanisms.", "collection_id": "8f3fd012f7e4b5c3186b3987cebdd2baf4214f0ec73e2175ebf0c26c9c4ffa19"}, "question": "What is the main goal of AI developers to achieve trust from system users and stakeholders?\n\n", "answer": "The main goal of AI developers to achieve trust from system users and stakeholders is to make verifiable claims about the responsible development of AI systems, ensuring accountability for the safety, security, fairness, and privacy protection of these systems."}
{"document": {"content": "Adaptive optimization algorithms, such as Adam and RMSprop, have shown better\noptimization performance than stochastic gradient descent (SGD) in some\nscenarios. However, recent studies show that they often lead to worse\ngeneralization performance than SGD, especially for training deep neural\nnetworks (DNNs). In this work, we identify the reasons that Adam generalizes\nworse than SGD, and develop a variant of Adam to eliminate the generalization\ngap. The proposed method, normalized direction-preserving Adam (ND-Adam),\nenables more precise control of the direction and step size for updating weight\nvectors, leading to significantly improved generalization performance.\nFollowing a similar rationale, we further improve the generalization\nperformance in classification tasks by regularizing the softmax logits. By\nbridging the gap between SGD and Adam, we also hope to shed light on why\ncertain optimization algorithms generalize better than others.", "collection_id": "7d9924575541bd71223f8a2aba420f1873bacb2f886c348b8cb80eeda0b49933"}, "question": "What is the name of the proposed variant of Adam that aims to eliminate the generalization gap with SGD?\n", "answer": "The proposed variant of Adam is called Normalized Direction-Preserving Adam (ND-Adam). It enables more precise control of the direction and step size for updating weight vectors, which leads to significantly improved generalization performance."}
{"document": {"content": "Teaching machines to accomplish tasks by conversing naturally with humans is\nchallenging. Currently, developing task-oriented dialogue systems requires\ncreating multiple components and typically this involves either a large amount\nof handcrafting, or acquiring costly labelled datasets to solve a statistical\nlearning problem for each component. In this work we introduce a neural\nnetwork-based text-in, text-out end-to-end trainable goal-oriented dialogue\nsystem along with a new way of collecting dialogue data based on a novel\npipe-lined Wizard-of-Oz framework. This approach allows us to develop dialogue\nsystems easily and without making too many assumptions about the task at hand.\nThe results show that the model can converse with human subjects naturally\nwhilst helping them to accomplish tasks in a restaurant search domain.", "collection_id": "693fd49139c89fe60939ff6583ebce83f3166dd56626672e480f1abd10c7ccbc"}, "question": "What type of dialogue system is being introduced to converse naturally with humans and accomplish tasks?\n", "answer": "A neural network-based text-in, text-out end-to-end trainable goal-oriented dialogue system is being introduced to converse naturally with humans and help them accomplish tasks. This system is designed to be easily developed and adaptable to various tasks without requiring a large amount of handcrafting or costly labelled datasets."}
{"document": {"content": "This paper proposes to tackle open- domain question answering using Wikipedia\nas the unique knowledge source: the answer to any factoid question is a text\nspan in a Wikipedia article. This task of machine reading at scale combines the\nchallenges of document retrieval (finding the relevant articles) with that of\nmachine comprehension of text (identifying the answer spans from those\narticles). Our approach combines a search component based on bigram hashing and\nTF-IDF matching with a multi-layer recurrent neural network model trained to\ndetect answers in Wikipedia paragraphs. Our experiments on multiple existing QA\ndatasets indicate that (1) both modules are highly competitive with respect to\nexisting counterparts and (2) multitask learning using distant supervision on\ntheir combination is an effective complete system on this challenging task.", "collection_id": "d972f09ae8fa4aa3bd2fdd15bb7817c696dab14212f0ee3b88b089548dfcb3fd"}, "question": "What are the challenges of open-domain question answering using Wikipedia?\n", "answer": "The challenges of open-domain question answering using Wikipedia are document retrieval (finding the relevant articles) and machine comprehension of text (identifying the answer spans from those articles)."}
{"document": {"content": "With the starting point that implicit human biases are reflected in the\nstatistical regularities of language, it is possible to measure biases in\nEnglish static word embeddings. State-of-the-art neural language models\ngenerate dynamic word embeddings dependent on the context in which the word\nappears. Current methods measure pre-defined social and intersectional biases\nthat appear in particular contexts defined by sentence templates. Dispensing\nwith templates, we introduce the Contextualized Embedding Association Test\n(CEAT), that can summarize the magnitude of overall bias in neural language\nmodels by incorporating a random-effects model. Experiments on social and\nintersectional biases show that CEAT finds evidence of all tested biases and\nprovides comprehensive information on the variance of effect magnitudes of the\nsame bias in different contexts. All the models trained on English corpora that\nwe study contain biased representations.\n  Furthermore, we develop two methods, Intersectional Bias Detection (IBD) and\nEmergent Intersectional Bias Detection (EIBD), to automatically identify the\nintersectional biases and emergent intersectional biases from static word\nembeddings in addition to measuring them in contextualized word embeddings. We\npresent the first algorithmic bias detection findings on how intersectional\ngroup members are strongly associated with unique emergent biases that do not\noverlap with the biases of their constituent minority identities. IBD and EIBD\nachieve high accuracy when detecting the intersectional and emergent biases of\nAfrican American females and Mexican American females. Our results indicate\nthat biases at the intersection of race and gender associated with members of\nmultiple minority groups, such as African American females and Mexican American\nfemales, have the highest magnitude across all neural language models.", "collection_id": "d6eb0b3edbcbd10ae6ec43c881bf1fe35c10bda62356ef4b1c6affdd5c925015"}, "question": "What types of biases are strongly associated with intersectional group members according to recent algorithmic bias detection findings?\n\n", "answer": "Intersectional group members are strongly associated with unique emergent biases that do not overlap with the biases of their constituent minority identities. These emergent biases are particularly significant for individuals who belong to multiple minority groups, such as African American females and Mexican American females, where the intersection of race and gender results in biases of the highest magnitude across all neural language models."}
{"document": {"content": "Step-by-step reasoning approaches like chain of thought (CoT) have proved to\nbe very effective in inducing reasoning capabilities in large language models.\nHowever, the success of the CoT approach is fundamentally tied to the model\nsize, and billion parameter-scale models are often needed to get CoT to work.\nIn this paper, we propose a knowledge distillation approach that leverages the\nstep-by-step CoT reasoning capabilities of larger models and distills these\nabilities into smaller models.\n  In this work, we propose an alternative reasoning scheme, Socratic CoT, that\nlearns a decomposition of the original problem into a sequence of subproblems\nand uses it to guide the intermediate reasoning steps. We use Socratic CoT to\ntrain a combination of two small distilled models: a problem decomposer and a\nsubproblem solver. In practice, given a new problem, the two distilled models\nwork in sync to decompose and solve complex problems. On multiple reasoning\ndatasets (GSM8K, StrategyQA, and SVAMP), our proposed distillation strategies\nboosts the performance of smaller models over 70% compared to the baselines.\nFinally, we investigate when Socratic CoT is an effective alternative to CoT,\ndemonstrating cases where a much smaller model (GPT-2 large) can outperform a\n10X larger model (GPT-3 6B). Our code is available here:\nhttps://github.com/kumar-shridhar/Distiiling-LM", "collection_id": "030588747435b8b718d34b681ed994c6e3c2f1e41c7956725d77539ef2e64f67"}, "question": "What is the main goal of the proposed knowledge distillation approach in the paper?\n\n", "answer": "The main goal of the proposed knowledge distillation approach is to leverage the step-by-step chain of thought (CoT) reasoning capabilities of larger models and distill these abilities into smaller models, enabling them to solve complex problems effectively."}
{"document": {"content": "Drilling boreholes for gas and oil extraction is an expensive process and\nprofitability strongly depends on characteristics of the subsurface. As\nprofitability is a key success factor, companies in the industry utilise well\nlogs to explore the subsurface beforehand. These well logs contain various\ncharacteristics of the rock around the borehole, which allow petrophysicists to\ndetermine the expected amount of contained hydrocarbon. However, these logs are\noften incomplete and, as a consequence, the subsequent analyses cannot exploit\nthe full potential of the well logs.\n  In this paper we demonstrate that Machine Learning can be applied to\n\\emph{fill in the gaps} and estimate missing values. We investigate how the\namount of training data influences the accuracy of prediction and how to best\ndesign regression models (Gradient Boosting and neural network) to obtain\noptimal results. We then explore the models' predictions both quantitatively,\ntracking the prediction error, and qualitatively, capturing the evolution of\nthe measured and predicted values for a given property with depth. Combining\nthe findings has enabled us to develop a predictive model that completes the\nwell logs, increasing their quality and potential commercial value.", "collection_id": "2c41f263ab89a08038e8cefa03bd2a9d7cb6c668f127dee8a5f10d5e99ee82a0"}, "question": "What is the purpose of using well logs in the gas and oil extraction industry?\n\n", "answer": "The purpose of using well logs in the gas and oil extraction industry is to explore the subsurface beforehand, allowing petrophysicists to determine the expected amount of contained hydrocarbon in the rock around the borehole. This information is crucial for determining the profitability of drilling boreholes for gas and oil extraction."}
{"document": {"content": "Question answering (QA) models have shown rapid progress enabled by the\navailability of large, high-quality benchmark datasets. Such annotated datasets\nare difficult and costly to collect, and rarely exist in languages other than\nEnglish, making training QA systems in other languages challenging. An\nalternative to building large monolingual training datasets is to develop\ncross-lingual systems which can transfer to a target language without requiring\ntraining data in that language. In order to develop such systems, it is crucial\nto invest in high quality multilingual evaluation benchmarks to measure\nprogress. We present MLQA, a multi-way aligned extractive QA evaluation\nbenchmark intended to spur research in this area. MLQA contains QA instances in\n7 languages, namely English, Arabic, German, Spanish, Hindi, Vietnamese and\nSimplified Chinese. It consists of over 12K QA instances in English and 5K in\neach other language, with each QA instance being parallel between 4 languages\non average. MLQA is built using a novel alignment context strategy on Wikipedia\narticles, and serves as a cross-lingual extension to existing extractive QA\ndatasets. We evaluate current state-of-the-art cross-lingual representations on\nMLQA, and also provide machine-translation-based baselines. In all cases,\ntransfer results are shown to be significantly behind training-language\nperformance.", "collection_id": "ec77b32fe2b145d09c11d1dd140bec3137cad30a60fc2a5b33df0f573c4b00cf"}, "question": "How many languages does the MLQA benchmark contain QA instances in?\n", "answer": "The MLQA benchmark contains QA instances in 7 languages, which are English, Arabic, German, Spanish, Hindi, Vietnamese, and Simplified Chinese."}
{"document": {"content": "This paper explores zero-label learning in Natural Language Processing (NLP),\nwhereby no human-annotated data is used anywhere during training and models are\ntrained purely on synthetic data. At the core of our framework is a novel\napproach for better leveraging the powerful pretrained language models.\nSpecifically, inspired by the recent success of few-shot inference on GPT-3, we\npresent a training data creation procedure named Unsupervised Data Generation\n(UDG), which leverages few-shot prompts to synthesize high-quality training\ndata without real human annotations. Our method enables zero-label learning as\nwe train task-specific models solely on the synthetic data, yet we achieve\nbetter or comparable results from strong baseline models trained on\nhuman-labeled data. Furthermore, when mixed with labeled data, our approach\nserves as a highly effective data augmentation procedure, achieving new\nstate-of-the-art results on the SuperGLUE benchmark.", "collection_id": "3c724a4b9507c0864539df94283d7e49667add16a14413ac25236098c37ddab8"}, "question": "What is Unsupervised Data Generation (UDG) in Natural Language Processing?\n", "answer": "Unsupervised Data Generation (UDG) is a training data creation procedure that leverages few-shot prompts to synthesize high-quality training data without real human annotations, enabling zero-label learning in Natural Language Processing (NLP)."}
{"document": {"content": "Increasing interest in privacy-preserving machine learning has led to new and\nevolved approaches for generating private synthetic data from undisclosed real\ndata. However, mechanisms of privacy preservation can significantly reduce the\nutility of synthetic data, which in turn impacts downstream tasks such as\nlearning predictive models or inference. We propose several re-weighting\nstrategies using privatised likelihood ratios that not only mitigate\nstatistical bias of downstream estimators but also have general applicability\nto differentially private generative models. Through large-scale empirical\nevaluation, we show that private importance weighting provides simple and\neffective privacy-compliant augmentation for general applications of synthetic\ndata.", "collection_id": "e26bc16f4790953f906eb07096484a05ef7b3ed1a89ef341226d70a5ac981327"}, "question": "What is the purpose of re-weighting strategies in generating private synthetic data?\n", "answer": "The purpose of re-weighting strategies in generating private synthetic data is to mitigate statistical bias of downstream estimators, such as those used in learning predictive models or inference, while maintaining general applicability to differentially private generative models."}
{"document": {"content": "We introduce Social IQa, the first largescale benchmark for commonsense\nreasoning about social situations. Social IQa contains 38,000 multiple choice\nquestions for probing emotional and social intelligence in a variety of\neveryday situations (e.g., Q: \"Jordan wanted to tell Tracy a secret, so Jordan\nleaned towards Tracy. Why did Jordan do this?\" A: \"Make sure no one else could\nhear\"). Through crowdsourcing, we collect commonsense questions along with\ncorrect and incorrect answers about social interactions, using a new framework\nthat mitigates stylistic artifacts in incorrect answers by asking workers to\nprovide the right answer to a different but related question. Empirical results\nshow that our benchmark is challenging for existing question-answering models\nbased on pretrained language models, compared to human performance (>20% gap).\nNotably, we further establish Social IQa as a resource for transfer learning of\ncommonsense knowledge, achieving state-of-the-art performance on multiple\ncommonsense reasoning tasks (Winograd Schemas, COPA).", "collection_id": "a245db42276d36da7ddf8aaad13536a58b179f3401a09f4d3b94abdc06d51524"}, "question": "What is Social IQa used for?\n", "answer": "Social IQa is a large-scale benchmark used for probing emotional and social intelligence in a variety of everyday situations, specifically for commonsense reasoning about social situations. It is also used as a resource for transfer learning of commonsense knowledge to achieve state-of-the-art performance on multiple commonsense reasoning tasks."}
{"document": {"content": "Large Language Models (LLMs) have demonstrated a remarkable ability to\ngeneralize zero-shot to various language-related tasks. This paper focuses on\nthe study of exploring generative LLMs such as ChatGPT and GPT-4 for relevance\nranking in Information Retrieval (IR). Surprisingly, our experiments reveal\nthat properly instructed ChatGPT and GPT-4 can deliver competitive, even\nsuperior results than supervised methods on popular IR benchmarks. Notably,\nGPT-4 outperforms the fully fine-tuned monoT5-3B on MS MARCO by an average of\n2.7 nDCG on TREC datasets, an average of 2.3 nDCG on eight BEIR datasets, and\nan average of 2.7 nDCG on ten low-resource languages Mr.TyDi. Subsequently, we\ndelve into the potential for distilling the ranking capabilities of ChatGPT\ninto a specialized model. Our small specialized model that trained on 10K\nChatGPT generated data outperforms monoT5 trained on 400K annotated MS MARCO\ndata on BEIR. The code to reproduce our results is available at\nwww.github.com/sunnweiwei/RankGPT", "collection_id": "ad17911a59ee398a89274d00dba62c8adc01776dcf9367b060fc9780179fce65"}, "question": "What is the average nDCG improvement of GPT-4 over the fully fine-tuned monoT5-3B on eight BEIR datasets?\n", "answer": "GPT-4 outperforms the fully fine-tuned monoT5-3B by an average of 2.3 nDCG on eight BEIR datasets."}
{"document": {"content": "Cyber-defense systems are being developed to automatically ingest Cyber\nThreat Intelligence (CTI) that contains semi-structured data and/or text to\npopulate knowledge graphs. A potential risk is that fake CTI can be generated\nand spread through Open-Source Intelligence (OSINT) communities or on the Web\nto effect a data poisoning attack on these systems. Adversaries can use fake\nCTI examples as training input to subvert cyber defense systems, forcing the\nmodel to learn incorrect inputs to serve their malicious needs.\n  In this paper, we automatically generate fake CTI text descriptions using\ntransformers. We show that given an initial prompt sentence, a public language\nmodel like GPT-2 with fine-tuning, can generate plausible CTI text with the\nability of corrupting cyber-defense systems. We utilize the generated fake CTI\ntext to perform a data poisoning attack on a Cybersecurity Knowledge Graph\n(CKG) and a cybersecurity corpus. The poisoning attack introduced adverse\nimpacts such as returning incorrect reasoning outputs, representation\npoisoning, and corruption of other dependent AI-based cyber defense systems. We\nevaluate with traditional approaches and conduct a human evaluation study with\ncybersecurity professionals and threat hunters. Based on the study,\nprofessional threat hunters were equally likely to consider our fake generated\nCTI as true.", "collection_id": "e1cd8a80dcc3c896108d124311f57f5a1d710de83f04ede3b5d61c3e43f36179"}, "question": "Can fake Cyber Threat Intelligence (CTI) be used to subvert cyber defense systems?\n", "answer": "Yes, fake CTI can be used to subvert cyber defense systems. Adversaries can use fake CTI examples as training input to force the model to learn incorrect inputs, serving their malicious needs. This can lead to data poisoning attacks, which can have adverse impacts such as returning incorrect reasoning outputs, representation poisoning, and corruption of other dependent AI-based cyber defense systems. In fact, studies have shown that professional threat hunters were equally likely to consider fake generated CTI as true, highlighting the potential risk of fake CTI in compromising cyber defense systems."}
{"document": {"content": "Existing pre-trained models are generally geared towards a particular class\nof problems. To date, there seems to be still no consensus on what the right\narchitecture and pre-training setup should be. This paper presents a unified\nframework for pre-training models that are universally effective across\ndatasets and setups. We begin by disentangling architectural archetypes with\npre-training objectives -- two concepts that are commonly conflated. Next, we\npresent a generalized & unified perspective for self-supervision in NLP and\nshow how different pre-training objectives can be cast as one another and how\ninterpolating between different objectives can be effective. We then propose\nMixture-of-Denoisers (MoD), a pre-training objective that combines diverse\npre-training paradigms together. We furthermore introduce a notion of mode\nswitching, wherein downstream fine-tuning is associated with specific\npre-training schemes. We conduct extensive ablative experiments to compare\nmultiple pre-training objectives and find that our method pushes the\nPareto-frontier by outperforming T5 & GPT-like models across multiple diverse\nsetups. By scaling our model up to 20B parameters, we achieve SOTA performance\non 50 well-established supervised finetuning based NLP tasks. Our model also\nachieve strong results at in-context learning, outperforming 175B GPT-3 on\nzero-shot SuperGLUE and tripling the performance of T5-XXL on one-shot\nsummarization. On 0-shot MMLU, UL2 20B outperforms T0 and T5 models. UL2 20B\nalso works well with chain-of-thought prompting and reasoning, making it an\nappealing choice for research into reasoning at a small to medium scale of 20B\nparameters. Finally, we apply FLAN instruction tuning to the UL2 20B model,\nachieving MMLU and Big-Bench scores competitive to FLAN-PaLM 62B. We release\nFlax-based T5X checkpoints for the UL2 20B & Flan-UL2 20B.", "collection_id": "e880c739f1c9cc772624123d7fbec01816475cc751c7e648a7862dd572be7e98"}, "question": "What is the name of the pre-training objective that combines diverse pre-training paradigms together proposed in the paper?\n\n", "answer": "The pre-training objective that combines diverse pre-training paradigms together proposed in the paper is called Mixture-of-Denoisers (MoD)."}
{"document": {"content": "Foundation models have received much attention due to their effectiveness\nacross a broad range of downstream applications. Though there is a big\nconvergence in terms of architecture, most pretrained models are typically\nstill developed for specific tasks or modalities. In this work, we propose to\nuse language models as a general-purpose interface to various foundation\nmodels. A collection of pretrained encoders perceive diverse modalities (such\nas vision, and language), and they dock with a language model that plays the\nrole of a universal task layer. We propose a semi-causal language modeling\nobjective to jointly pretrain the interface and the modular encoders. We\nsubsume the advantages and capabilities from both causal and non-causal\nmodeling, thereby combining the best of two worlds. Specifically, the proposed\nmethod not only inherits the capabilities of in-context learning and open-ended\ngeneration from causal language modeling, but also is conducive to finetuning\nbecause of the bidirectional encoders. More importantly, our approach\nseamlessly unlocks the combinations of the above capabilities, e.g., enabling\nin-context learning or instruction following with finetuned encoders.\nExperimental results across various language-only and vision-language\nbenchmarks show that our model outperforms or is competitive with specialized\nmodels on finetuning, zero-shot generalization, and few-shot learning.", "collection_id": "90e9132bf7f906f84efc0fb95954b29586664b509add9bbdbcf3755ea944d123"}, "question": "What is the role of the language model in the proposed interface to various foundation models?\n", "answer": "The language model plays the role of a universal task layer that docks with a collection of pretrained encoders perceiving diverse modalities, such as vision and language."}
{"document": {"content": "We propose a Distributional Approach for addressing Controlled Text\nGeneration from pre-trained Language Models (LMs). This approach permits to\nspecify, in a single formal framework, both \"pointwise\" and \"distributional\"\nconstraints over the target LM -- to our knowledge, the first model with such\ngenerality -- while minimizing KL divergence from the initial LM distribution.\nThe optimal target distribution is then uniquely determined as an explicit EBM\n(Energy-Based Model) representation. From that optimal representation we then\ntrain a target controlled Autoregressive LM through an adaptive distributional\nvariant of Policy Gradient. We conduct a first set of experiments over\npointwise constraints showing the advantages of our approach over a set of\nbaselines, in terms of obtaining a controlled LM balancing constraint\nsatisfaction with divergence from the initial LM. We then perform experiments\nover distributional constraints, a unique feature of our approach,\ndemonstrating its potential as a remedy to the problem of Bias in Language\nModels. Through an ablation study, we show the effectiveness of our adaptive\ntechnique for obtaining faster convergence. (Code available at\nhttps://github.com/naver/gdc)", "collection_id": "8fe40e6baf1611ab443d4976ec104e810159005b6a98b40eff220ee92fee478f"}, "question": "What method is used to train a target controlled Autoregressive Language Model from the optimal Energy-Based Model representation?\n\n", "answer": "A target controlled Autoregressive Language Model is trained from the optimal Energy-Based Model representation through an adaptive distributional variant of Policy Gradient."}
{"document": {"content": "Large language models (LLMs) have been shown to be capable of impressive\nfew-shot generalisation to new tasks. However, they still tend to perform\npoorly on multi-step logical reasoning problems. Here we carry out a\ncomprehensive evaluation of LLMs on 50 tasks that probe different aspects of\nlogical reasoning. We show that language models tend to perform fairly well at\nsingle step inference or entailment tasks, but struggle to chain together\nmultiple reasoning steps to solve more complex problems. In light of this, we\npropose a Selection-Inference (SI) framework that exploits pre-trained LLMs as\ngeneral processing modules, and alternates between selection and inference to\ngenerate a series of interpretable, casual reasoning steps leading to the final\nanswer. We show that a 7B parameter LLM used within the SI framework in a\n5-shot generalisation setting, with no fine-tuning, yields a performance\nimprovement of over 100% compared to an equivalent vanilla baseline on a suite\nof 10 logical reasoning tasks. The same model in the same setting even\noutperforms a significantly larger 280B parameter baseline on the same suite of\ntasks. Moreover, answers produced by the SI framework are accompanied by a\ncausal natural-language-based reasoning trace, which has important implications\nfor the safety and trustworthiness of the system.", "collection_id": "06bfc0f1a0cd54ea503e89de765dfc78738764f867d85916aa7fb1a84289f11a"}, "question": "What improvement in performance does a 7B parameter LLM used within the SI framework show compared to an equivalent vanilla baseline on a suite of 10 logical reasoning tasks?\n\n", "answer": "A 7B parameter LLM used within the SI framework in a 5-shot generalisation setting, with no fine-tuning, yields a performance improvement of over 100% compared to an equivalent vanilla baseline on a suite of 10 logical reasoning tasks."}
{"document": {"content": "Language model (LM) pre-training is useful in many language processing tasks.\nBut can pre-trained LMs be further leveraged for more general machine learning\nproblems? We propose an approach for using LMs to scaffold learning and\ngeneralization in general sequential decision-making problems. In this\napproach, goals and observations are represented as a sequence of embeddings,\nand a policy network initialized with a pre-trained LM predicts the next\naction. We demonstrate that this framework enables effective combinatorial\ngeneralization across different environments and supervisory modalities. We\nbegin by assuming access to a set of expert demonstrations, and show that\ninitializing policies with LMs and fine-tuning them via behavior cloning\nimproves task completion rates by 43.6% in the VirtualHome environment. Next,\nwe integrate an active data gathering procedure in which agents iteratively\ninteract with the environment, relabel past \"failed\" experiences with new\ngoals, and update their policies in a self-supervised loop. Active data\ngathering further improves combinatorial generalization, outperforming the best\nbaseline by 25.1%. Finally, we explain these results by investigating three\npossible factors underlying the effectiveness of the LM-based policy. We find\nthat sequential input representations (vs. fixed-dimensional feature vectors)\nand LM-based weight initialization are both important for generalization.\nSurprisingly, however, the format of the policy inputs encoding (e.g. as a\nnatural language string vs. an arbitrary sequential encoding) has little\ninfluence. Together, these results suggest that language modeling induces\nrepresentations that are useful for modeling not just language, but also goals\nand plans; these representations can aid learning and generalization even\noutside of language processing.", "collection_id": "e55b64f3ce56e414c2afb73554a4ae981750d9ee506e4c6b8347f9b2dc31c910"}, "question": "How much does active data gathering improve combinatorial generalization compared to the best baseline in machine learning problems?\n\n", "answer": "Active data gathering improves combinatorial generalization by 25.1% compared to the best baseline. This is achieved through an iterative process where agents interact with the environment, relabel past experiences with new goals, and update their policies in a self-supervised loop. This approach leverages pre-trained language models to scaffold learning and generalization in sequential decision-making problems, enabling effective combinatorial generalization across different environments and supervisory modalities."}
{"document": {"content": "As offensive content has become pervasive in social media, there has been\nmuch research in identifying potentially offensive messages. However, previous\nwork on this topic did not consider the problem as a whole, but rather focused\non detecting very specific types of offensive content, e.g., hate speech,\ncyberbulling, or cyber-aggression. In contrast, here we target several\ndifferent kinds of offensive content. In particular, we model the task\nhierarchically, identifying the type and the target of offensive messages in\nsocial media. For this purpose, we complied the Offensive Language\nIdentification Dataset (OLID), a new dataset with tweets annotated for\noffensive content using a fine-grained three-layer annotation scheme, which we\nmake publicly available. We discuss the main similarities and differences\nbetween OLID and pre-existing datasets for hate speech identification,\naggression detection, and similar tasks. We further experiment with and we\ncompare the performance of different machine learning models on OLID.", "collection_id": "9edc29566b962063fe5c7511f90ef5e96e264202a4dc92653baef49d01b04eee"}, "question": "What is the name of the dataset compiled for identifying offensive content in social media using a fine-grained three-layer annotation scheme?\n\n", "answer": "The dataset compiled for identifying offensive content in social media using a fine-grained three-layer annotation scheme is called the Offensive Language Identification Dataset (OLID)."}
{"document": {"content": "We show that the mutual information between two symbols, as a function of the\nnumber of symbols between the two, decays exponentially in any probabilistic\nregular grammar, but can decay like a power law for a context-free grammar.\nThis result about formal languages is closely related to a well-known result in\nclassical statistical mechanics that there are no phase transitions in\ndimensions fewer than two. It is also related to the emergence of power-law\ncorrelations in turbulence and cosmological inflation through recursive\ngenerative processes. We elucidate these physics connections and comment on\npotential applications of our results to machine learning tasks like training\nartificial recurrent neural networks. Along the way, we introduce a useful\nquantity which we dub the rational mutual information and discuss\ngeneralizations of our claims involving more complicated Bayesian networks.", "collection_id": "4d0fa898d40367b798e3a8d564c5569609889e7987aedaa358c259803efdd5fb"}, "question": "How does the mutual information between two symbols decay in probabilistic regular grammar versus context-free grammar?\n", "answer": "The mutual information between two symbols decays exponentially in any probabilistic regular grammar, but can decay like a power law for a context-free grammar. This difference in decay patterns highlights distinct characteristics of these two types of grammars in terms of how they structure and generate sequences of symbols."}
{"document": {"content": "Recent large language models often answer factual questions correctly. But\nusers can't trust any given claim a model makes without fact-checking, because\nlanguage models can hallucinate convincing nonsense. In this work we use\nreinforcement learning from human preferences (RLHP) to train \"open-book\" QA\nmodels that generate answers whilst also citing specific evidence for their\nclaims, which aids in the appraisal of correctness. Supporting evidence is\ndrawn from multiple documents found via a search engine, or from a single\nuser-provided document. Our 280 billion parameter model, GopherCite, is able to\nproduce answers with high quality supporting evidence and abstain from\nanswering when unsure. We measure the performance of GopherCite by conducting\nhuman evaluation of answers to questions in a subset of the NaturalQuestions\nand ELI5 datasets. The model's response is found to be high-quality 80\\% of the\ntime on this Natural Questions subset, and 67\\% of the time on the ELI5 subset.\nAbstaining from the third of questions for which it is most unsure improves\nperformance to 90\\% and 80\\% respectively, approaching human baselines.\nHowever, analysis on the adversarial TruthfulQA dataset shows why citation is\nonly one part of an overall strategy for safety and trustworthiness: not all\nclaims supported by evidence are true.", "collection_id": "0451cd5d8b8092d25f921b81facf5dbcfffe6ae913674205e5904a0c977aafa7"}, "question": "What percentage of the time does the GopherCite model produce high-quality responses to questions from the NaturalQuestions dataset subset?\n", "answer": "The GopherCite model produces high-quality responses 80% of the time to questions from the NaturalQuestions dataset subset. However, when the model abstains from answering questions for which it is most unsure, the performance improves to 90%."}
{"document": {"content": "The detection of offensive language in the context of a dialogue has become\nan increasingly important application of natural language processing. The\ndetection of trolls in public forums (Gal\\'an-Garc\\'ia et al., 2016), and the\ndeployment of chatbots in the public domain (Wolf et al., 2017) are two\nexamples that show the necessity of guarding against adversarially offensive\nbehavior on the part of humans. In this work, we develop a training scheme for\na model to become robust to such human attacks by an iterative build it, break\nit, fix it strategy with humans and models in the loop. In detailed experiments\nwe show this approach is considerably more robust than previous systems.\nFurther, we show that offensive language used within a conversation critically\ndepends on the dialogue context, and cannot be viewed as a single sentence\noffensive detection task as in most previous work. Our newly collected tasks\nand methods will be made open source and publicly available.", "collection_id": "b8d195faae037dd2c99413ffd53a6c6f333e8d4d9b15c0c2c97eb5fadf37e135"}, "question": "Why is detecting offensive language in dialogue important?\n", "answer": "Detecting offensive language in dialogue is important due to the increasing need to guard against adversarially offensive behavior on the part of humans, particularly in applications such as detecting trolls in public forums and deploying chatbots in the public domain."}
{"document": {"content": "Neural networks are both computationally intensive and memory intensive,\nmaking them difficult to deploy on embedded systems with limited hardware\nresources. To address this limitation, we introduce \"deep compression\", a three\nstage pipeline: pruning, trained quantization and Huffman coding, that work\ntogether to reduce the storage requirement of neural networks by 35x to 49x\nwithout affecting their accuracy. Our method first prunes the network by\nlearning only the important connections. Next, we quantize the weights to\nenforce weight sharing, finally, we apply Huffman coding. After the first two\nsteps we retrain the network to fine tune the remaining connections and the\nquantized centroids. Pruning, reduces the number of connections by 9x to 13x;\nQuantization then reduces the number of bits that represent each connection\nfrom 32 to 5. On the ImageNet dataset, our method reduced the storage required\nby AlexNet by 35x, from 240MB to 6.9MB, without loss of accuracy. Our method\nreduced the size of VGG-16 by 49x from 552MB to 11.3MB, again with no loss of\naccuracy. This allows fitting the model into on-chip SRAM cache rather than\noff-chip DRAM memory. Our compression method also facilitates the use of\ncomplex neural networks in mobile applications where application size and\ndownload bandwidth are constrained. Benchmarked on CPU, GPU and mobile GPU,\ncompressed network has 3x to 4x layerwise speedup and 3x to 7x better energy\nefficiency.", "collection_id": "9f24c6176d6c455ddebffe27f92ac1df67ca57de66c7ed555f67ac526c3d3208"}, "question": "How much does the deep compression method reduce the storage requirement of neural networks?\n", "answer": "The deep compression method reduces the storage requirement of neural networks by 35x to 49x without affecting their accuracy. This is achieved through a three-stage pipeline of pruning, trained quantization, and Huffman coding. For example, the method reduced the storage required by AlexNet by 35x, from 240MB to 6.9MB, and the size of VGG-16 by 49x, from 552MB to 11.3MB, both with no loss of accuracy."}
{"document": {"content": "The availability of large, high-quality datasets has been one of the main\ndrivers of recent progress in question answering (QA). Such annotated datasets\nhowever are difficult and costly to collect, and rarely exist in languages\nother than English, rendering QA technology inaccessible to underrepresented\nlanguages. An alternative to building large monolingual training datasets is to\nleverage pre-trained language models (PLMs) under a few-shot learning setting.\nOur approach, QAmeleon, uses a PLM to automatically generate multilingual data\nupon which QA models are trained, thus avoiding costly annotation. Prompt\ntuning the PLM for data synthesis with only five examples per language delivers\naccuracy superior to translation-based baselines, bridges nearly 60% of the gap\nbetween an English-only baseline and a fully supervised upper bound trained on\nalmost 50,000 hand labeled examples, and always leads to substantial\nimprovements compared to fine-tuning a QA model directly on labeled examples in\nlow resource settings. Experiments on the TyDiQA-GoldP and MLQA benchmarks show\nthat few-shot prompt tuning for data synthesis scales across languages and is a\nviable alternative to large-scale annotation.", "collection_id": "ea0f719e0ab8c56fb8f4b1e6c41be4b85a03ddcd87146b437b237b9382c9808f"}, "question": "What is QAmeleon and how does it improve question answering technology?\n", "answer": "QAmeleon is an approach that uses a pre-trained language model (PLM) to automatically generate multilingual data for training question answering (QA) models. This method avoids costly annotation and leverages few-shot learning to deliver accuracy superior to translation-based baselines. By prompt tuning the PLM with only five examples per language, QAmeleon bridges nearly 60% of the gap between an English-only baseline and a fully supervised upper bound, and leads to substantial improvements in low-resource settings."}
{"document": {"content": "Generating sound effects that humans want is an important topic. However,\nthere are few studies in this area for sound generation. In this study, we\ninvestigate generating sound conditioned on a text prompt and propose a novel\ntext-to-sound generation framework that consists of a text encoder, a Vector\nQuantized Variational Autoencoder (VQ-VAE), a decoder, and a vocoder. The\nframework first uses the decoder to transfer the text features extracted from\nthe text encoder to a mel-spectrogram with the help of VQ-VAE, and then the\nvocoder is used to transform the generated mel-spectrogram into a waveform. We\nfound that the decoder significantly influences the generation performance.\nThus, we focus on designing a good decoder in this study. We begin with the\ntraditional autoregressive decoder, which has been proved as a state-of-the-art\nmethod in previous sound generation works. However, the AR decoder always\npredicts the mel-spectrogram tokens one by one in order, which introduces the\nunidirectional bias and accumulation of errors problems. Moreover, with the AR\ndecoder, the sound generation time increases linearly with the sound duration.\nTo overcome the shortcomings introduced by AR decoders, we propose a\nnon-autoregressive decoder based on the discrete diffusion model, named\nDiffsound. Specifically, the Diffsound predicts all of the mel-spectrogram\ntokens in one step and then refines the predicted tokens in the next step, so\nthe best-predicted results can be obtained after several steps. Our experiments\nshow that our proposed Diffsound not only produces better text-to-sound\ngeneration results when compared with the AR decoder but also has a faster\ngeneration speed, e.g., MOS: 3.56 \\textit{v.s} 2.786, and the generation speed\nis five times faster than the AR decoder.", "collection_id": "33830448b14cb26827dd0c1a9a8a5db52a3b701bcfdc0639a6c4ac236f0c5f8e"}, "question": "What is the name of the proposed non-autoregressive decoder based on the discrete diffusion model for text-to-sound generation?\n\n", "answer": "The proposed non-autoregressive decoder based on the discrete diffusion model is named Diffsound. It predicts all mel-spectrogram tokens in one step and refines the predicted tokens in subsequent steps to achieve the best results."}
{"document": {"content": "Instruction tuning enables pretrained language models to perform new tasks\nfrom inference-time natural language descriptions. These approaches rely on\nvast amounts of human supervision in the form of crowdsourced datasets or user\ninteractions. In this work, we introduce Unnatural Instructions: a large\ndataset of creative and diverse instructions, collected with virtually no human\nlabor. We collect 64,000 examples by prompting a language model with three seed\nexamples of instructions and eliciting a fourth. This set is then expanded by\nprompting the model to rephrase each instruction, creating a total of\napproximately 240,000 examples of instructions, inputs, and outputs.\nExperiments show that despite containing a fair amount of noise, training on\nUnnatural Instructions rivals the effectiveness of training on open-source\nmanually-curated datasets, surpassing the performance of models such as T0++\nand Tk-Instruct across various benchmarks. These results demonstrate the\npotential of model-generated data as a cost-effective alternative to\ncrowdsourcing for dataset expansion and diversification.", "collection_id": "e964087e1910ae16c1ca7b3ba74b7692c211ad9d37290e70836c501a806f16e5"}, "question": "How many initial examples of instructions were used to prompt a language model for collecting the Unnatural Instructions dataset?\n", "answer": "Three seed examples of instructions were used to prompt a language model, eliciting a fourth example, and ultimately leading to the collection of the Unnatural Instructions dataset."}
{"document": {"content": "Given the broad capabilities of large language models, it should be possible\nto work towards a general-purpose, text-based assistant that is aligned with\nhuman values, meaning that it is helpful, honest, and harmless. As an initial\nforay in this direction we study simple baseline techniques and evaluations,\nsuch as prompting. We find that the benefits from modest interventions increase\nwith model size, generalize to a variety of alignment evaluations, and do not\ncompromise the performance of large models. Next we investigate scaling trends\nfor several training objectives relevant to alignment, comparing imitation\nlearning, binary discrimination, and ranked preference modeling. We find that\nranked preference modeling performs much better than imitation learning, and\noften scales more favorably with model size. In contrast, binary discrimination\ntypically performs and scales very similarly to imitation learning. Finally we\nstudy a `preference model pre-training' stage of training, with the goal of\nimproving sample efficiency when finetuning on human preferences.", "collection_id": "400555cdeb7612b4c3327ac83da33f8b231e58dd78e88d8af3cbca2223030f5a"}, "question": "What are the key characteristics of a general-purpose, text-based assistant aligned with human values?\n\n", "answer": "A general-purpose, text-based assistant aligned with human values should be helpful, honest, and harmless. These characteristics are considered essential for ensuring that the assistant is beneficial and trustworthy for users."}
{"document": {"content": "Recent progress in hardware and methodology for training neural networks has\nushered in a new generation of large networks trained on abundant data. These\nmodels have obtained notable gains in accuracy across many NLP tasks. However,\nthese accuracy improvements depend on the availability of exceptionally large\ncomputational resources that necessitate similarly substantial energy\nconsumption. As a result these models are costly to train and develop, both\nfinancially, due to the cost of hardware and electricity or cloud compute time,\nand environmentally, due to the carbon footprint required to fuel modern tensor\nprocessing hardware. In this paper we bring this issue to the attention of NLP\nresearchers by quantifying the approximate financial and environmental costs of\ntraining a variety of recently successful neural network models for NLP. Based\non these findings, we propose actionable recommendations to reduce costs and\nimprove equity in NLP research and practice.", "collection_id": "534ecede101b6cb78b13fcf0af774a47f87d60aa004807f6ba574afa27ab5936"}, "question": "What are the costs associated with training large neural network models for NLP tasks?\n\n", "answer": "The costs associated with training large neural network models for NLP tasks are both financial and environmental. Financially, the costs include the expense of hardware and electricity or cloud compute time. Environmentally, the costs are due to the substantial carbon footprint required to fuel modern tensor processing hardware."}
{"document": {"content": "We analyze the growth of dataset sizes used in machine learning for natural\nlanguage processing and computer vision, and extrapolate these using two\nmethods; using the historical growth rate and estimating the compute-optimal\ndataset size for future predicted compute budgets. We investigate the growth in\ndata usage by estimating the total stock of unlabeled data available on the\ninternet over the coming decades. Our analysis indicates that the stock of\nhigh-quality language data will be exhausted soon; likely before 2026. By\ncontrast, the stock of low-quality language data and image data will be\nexhausted only much later; between 2030 and 2050 (for low-quality language) and\nbetween 2030 and 2060 (for images). Our work suggests that the current trend of\never-growing ML models that rely on enormous datasets might slow down if data\nefficiency is not drastically improved or new sources of data become available.", "collection_id": "15fe4217e9e8a51a59a622a70c34c6a2e2a6923174c4612c6cb541894b13406a"}, "question": "When is the stock of high-quality language data expected to be exhausted?\n", "answer": "The stock of high-quality language data is expected to be exhausted soon, likely before 2026."}
{"document": {"content": "From an environmental standpoint, there are a few crucial aspects of training\na neural network that have a major impact on the quantity of carbon that it\nemits. These factors include: the location of the server used for training and\nthe energy grid that it uses, the length of the training procedure, and even\nthe make and model of hardware on which the training takes place. In order to\napproximate these emissions, we present our Machine Learning Emissions\nCalculator, a tool for our community to better understand the environmental\nimpact of training ML models. We accompany this tool with an explanation of the\nfactors cited above, as well as concrete actions that individual practitioners\nand organizations can take to mitigate their carbon emissions.", "collection_id": "25ff5c786f456dde1f6a62acd42c091b59c174e425df598539a77c4b9aa1f564"}, "question": "What factors contribute to the environmental impact of training a neural network?\n", "answer": "The factors that contribute to the environmental impact of training a neural network include the location of the server used for training and the energy grid that it uses, the length of the training procedure, and the make and model of hardware on which the training takes place."}
{"document": {"content": "It has become common to publish large (billion parameter) language models\nthat have been trained on private datasets. This paper demonstrates that in\nsuch settings, an adversary can perform a training data extraction attack to\nrecover individual training examples by querying the language model.\n  We demonstrate our attack on GPT-2, a language model trained on scrapes of\nthe public Internet, and are able to extract hundreds of verbatim text\nsequences from the model's training data. These extracted examples include\n(public) personally identifiable information (names, phone numbers, and email\naddresses), IRC conversations, code, and 128-bit UUIDs. Our attack is possible\neven though each of the above sequences are included in just one document in\nthe training data.\n  We comprehensively evaluate our extraction attack to understand the factors\nthat contribute to its success. Worryingly, we find that larger models are more\nvulnerable than smaller models. We conclude by drawing lessons and discussing\npossible safeguards for training large language models.", "collection_id": "e11baec9f89303e91ef180eca7f0a77b03d888646b706395338cb244f5e0d6eb"}, "question": "What type of information can be extracted from a language model through a training data extraction attack?\n", "answer": "A training data extraction attack on a language model can extract hundreds of verbatim text sequences from the model's training data, including personally identifiable information (such as names, phone numbers, and email addresses), IRC conversations, code, and 128-bit UUIDs."}
{"document": {"content": "Advances in language modeling architectures and the availability of large\ntext corpora have driven progress in automatic text generation. While this\nresults in models capable of generating coherent texts, it also prompts models\nto internalize social biases present in the training corpus. This paper aims to\nquantify and reduce a particular type of bias exhibited by language models:\nbias in the sentiment of generated text. Given a conditioning context (e.g., a\nwriting prompt) and a language model, we analyze if (and how) the sentiment of\nthe generated text is affected by changes in values of sensitive attributes\n(e.g., country names, occupations, genders) in the conditioning context using a\nform of counterfactual evaluation. We quantify sentiment bias by adopting\nindividual and group fairness metrics from the fair machine learning\nliterature, and demonstrate that large-scale models trained on two different\ncorpora (news articles, and Wikipedia) exhibit considerable levels of bias. We\nthen propose embedding and sentiment prediction-derived regularization on the\nlanguage model's latent representations. The regularizations improve fairness\nmetrics while retaining comparable levels of perplexity and semantic\nsimilarity.", "collection_id": "3581ca4790aeae49ecacebd249c1be6c7f543075855dd7803bd43ed8214e7d4d"}, "question": "What are the sources of the large-scale models' training corpora that exhibit considerable levels of bias?\n\n", "answer": "The sources of the large-scale models' training corpora that exhibit considerable levels of bias are news articles and Wikipedia."}
{"document": {"content": "Natural Language Inference (NLI) models are known to learn from biases and\nartefacts within their training data, impacting how well they generalise to\nother unseen datasets. Existing de-biasing approaches focus on preventing the\nmodels from learning these biases, which can result in restrictive models and\nlower performance. We instead investigate teaching the model how a human would\napproach the NLI task, in order to learn features that will generalise better\nto previously unseen examples. Using natural language explanations, we\nsupervise the model's attention weights to encourage more attention to be paid\nto the words present in the explanations, significantly improving model\nperformance. Our experiments show that the in-distribution improvements of this\nmethod are also accompanied by out-of-distribution improvements, with the\nsupervised models learning from features that generalise better to other NLI\ndatasets. Analysis of the model indicates that human explanations encourage\nincreased attention on the important words, with more attention paid to words\nin the premise and less attention paid to punctuation and stop-words.", "collection_id": "5d8efa87bd589dd788159da659e1fa8e1837ad5492ee134f8d5afcdc9e1e038c"}, "question": "How do Natural Language Inference models typically learn from their training data?\n", "answer": "Natural Language Inference (NLI) models are known to learn from biases and artefacts within their training data, which can negatively impact how well they generalize to other unseen datasets."}
{"document": {"content": "We study empirical scaling laws for language model performance on the\ncross-entropy loss. The loss scales as a power-law with model size, dataset\nsize, and the amount of compute used for training, with some trends spanning\nmore than seven orders of magnitude. Other architectural details such as\nnetwork width or depth have minimal effects within a wide range. Simple\nequations govern the dependence of overfitting on model/dataset size and the\ndependence of training speed on model size. These relationships allow us to\ndetermine the optimal allocation of a fixed compute budget. Larger models are\nsignificantly more sample-efficient, such that optimally compute-efficient\ntraining involves training very large models on a relatively modest amount of\ndata and stopping significantly before convergence.", "collection_id": "c190d056c7934f2f017f07d2d9f057766d85907e2712c2b8402ef3c5e70d2784"}, "question": "How does the loss scale with model size, dataset size, and compute used for training in language models?\n", "answer": "The loss in language models scales as a power-law with model size, dataset size, and the amount of compute used for training, with some trends spanning more than seven orders of magnitude."}
{"document": {"content": "By providing unprecedented access to computational resources, cloud computing\nhas enabled rapid growth in technologies such as machine learning, the\ncomputational demands of which incur a high energy cost and a commensurate\ncarbon footprint. As a result, recent scholarship has called for better\nestimates of the greenhouse gas impact of AI: data scientists today do not have\neasy or reliable access to measurements of this information, precluding\ndevelopment of actionable tactics. Cloud providers presenting information about\nsoftware carbon intensity to users is a fundamental stepping stone towards\nminimizing emissions. In this paper, we provide a framework for measuring\nsoftware carbon intensity, and propose to measure operational carbon emissions\nby using location-based and time-specific marginal emissions data per energy\nunit. We provide measurements of operational software carbon intensity for a\nset of modern models for natural language processing and computer vision, and a\nwide range of model sizes, including pretraining of a 6.1 billion parameter\nlanguage model. We then evaluate a suite of approaches for reducing emissions\non the Microsoft Azure cloud compute platform: using cloud instances in\ndifferent geographic regions, using cloud instances at different times of day,\nand dynamically pausing cloud instances when the marginal carbon intensity is\nabove a certain threshold. We confirm previous results that the geographic\nregion of the data center plays a significant role in the carbon intensity for\na given cloud instance, and find that choosing an appropriate region can have\nthe largest operational emissions reduction impact. We also show that the time\nof day has notable impact on operational software carbon intensity. Finally, we\nconclude with recommendations for how machine learning practitioners can use\nsoftware carbon intensity information to reduce environmental impact.", "collection_id": "9b9e32aecfe081582900263a3b50191508f8b476b1c360fbeca48fd6ac8445b3"}, "question": "How can machine learning practitioners reduce the environmental impact of their work?\n\n", "answer": "Machine learning practitioners can reduce the environmental impact of their work by choosing cloud instances in geographic regions with lower carbon intensity, using cloud instances at times of day with lower carbon intensity, and dynamically pausing cloud instances when the marginal carbon intensity is above a certain threshold. Additionally, having access to reliable measurements of software carbon intensity can help practitioners develop actionable tactics to minimize emissions."}
{"document": {"content": "Math word problem (MWP) is a challenging and critical task in natural\nlanguage processing. Many recent studies formalize MWP as a generation task and\nhave adopted sequence-to-sequence models to transform problem descriptions to\nmathematical expressions. However, mathematical expressions are prone to minor\nmistakes while the generation objective does not explicitly handle such\nmistakes. To address this limitation, we devise a new ranking task for MWP and\npropose Generate & Rank, a multi-task framework based on a generative\npre-trained language model. By joint training with generation and ranking, the\nmodel learns from its own mistakes and is able to distinguish between correct\nand incorrect expressions. Meanwhile, we perform tree-based disturbance\nspecially designed for MWP and an online update to boost the ranker. We\ndemonstrate the effectiveness of our proposed method on the benchmark and the\nresults show that our method consistently outperforms baselines in all\ndatasets. Particularly, in the classical Math23k, our method is 7% (78.4%\n$\\rightarrow$ 85.4%) higher than the state-of-the-art.", "collection_id": "ecf9be4bd9d49a5bdc3eb81dd527adc95b2f444aedab4e9fffc31319aac70324"}, "question": "What percentage improvement did the proposed method achieve over the state-of-the-art in the Math23k dataset?\n", "answer": "The proposed method achieved a 7% improvement over the state-of-the-art in the Math23k dataset, increasing the performance from 78.4% to 85.4%."}
{"document": {"content": "Programming is a powerful and ubiquitous problem-solving tool. Developing\nsystems that can assist programmers or even generate programs independently\ncould make programming more productive and accessible, yet so far incorporating\ninnovations in AI has proven challenging. Recent large-scale language models\nhave demonstrated an impressive ability to generate code, and are now able to\ncomplete simple programming tasks. However, these models still perform poorly\nwhen evaluated on more complex, unseen problems that require problem-solving\nskills beyond simply translating instructions into code. For example,\ncompetitive programming problems which require an understanding of algorithms\nand complex natural language remain extremely challenging. To address this gap,\nwe introduce AlphaCode, a system for code generation that can create novel\nsolutions to these problems that require deeper reasoning. In simulated\nevaluations on recent programming competitions on the Codeforces platform,\nAlphaCode achieved on average a ranking of top 54.3% in competitions with more\nthan 5,000 participants. We found that three key components were critical to\nachieve good and reliable performance: (1) an extensive and clean competitive\nprogramming dataset for training and evaluation, (2) large and\nefficient-to-sample transformer-based architectures, and (3) large-scale model\nsampling to explore the search space, followed by filtering based on program\nbehavior to a small set of submissions.", "collection_id": "c75f949b07ea1991c2339011f4d40ea91cc210651fe484e5c267825a267a150f"}, "question": "What ranking did AlphaCode achieve in simulated evaluations on recent programming competitions on the Codeforces platform?\n", "answer": "AlphaCode achieved an average ranking of top 54.3% in competitions with more than 5,000 participants on the Codeforces platform. This ranking showcases the system's ability to create novel solutions to complex programming problems that require deeper reasoning."}
{"document": {"content": "Meta-training, which fine-tunes the language model (LM) on various downstream\ntasks by maximizing the likelihood of the target label given the task\ninstruction and input instance, has improved the zero-shot task generalization\nperformance. However, meta-trained LMs still struggle to generalize to\nchallenging tasks containing novel labels unseen during meta-training. In this\npaper, we propose Flipped Learning, an alternative method of meta-training\nwhich trains the LM to generate the task instruction given the input instance\nand label. During inference, the LM trained with Flipped Learning, referred to\nas Flipped, selects the label option that is most likely to generate the task\ninstruction. On 14 tasks of the BIG-bench benchmark, the 11B-sized Flipped\noutperforms zero-shot T0-11B and even a 16 times larger 3-shot GPT-3 (175B) on\naverage by 8.4% and 9.7% points, respectively. Flipped gives particularly large\nimprovements on tasks with unseen labels, outperforming T0-11B by up to +20%\naverage F1 score. This indicates that the strong task generalization of Flipped\ncomes from improved generalization to novel labels. We release our code at\nhttps://github.com/seonghyeonye/Flipped-Learning.", "collection_id": "0a832c6fd7c83f3519eaf238ccd40a0b7805b476ea18f984b26258fe7870d7f8"}, "question": "What is the average improvement of Flipped over zero-shot T0-11B on 14 tasks of the BIG-bench benchmark?\n\n", "answer": "Flipped outperforms zero-shot T0-11B by an average of 8.4% points on 14 tasks of the BIG-bench benchmark."}
{"document": {"content": "It has been observed that large-scale language models capture undesirable\nsocietal biases, e.g. relating to race and gender; yet religious bias has been\nrelatively unexplored. We demonstrate that GPT-3, a state-of-the-art contextual\nlanguage model, captures persistent Muslim-violence bias. We probe GPT-3 in\nvarious ways, including prompt completion, analogical reasoning, and story\ngeneration, to understand this anti-Muslim bias, demonstrating that it appears\nconsistently and creatively in different uses of the model and that it is\nsevere even compared to biases about other religious groups. For instance,\n\"Muslim\" is analogized to \"terrorist\" in 23% of test cases, while \"Jewish\" is\nmapped to \"money\" in 5% of test cases. We quantify the positive distraction\nneeded to overcome this bias with adversarial text prompts, and find that use\nof the most positive 6 adjectives reduces violent completions for \"Muslims\"\nfrom 66% to 20%, but which is still higher than for other religious groups.", "collection_id": "e6ff85509a08a843bb79215ad21f7492de8b86e34abbf21a5be64a9d2e85086c"}, "question": "What percentage of test cases associate the term \"Muslim\" with \"terrorist\" in a language model?\n\n", "answer": "In a study examining anti-Muslim bias in a state-of-the-art language model, GPT-3, it was found that the term \"Muslim\" is associated with \"terrorist\" in 23% of test cases. This was discovered through various methods, including prompt completion, analogical reasoning, and story generation, highlighting the persistent Muslim-violence bias present in the model."}
{"document": {"content": "Driven by the goal of eradicating language barriers on a global scale,\nmachine translation has solidified itself as a key focus of artificial\nintelligence research today. However, such efforts have coalesced around a\nsmall subset of languages, leaving behind the vast majority of mostly\nlow-resource languages. What does it take to break the 200 language barrier\nwhile ensuring safe, high quality results, all while keeping ethical\nconsiderations in mind? In No Language Left Behind, we took on this challenge\nby first contextualizing the need for low-resource language translation support\nthrough exploratory interviews with native speakers. Then, we created datasets\nand models aimed at narrowing the performance gap between low and high-resource\nlanguages. More specifically, we developed a conditional compute model based on\nSparsely Gated Mixture of Experts that is trained on data obtained with novel\nand effective data mining techniques tailored for low-resource languages. We\npropose multiple architectural and training improvements to counteract\noverfitting while training on thousands of tasks. Critically, we evaluated the\nperformance of over 40,000 different translation directions using a\nhuman-translated benchmark, Flores-200, and combined human evaluation with a\nnovel toxicity benchmark covering all languages in Flores-200 to assess\ntranslation safety. Our model achieves an improvement of 44% BLEU relative to\nthe previous state-of-the-art, laying important groundwork towards realizing a\nuniversal translation system. Finally, we open source all contributions\ndescribed in this work, accessible at\nhttps://github.com/facebookresearch/fairseq/tree/nllb.", "collection_id": "d63d0e00c191026ddec68849dfceb613c37120e443706026af7869e09dc2a858"}, "question": "What is the percentage of improvement in BLEU achieved by the No Language Left Behind model compared to the previous state-of-the-art?\n\n", "answer": "The No Language Left Behind model achieves an improvement of 44% BLEU relative to the previous state-of-the-art in machine translation, marking a significant step towards realizing a universal translation system."}
{"document": {"content": "Recent work has shown that fine-tuning large pre-trained language models on a\ncollection of tasks described via instructions, a.k.a. instruction-tuning,\nimproves their zero and few-shot generalization to unseen tasks. However, there\nis a limited understanding of the performance trade-offs of different decisions\nmade during the instruction-tuning process. These decisions include the scale\nand diversity of the instruction-tuning benchmark, different task sampling\nstrategies, fine-tuning with and without demonstrations, training using\nspecialized datasets for reasoning and dialogue, and finally, the fine-tuning\nobjectives themselves. In this paper, we characterize the effect of\ninstruction-tuning decisions on downstream task performance when scaling both\nmodel and benchmark sizes. To this end, we create OPT-IML Bench: a large\nbenchmark for Instruction Meta-Learning (IML) of 2000 NLP tasks consolidated\ninto task categories from 8 existing benchmarks, and prepare an evaluation\nframework to measure three types of model generalizations: to tasks from fully\nheld-out categories, to held-out tasks from seen categories, and to held-out\ninstances from seen tasks. Through the lens of this framework, we first present\ninsights about instruction-tuning decisions as applied to OPT-30B and further\nexploit these insights to train OPT-IML 30B and 175B, which are\ninstruction-tuned versions of OPT. OPT-IML demonstrates all three\ngeneralization abilities at both scales on four different evaluation benchmarks\nwith diverse tasks and input formats -- PromptSource, FLAN,\nSuper-NaturalInstructions, and UnifiedSKG. Not only does it significantly\noutperform OPT on all benchmarks but is also highly competitive with existing\nmodels fine-tuned on each specific benchmark. We release OPT-IML at both\nscales, together with the OPT-IML Bench evaluation framework.", "collection_id": "f8fcc896c664af16bf323bc51ad6ede5feadf650ceb9d521160d7ff0fce061d6"}, "question": "What is OPT-IML Bench and what is its purpose in natural language processing research?\n\n", "answer": "OPT-IML Bench is a large benchmark for Instruction Meta-Learning (IML) consisting of 2000 NLP tasks consolidated into task categories from 8 existing benchmarks. Its purpose is to provide a comprehensive evaluation framework to measure the performance of large pre-trained language models fine-tuned on a collection of tasks described via instructions, and to characterize the effect of instruction-tuning decisions on downstream task performance when scaling both model and benchmark sizes."}
{"document": {"content": "In this work, we aim at equipping pre-trained language models with structured\nknowledge. We present two self-supervised tasks learning over raw text with the\nguidance from knowledge graphs. Building upon entity-level masked language\nmodels, our first contribution is an entity masking scheme that exploits\nrelational knowledge underlying the text. This is fulfilled by using a linked\nknowledge graph to select informative entities and then masking their mentions.\nIn addition we use knowledge graphs to obtain distractors for the masked\nentities, and propose a novel distractor-suppressed ranking objective which is\noptimized jointly with masked language model. In contrast to existing\nparadigms, our approach uses knowledge graphs implicitly, only during\npre-training, to inject language models with structured knowledge via learning\nfrom raw text. It is more efficient than retrieval-based methods that perform\nentity linking and integration during finetuning and inference, and generalizes\nmore effectively than the methods that directly learn from concatenated graph\ntriples. Experiments show that our proposed model achieves improved performance\non five benchmark datasets, including question answering and knowledge base\ncompletion tasks.", "collection_id": "e65836bf9675c33eb8c757e89c3c6559b3fad1e549959913363a780e9c12731b"}, "question": "How do the proposed self-supervised tasks utilize knowledge graphs for pre-training language models?\n\n", "answer": "The proposed self-supervised tasks use knowledge graphs to select informative entities for masking, obtain distractors for the masked entities, and optimize a distractor-suppressed ranking objective jointly with a masked language model. This approach injects language models with structured knowledge via learning from raw text, without requiring explicit entity linking and integration during finetuning and inference."}
{"document": {"content": "Large language models (LM) generate remarkably fluent text and can be\nefficiently adapted across NLP tasks. Measuring and guaranteeing the quality of\ngenerated text in terms of safety is imperative for deploying LMs in the real\nworld; to this end, prior work often relies on automatic evaluation of LM\ntoxicity. We critically discuss this approach, evaluate several toxicity\nmitigation strategies with respect to both automatic and human evaluation, and\nanalyze consequences of toxicity mitigation in terms of model bias and LM\nquality. We demonstrate that while basic intervention strategies can\neffectively optimize previously established automatic metrics on the\nRealToxicityPrompts dataset, this comes at the cost of reduced LM coverage for\nboth texts about, and dialects of, marginalized groups. Additionally, we find\nthat human raters often disagree with high automatic toxicity scores after\nstrong toxicity reduction interventions -- highlighting further the nuances\ninvolved in careful evaluation of LM toxicity.", "collection_id": "b515c171d0770ce66c5e26b7aba477f5209484d6c57a9a26aa6e68d7e4c18e93"}, "question": "What are the consequences of implementing toxicity mitigation strategies in large language models?\n\n", "answer": "Implementing toxicity mitigation strategies in large language models can lead to reduced model coverage for texts about, and dialects of, marginalized groups. This means that the models may become less effective at generating or understanding content related to these groups, potentially exacerbating existing biases. Additionally, strong toxicity reduction interventions can result in discrepancies between human evaluations and automatic toxicity scores, highlighting the complexities involved in evaluating language model toxicity."}
{"document": {"content": "Large Language Models (LMs) are known to encode world knowledge in their\nparameters as they pretrain on a vast amount of web corpus, which is often\nutilized for performing knowledge-dependent downstream tasks such as question\nanswering, fact-checking, and open dialogue. In real-world scenarios, the world\nknowledge stored in the LMs can quickly become outdated as the world changes,\nbut it is non-trivial to avoid catastrophic forgetting and reliably acquire new\nknowledge while preserving invariant knowledge. To push the community towards\nbetter maintenance of ever-changing LMs, we formulate a new continual learning\n(CL) problem called Continual Knowledge Learning (CKL). We construct a new\nbenchmark and metric to quantify the retention of time-invariant world\nknowledge, the update of outdated knowledge, and the acquisition of new\nknowledge. We adopt applicable recent methods from literature to create several\nstrong baselines. Through extensive experiments, we find that CKL exhibits\nunique challenges that are not addressed in previous CL setups, where parameter\nexpansion is necessary to reliably retain and learn knowledge simultaneously.\nBy highlighting the critical causes of knowledge forgetting, we show that CKL\nis a challenging and important problem that helps us better understand and\ntrain ever-changing LMs. The benchmark datasets, evaluation script, and\nbaseline code to reproduce our results are available at\nhttps://github.com/joeljang/continual-knowledge-learning.", "collection_id": "bcc47274adab477696bdc0c84c61d1d840ac9d192953b8f2182075b33248d0a4"}, "question": "What is Continual Knowledge Learning (CKL) in the context of Large Language Models (LMs)?\n", "answer": "Continual Knowledge Learning (CKL) is a new continual learning problem that aims to address the challenge of maintaining and updating the world knowledge stored in Large Language Models (LMs) as the world changes. It involves the retention of time-invariant world knowledge, the update of outdated knowledge, and the acquisition of new knowledge, with a focus on avoiding catastrophic forgetting and reliably acquiring new knowledge while preserving invariant knowledge."}
{"document": {"content": "The dependency of the generalization error of neural networks on model and\ndataset size is of critical importance both in practice and for understanding\nthe theory of neural networks. Nevertheless, the functional form of this\ndependency remains elusive. In this work, we present a functional form which\napproximates well the generalization error in practice. Capitalizing on the\nsuccessful concept of model scaling (e.g., width, depth), we are able to\nsimultaneously construct such a form and specify the exact models which can\nattain it across model/data scales. Our construction follows insights obtained\nfrom observations conducted over a range of model/data scales, in various model\ntypes and datasets, in vision and language tasks. We show that the form both\nfits the observations well across scales, and provides accurate predictions\nfrom small- to large-scale models and data.", "collection_id": "c295852e493d4a91300998be13a18ffebd5c0a5835d985a39b949d76cd444844"}, "question": "What is the main goal of understanding the dependency of generalization error of neural networks on model and dataset size?\n\n", "answer": "The main goal of understanding the dependency of generalization error of neural networks on model and dataset size is to determine a functional form that approximates this dependency well in practice, which is crucial for both practical applications and theoretical understanding of neural networks."}
{"document": {"content": "Toxic language detection systems often falsely flag text that contains\nminority group mentions as toxic, as those groups are often the targets of\nonline hate. Such over-reliance on spurious correlations also causes systems to\nstruggle with detecting implicitly toxic language. To help mitigate these\nissues, we create ToxiGen, a new large-scale and machine-generated dataset of\n274k toxic and benign statements about 13 minority groups. We develop a\ndemonstration-based prompting framework and an adversarial\nclassifier-in-the-loop decoding method to generate subtly toxic and benign text\nwith a massive pretrained language model. Controlling machine generation in\nthis way allows ToxiGen to cover implicitly toxic text at a larger scale, and\nabout more demographic groups, than previous resources of human-written text.\nWe conduct a human evaluation on a challenging subset of ToxiGen and find that\nannotators struggle to distinguish machine-generated text from human-written\nlanguage. We also find that 94.5% of toxic examples are labeled as hate speech\nby human annotators. Using three publicly-available datasets, we show that\nfinetuning a toxicity classifier on our data improves its performance on\nhuman-written data substantially. We also demonstrate that ToxiGen can be used\nto fight machine-generated toxicity as finetuning improves the classifier\nsignificantly on our evaluation subset. Our code and data can be found at\nhttps://github.com/microsoft/ToxiGen.", "collection_id": "138efbfa6285fc8bb32df845bb9b8a50b0bfc3a7ae4fd21fdfc88550418ccf5c"}, "question": "What percentage of toxic examples in ToxiGen are labeled as hate speech by human annotators?\n", "answer": "94.5% of toxic examples in ToxiGen are labeled as hate speech by human annotators, indicating that the dataset effectively captures hate speech and can be used to improve toxicity classifiers."}
{"document": {"content": "In this work, we aim to capitalize on the unique few-shot capabilities of\nlarge-scale language models (LSLMs) to overcome some of their challenges with\nrespect to grounding to factual and up-to-date information. Motivated by\nsemi-parametric language models (LMs), which ground their decisions in external\nretrieved evidence, we use few-shot prompting to learn to condition LMs on\ninformation returned from the web using Google Search, a broad and constantly\nupdated knowledge source. Our approach does not involve fine-tuning or learning\nadditional parameters, thus making it applicable to any LM, offering therefore\na strong baseline. Indeed, we find that LMs conditioned on the web surpass\nperformance of closed-book models of similar, or even larger, model sizes in\nopen-domain question answering. Finally, we find that increasing the\ninference-time compute of models, achieved via using multiple retrieved\nevidences to generate multiple answers followed by a reranking stage that uses\nscores generated by the same LMs, leads to better performance and alleviates\nlower performance of smaller few-shot LMs. All in all, our findings suggest\nthat it might be beneficial to slow down the race towards the biggest model and\ninstead shift attention towards finding more effective ways to use models,\nincluding but not limited to, better prompting or increasing inference-time\ncompute.", "collection_id": "32d7603b750980c9ca59b95807f93c264c35a8c94d912f1010c54ff1ec97cd6a"}, "question": "How can the performance of smaller few-shot language models be improved in open-domain question answering?\n", "answer": "The performance of smaller few-shot language models can be improved in open-domain question answering by increasing the inference-time compute of models. This can be achieved by using multiple retrieved evidences to generate multiple answers, followed by a reranking stage that uses scores generated by the same language models. This approach leads to better performance and alleviates the lower performance of smaller few-shot language models."}
{"document": {"content": "Learning to autonomously navigate the web is a difficult sequential decision\nmaking task. The state and action spaces are large and combinatorial in nature,\nand websites are dynamic environments consisting of several pages. One of the\nbottlenecks of training web navigation agents is providing a learnable\ncurriculum of training environments that can cover the large variety of\nreal-world websites. Therefore, we propose using Adversarial Environment\nGeneration (AEG) to generate challenging web environments in which to train\nreinforcement learning (RL) agents. We provide a new benchmarking environment,\ngMiniWoB, which enables an RL adversary to use compositional primitives to\nlearn to generate arbitrarily complex websites. To train the adversary, we\npropose a new technique for maximizing regret using the difference in the\nscores obtained by a pair of navigator agents. Our results show that our\napproach significantly outperforms prior methods for minimax regret AEG. The\nregret objective trains the adversary to design a curriculum of environments\nthat are \"just-the-right-challenge\" for the navigator agents; our results show\nthat over time, the adversary learns to generate increasingly complex web\nnavigation tasks. The navigator agents trained with our technique learn to\ncomplete challenging, high-dimensional web navigation tasks, such as form\nfilling, booking a flight etc. We show that the navigator agent trained with\nour proposed Flexible b-PAIRED technique significantly outperforms competitive\nautomatic curriculum generation baselines -- including a state-of-the-art RL\nweb navigation approach -- on a set of challenging unseen test environments,\nand achieves more than 80% success rate on some tasks.", "collection_id": "f8975a787cce47bfd635662c0f939ed922177f482287186c72d4fd627c6d2070"}, "question": "What is Adversarial Environment Generation (AEG) used for in the context of web navigation agents?\n\n", "answer": "Adversarial Environment Generation (AEG) is used to generate challenging web environments in which to train reinforcement learning (RL) agents, with the goal of providing a learnable curriculum of training environments that can cover the large variety of real-world websites. This approach enables the training of navigator agents to complete complex web navigation tasks, such as form filling and booking a flight."}
{"document": {"content": "Evaluating the general abilities of foundation models to tackle human-level\ntasks is a vital aspect of their development and application in the pursuit of\nArtificial General Intelligence (AGI). Traditional benchmarks, which rely on\nartificial datasets, may not accurately represent human-level capabilities. In\nthis paper, we introduce AGIEval, a novel benchmark specifically designed to\nassess foundation model in the context of human-centric standardized exams,\nsuch as college entrance exams, law school admission tests, math competitions,\nand lawyer qualification tests. We evaluate several state-of-the-art foundation\nmodels, including GPT-4, ChatGPT, and Text-Davinci-003, using this benchmark.\nImpressively, GPT-4 surpasses average human performance on SAT, LSAT, and math\ncompetitions, attaining a 95% accuracy rate on the SAT Math test and a 92.5%\naccuracy on the English test of the Chinese national college entrance exam.\nThis demonstrates the extraordinary performance of contemporary foundation\nmodels. In contrast, we also find that GPT-4 is less proficient in tasks that\nrequire complex reasoning or specific domain knowledge. Our comprehensive\nanalyses of model capabilities (understanding, knowledge, reasoning, and\ncalculation) reveal these models' strengths and limitations, providing valuable\ninsights into future directions for enhancing their general capabilities. By\nconcentrating on tasks pertinent to human cognition and decision-making, our\nbenchmark delivers a more meaningful and robust evaluation of foundation\nmodels' performance in real-world scenarios. The data, code, and all model\noutputs are released in https://github.com/ruixiangcui/AGIEval.", "collection_id": "2e72e5bbf70523cc25de9a5421e24b262b801d87cc869ab5199f797c3241c4d7"}, "question": "What accuracy rate did GPT-4 attain on the SAT Math test?\n", "answer": "GPT-4 attained a 95% accuracy rate on the SAT Math test, demonstrating its extraordinary performance in tackling human-level tasks. This achievement showcases the capabilities of contemporary foundation models in handling mathematical problems, as assessed by the AGIEval benchmark."}
{"document": {"content": "We present a data-driven approach using word embeddings to discover and\ncategorise language biases on the discussion platform Reddit. As spaces for\nisolated user communities, platforms such as Reddit are increasingly connected\nto issues of racism, sexism and other forms of discrimination. Hence, there is\na need to monitor the language of these groups. One of the most promising AI\napproaches to trace linguistic biases in large textual datasets involves word\nembeddings, which transform text into high-dimensional dense vectors and\ncapture semantic relations between words. Yet, previous studies require\npredefined sets of potential biases to study, e.g., whether gender is more or\nless associated with particular types of jobs. This makes these approaches\nunfit to deal with smaller and community-centric datasets such as those on\nReddit, which contain smaller vocabularies and slang, as well as biases that\nmay be particular to that community. This paper proposes a data-driven approach\nto automatically discover language biases encoded in the vocabulary of online\ndiscourse communities on Reddit. In our approach, protected attributes are\nconnected to evaluative words found in the data, which are then categorised\nthrough a semantic analysis system. We verify the effectiveness of our method\nby comparing the biases we discover in the Google News dataset with those found\nin previous literature. We then successfully discover gender bias, religion\nbias, and ethnic bias in different Reddit communities. We conclude by\ndiscussing potential application scenarios and limitations of this data-driven\nbias discovery method.", "collection_id": "f83d52f804b63aaf367c1463d4b63d61a5ad436ef465fe97983cb844432f0d28"}, "question": "What AI approach is used to discover and categorize language biases on the discussion platform Reddit?\n\n", "answer": "The AI approach used to discover and categorize language biases on Reddit involves word embeddings, which transform text into high-dimensional dense vectors and capture semantic relations between words. This approach is utilized in a data-driven method that connects protected attributes to evaluative words found in the data, which are then categorized through a semantic analysis system."}
{"document": {"content": "The release of ChatGPT, a language model capable of generating text that\nappears human-like and authentic, has gained significant attention beyond the\nresearch community. We expect that the convincing performance of ChatGPT\nincentivizes users to apply it to a variety of downstream tasks, including\nprompting the model to simplify their own medical reports. To investigate this\nphenomenon, we conducted an exploratory case study. In a questionnaire, we\nasked 15 radiologists to assess the quality of radiology reports simplified by\nChatGPT. Most radiologists agreed that the simplified reports were factually\ncorrect, complete, and not potentially harmful to the patient. Nevertheless,\ninstances of incorrect statements, missed key medical findings, and potentially\nharmful passages were reported. While further studies are needed, the initial\ninsights of this study indicate a great potential in using large language\nmodels like ChatGPT to improve patient-centered care in radiology and other\nmedical domains.", "collection_id": "df514ad235a1f32b799b0a4a288a1a1168067516dd2ac184b832d6f8cde2c9a5"}, "question": "How many radiologists assessed the quality of radiology reports simplified by ChatGPT in a case study?\n\n", "answer": "In a case study, 15 radiologists assessed the quality of radiology reports simplified by ChatGPT through a questionnaire. They evaluated the reports for factual correctness, completeness, and potential harm to patients, providing initial insights into the potential of large language models like ChatGPT in improving patient-centered care in radiology and other medical domains."}
{"document": {"content": "This paper explores a simple method for improving the zero-shot learning\nabilities of language models. We show that instruction tuning -- finetuning\nlanguage models on a collection of tasks described via instructions --\nsubstantially improves zero-shot performance on unseen tasks.\n  We take a 137B parameter pretrained language model and instruction-tune it on\nover 60 NLP tasks verbalized via natural language instruction templates. We\nevaluate this instruction-tuned model, which we call FLAN, on unseen task\ntypes. FLAN substantially improves the performance of its unmodified\ncounterpart and surpasses zero-shot 175B GPT-3 on 20 of 25 tasks that we\nevaluate. FLAN even outperforms few-shot GPT-3 by a large margin on ANLI, RTE,\nBoolQ, AI2-ARC, OpenbookQA, and StoryCloze. Ablation studies reveal that number\nof finetuning datasets, model scale, and natural language instructions are key\nto the success of instruction tuning.", "collection_id": "a49c56ca52d6cc330a25b07fb4db58a0d6048001a009de8f4d2121a22fea5178"}, "question": "What is FLAN in the context of language models?\n", "answer": "FLAN is an instruction-tuned language model that has been fine-tuned on over 60 NLP tasks verbalized via natural language instruction templates, resulting in substantial improvements in zero-shot performance on unseen tasks."}
{"document": {"content": "Recent work has demonstrated that increased training dataset diversity\nimproves general cross-domain knowledge and downstream generalization\ncapability for large-scale language models. With this in mind, we present\n\\textit{the Pile}: an 825 GiB English text corpus targeted at training\nlarge-scale language models. The Pile is constructed from 22 diverse\nhigh-quality subsets -- both existing and newly constructed -- many of which\nderive from academic or professional sources. Our evaluation of the untuned\nperformance of GPT-2 and GPT-3 on the Pile shows that these models struggle on\nmany of its components, such as academic writing. Conversely, models trained on\nthe Pile improve significantly over both Raw CC and CC-100 on all components of\nthe Pile, while improving performance on downstream evaluations. Through an\nin-depth exploratory analysis, we document potentially concerning aspects of\nthe data for prospective users. We make publicly available the code used in its\nconstruction.", "collection_id": "497bdad70aa9314f6ef54483821b5023b8998f026155ad514591533fd8ed25a9"}, "question": "What is the size of the Pile English text corpus?\n", "answer": "The Pile English text corpus is 825 GiB in size, constructed from 22 diverse high-quality subsets, and is targeted at training large-scale language models."}
{"document": {"content": "Whereas much of the success of the current generation of neural language\nmodels has been driven by increasingly large training corpora, relatively\nlittle research has been dedicated to analyzing these massive sources of\ntextual data. In this exploratory analysis, we delve deeper into the Common\nCrawl, a colossal web corpus that is extensively used for training language\nmodels. We find that it contains a significant amount of undesirable content,\nincluding hate speech and sexually explicit content, even after filtering\nprocedures. We discuss the potential impacts of this content on language models\nand conclude with future research directions and a more mindful approach to\ncorpus collection and analysis.", "collection_id": "09f87087ebd0a00932a9bb6afc8814a4cc2d24dc0e3fe008620dee2b35530dc9"}, "question": "What is the Common Crawl used for?\n", "answer": "The Common Crawl is a colossal web corpus that is extensively used for training language models."}
{"document": {"content": "Text style transfer is usually performed using attributes that can take a\nhandful of discrete values (e.g., positive to negative reviews). In this work,\nwe introduce an architecture that can leverage pre-trained consistent\ncontinuous distributed style representations and use them to transfer to an\nattribute unseen during training, without requiring any re-tuning of the style\ntransfer model. We demonstrate the method by training an architecture to\ntransfer text conveying one sentiment to another sentiment, using a\nfine-grained set of over 20 sentiment labels rather than the binary\npositive/negative often used in style transfer. Our experiments show that this\nmodel can then rewrite text to match a target sentiment that was unseen during\ntraining.", "collection_id": "1b49a71a7e86b087423c733f3be878aa4112ace61a458738824b6f120675c975"}, "question": "How many sentiment labels are used in the fine-grained set for text style transfer instead of the binary positive/negative labels?\n\n", "answer": "A fine-grained set of over 20 sentiment labels is used for text style transfer instead of the binary positive/negative labels often used in style transfer. This allows the model to rewrite text to match a target sentiment that was unseen during training, providing a more nuanced approach to text style transfer."}
{"document": {"content": "Deep learning (DL) creates impactful advances following a virtuous recipe:\nmodel architecture search, creating large training data sets, and scaling\ncomputation. It is widely believed that growing training sets and models should\nimprove accuracy and result in better products. As DL application domains grow,\nwe would like a deeper understanding of the relationships between training set\nsize, computational scale, and model accuracy improvements to advance the\nstate-of-the-art.\n  This paper presents a large scale empirical characterization of\ngeneralization error and model size growth as training sets grow. We introduce\na methodology for this measurement and test four machine learning domains:\nmachine translation, language modeling, image processing, and speech\nrecognition. Our empirical results show power-law generalization error scaling\nacross a breadth of factors, resulting in power-law exponents---the \"steepness\"\nof the learning curve---yet to be explained by theoretical work. Further, model\nimprovements only shift the error but do not appear to affect the power-law\nexponent. We also show that model size scales sublinearly with data size. These\nscaling relationships have significant implications on deep learning research,\npractice, and systems. They can assist model debugging, setting accuracy\ntargets, and decisions about data set growth. They can also guide computing\nsystem design and underscore the importance of continued computational scaling.", "collection_id": "cbe541fdf16fc915d5510d6e8db9f4ae2e054257eeffd4a62247e3fdea559c31"}, "question": "What are the scaling relationships discovered in deep learning research and what implications do they have?\n\n", "answer": "The scaling relationships discovered in deep learning research are power-law generalization error scaling and sublinear model size scaling with data size. These relationships have significant implications on deep learning research, practice, and systems, including assisting model debugging, setting accuracy targets, and decisions about data set growth. They can also guide computing system design and underscore the importance of continued computational scaling."}
{"document": {"content": "In this work, we develop and release Llama 2, a collection of pretrained and\nfine-tuned large language models (LLMs) ranging in scale from 7 billion to 70\nbillion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for\ndialogue use cases. Our models outperform open-source chat models on most\nbenchmarks we tested, and based on our human evaluations for helpfulness and\nsafety, may be a suitable substitute for closed-source models. We provide a\ndetailed description of our approach to fine-tuning and safety improvements of\nLlama 2-Chat in order to enable the community to build on our work and\ncontribute to the responsible development of LLMs.", "collection_id": "71442d16e4635833344a09eb65bebcde24b437fcb6f9553ad92c05940b619f5f"}, "question": "What is the range of parameters for the Llama 2 large language models?\n", "answer": "The Llama 2 large language models range in scale from 7 billion to 70 billion parameters."}
{"document": {"content": "The ability of generative language models (GLMs) to generate text has\nimproved considerably in the last few years, enabling their use for generative\ndata augmentation. In this work, we propose CONDA, an approach to further\nimprove GLMs' ability to generate synthetic data by reformulating data\ngeneration as context generation for a given question-answer (QA) pair and\nleveraging QA datasets for training context generators. Then, we cast\ndownstream tasks into the same question answering format and adapt the\nfine-tuned context generators to the target task domain. Finally, we use the\nfine-tuned GLM to generate relevant contexts, which are in turn used as\nsynthetic training data for their corresponding tasks. We perform extensive\nexperiments on multiple classification datasets and demonstrate substantial\nimprovements in performance for both few- and zero-shot settings. Our analysis\nreveals that QA datasets that require high-level reasoning abilities (e.g.,\nabstractive and common-sense QA datasets) tend to give the best boost in\nperformance in both few-shot and zero-shot settings.", "collection_id": "5e14de63fea3f494bf3d516e95c23f26106bafac6d64476057e33f77957315cd"}, "question": "What type of QA datasets tend to give the best boost in performance for few-shot and zero-shot settings?\n\n", "answer": "QA datasets that require high-level reasoning abilities, such as abstractive and common-sense QA datasets, tend to give the best boost in performance for both few-shot and zero-shot settings."}
{"document": {"content": "The original ImageNet dataset is a popular large-scale benchmark for training\nDeep Neural Networks. Since the cost of performing experiments (e.g, algorithm\ndesign, architecture search, and hyperparameter tuning) on the original dataset\nmight be prohibitive, we propose to consider a downsampled version of ImageNet.\nIn contrast to the CIFAR datasets and earlier downsampled versions of ImageNet,\nour proposed ImageNet32$\\times$32 (and its variants ImageNet64$\\times$64 and\nImageNet16$\\times$16) contains exactly the same number of classes and images as\nImageNet, with the only difference that the images are downsampled to\n32$\\times$32 pixels per image (64$\\times$64 and 16$\\times$16 pixels for the\nvariants, respectively). Experiments on these downsampled variants are\ndramatically faster than on the original ImageNet and the characteristics of\nthe downsampled datasets with respect to optimal hyperparameters appear to\nremain similar. The proposed datasets and scripts to reproduce our results are\navailable at http://image-net.org/download-images and\nhttps://github.com/PatrykChrabaszcz/Imagenet32_Scripts", "collection_id": "efd7708a328cb4aba888ef4ed33ce2440c3cbd5a9112fcbc440747ce7de517d1"}, "question": "What is the main difference between the original ImageNet dataset and the proposed ImageNet32\u00d732 dataset?\n\n", "answer": "The main difference between the original ImageNet dataset and the proposed ImageNet32\u00d732 dataset is the resolution of the images. The original ImageNet dataset contains images at their original resolution, while the ImageNet32\u00d732 dataset contains the same number of classes and images, but with the images downsampled to 32\u00d732 pixels per image. This downsampling significantly reduces the computational cost of performing experiments on the dataset, making it a more accessible and efficient benchmark for training Deep Neural Networks."}
{"document": {"content": "When trained on large, unfiltered crawls from the internet, language models\npick up and reproduce all kinds of undesirable biases that can be found in the\ndata: they often generate racist, sexist, violent or otherwise toxic language.\nAs large models require millions of training examples to achieve good\nperformance, it is difficult to completely prevent them from being exposed to\nsuch content. In this paper, we first demonstrate a surprising finding:\npretrained language models recognize, to a considerable degree, their\nundesirable biases and the toxicity of the content they produce. We refer to\nthis capability as self-diagnosis. Based on this finding, we then propose a\ndecoding algorithm that, given only a textual description of the undesired\nbehavior, reduces the probability of a language model producing problematic\ntext. We refer to this approach as self-debiasing. Self-debiasing does not rely\non manually curated word lists, nor does it require any training data or\nchanges to the model's parameters. While we by no means eliminate the issue of\nlanguage models generating biased text, we believe our approach to be an\nimportant step in this direction.", "collection_id": "806173509744d36d13afea6448a17e211a4ed87f8c969d3c7257f8970cf7fd2e"}, "question": "What is self-debiasing in language models?\n", "answer": "Self-debiasing is a decoding algorithm that reduces the probability of a language model producing problematic text, given only a textual description of the undesired behavior. This approach does not rely on manually curated word lists, nor does it require any training data or changes to the model's parameters. It utilizes the language model's capability of self-diagnosis, which is the ability to recognize its own undesirable biases and the toxicity of the content it produces."}
{"document": {"content": "We study whether language models can evaluate the validity of their own\nclaims and predict which questions they will be able to answer correctly. We\nfirst show that larger models are well-calibrated on diverse multiple choice\nand true/false questions when they are provided in the right format. Thus we\ncan approach self-evaluation on open-ended sampling tasks by asking models to\nfirst propose answers, and then to evaluate the probability \"P(True)\" that\ntheir answers are correct. We find encouraging performance, calibration, and\nscaling for P(True) on a diverse array of tasks. Performance at self-evaluation\nfurther improves when we allow models to consider many of their own samples\nbefore predicting the validity of one specific possibility. Next, we\ninvestigate whether models can be trained to predict \"P(IK)\", the probability\nthat \"I know\" the answer to a question, without reference to any particular\nproposed answer. Models perform well at predicting P(IK) and partially\ngeneralize across tasks, though they struggle with calibration of P(IK) on new\ntasks. The predicted P(IK) probabilities also increase appropriately in the\npresence of relevant source materials in the context, and in the presence of\nhints towards the solution of mathematical word problems. We hope these\nobservations lay the groundwork for training more honest models, and for\ninvestigating how honesty generalizes to cases where models are trained on\nobjectives other than the imitation of human writing.", "collection_id": "1784cc2519de57aacc88bf5f0226f052828350140b594666e6bfddc607d121bb"}, "question": "Can language models predict which questions they will be able to answer correctly?\n", "answer": "Yes, language models can predict which questions they will be able to answer correctly by first proposing answers and then evaluating the probability that their answers are correct, referred to as \"P(True)\". They can also be trained to predict the probability that they know the answer to a question, referred to as \"P(IK)\", without reference to any particular proposed answer."}
{"document": {"content": "Ensembles of neural networks are known to be much more robust and accurate\nthan individual networks. However, training multiple deep networks for model\naveraging is computationally expensive. In this paper, we propose a method to\nobtain the seemingly contradictory goal of ensembling multiple neural networks\nat no additional training cost. We achieve this goal by training a single\nneural network, converging to several local minima along its optimization path\nand saving the model parameters. To obtain repeated rapid convergence, we\nleverage recent work on cyclic learning rate schedules. The resulting\ntechnique, which we refer to as Snapshot Ensembling, is simple, yet\nsurprisingly effective. We show in a series of experiments that our approach is\ncompatible with diverse network architectures and learning tasks. It\nconsistently yields lower error rates than state-of-the-art single models at no\nadditional training cost, and compares favorably with traditional network\nensembles. On CIFAR-10 and CIFAR-100 our DenseNet Snapshot Ensembles obtain\nerror rates of 3.4% and 17.4% respectively.", "collection_id": "580f13f9645cf648e43a0b8ec82ec61bf003a9a32f8addbf243a0af6b3612b20"}, "question": "What error rates do DenseNet Snapshot Ensembles achieve on CIFAR-10 and CIFAR-100?\n\n", "answer": "DenseNet Snapshot Ensembles achieve error rates of 3.4% on CIFAR-10 and 17.4% on CIFAR-100."}
{"document": {"content": "Neural network scaling has been critical for improving the model quality in\nmany real-world machine learning applications with vast amounts of training\ndata and compute. Although this trend of scaling is affirmed to be a sure-fire\napproach for better model quality, there are challenges on the path such as the\ncomputation cost, ease of programming, and efficient implementation on parallel\ndevices. GShard is a module composed of a set of lightweight annotation APIs\nand an extension to the XLA compiler. It provides an elegant way to express a\nwide range of parallel computation patterns with minimal changes to the\nexisting model code. GShard enabled us to scale up multilingual neural machine\ntranslation Transformer model with Sparsely-Gated Mixture-of-Experts beyond 600\nbillion parameters using automatic sharding. We demonstrate that such a giant\nmodel can efficiently be trained on 2048 TPU v3 accelerators in 4 days to\nachieve far superior quality for translation from 100 languages to English\ncompared to the prior art.", "collection_id": "369097dcf47faa4849eb0ca9f91b064d8bf2dfd5afe45c1ed510d2d9c97d670d"}, "question": "How long does it take to train a giant multilingual neural machine translation model with over 600 billion parameters on 2048 TPU v3 accelerators?\n\n", "answer": "A giant multilingual neural machine translation model with Sparsely-Gated Mixture-of-Experts and over 600 billion parameters can be efficiently trained on 2048 TPU v3 accelerators in 4 days, achieving superior quality for translation from 100 languages to English."}
{"document": {"content": "Standard artificial neural networks suffer from the well-known issue of\ncatastrophic forgetting, making continual or lifelong learning difficult for\nmachine learning. In recent years, numerous methods have been proposed for\ncontinual learning, but due to differences in evaluation protocols it is\ndifficult to directly compare their performance. To enable more structured\ncomparisons, we describe three continual learning scenarios based on whether at\ntest time task identity is provided and--in case it is not--whether it must be\ninferred. Any sequence of well-defined tasks can be performed according to each\nscenario. Using the split and permuted MNIST task protocols, for each scenario\nwe carry out an extensive comparison of recently proposed continual learning\nmethods. We demonstrate substantial differences between the three scenarios in\nterms of difficulty and in terms of how efficient different methods are. In\nparticular, when task identity must be inferred (i.e., class incremental\nlearning), we find that regularization-based approaches (e.g., elastic weight\nconsolidation) fail and that replaying representations of previous experiences\nseems required for solving this scenario.", "collection_id": "800766e222ac510448bc5d8c6ec2161c26190a9dc2764226eda058238a491de7"}, "question": "What is a major issue that standard artificial neural networks face in machine learning?\n\n", "answer": "Standard artificial neural networks suffer from the well-known issue of catastrophic forgetting, which makes continual or lifelong learning difficult for machine learning. This issue occurs when a neural network forgets previously learned information after being trained on new data, resulting in a significant decrease in performance on the original task."}
{"document": {"content": "Massively multilingual language models such as multilingual BERT offer\nstate-of-the-art cross-lingual transfer performance on a range of NLP tasks.\nHowever, due to limited capacity and large differences in pretraining data\nsizes, there is a profound performance gap between resource-rich and\nresource-poor target languages. The ultimate challenge is dealing with\nunder-resourced languages not covered at all by the models and written in\nscripts unseen during pretraining. In this work, we propose a series of novel\ndata-efficient methods that enable quick and effective adaptation of pretrained\nmultilingual models to such low-resource languages and unseen scripts. Relying\non matrix factorization, our methods capitalize on the existing latent\nknowledge about multiple languages already available in the pretrained model's\nembedding matrix. Furthermore, we show that learning of the new dedicated\nembedding matrix in the target language can be improved by leveraging a small\nnumber of vocabulary items (i.e., the so-called lexically overlapping tokens)\nshared between mBERT's and target language vocabulary. Our adaptation\ntechniques offer substantial performance gains for languages with unseen\nscripts. We also demonstrate that they can yield improvements for low-resource\nlanguages written in scripts covered by the pretrained model.", "collection_id": "c764c2344cfe3d9d8f7872ea6c3394ee134e6f25caf01af9e6e47fae657f710d"}, "question": "What is the main challenge in dealing with under-resourced languages in multilingual language models?\n\n", "answer": "The ultimate challenge in dealing with under-resourced languages in multilingual language models is adapting the models to languages not covered at all by the models and written in scripts unseen during pretraining, which results in a profound performance gap between resource-rich and resource-poor target languages."}
{"document": {"content": "By decomposing the image formation process into a sequential application of\ndenoising autoencoders, diffusion models (DMs) achieve state-of-the-art\nsynthesis results on image data and beyond. Additionally, their formulation\nallows for a guiding mechanism to control the image generation process without\nretraining. However, since these models typically operate directly in pixel\nspace, optimization of powerful DMs often consumes hundreds of GPU days and\ninference is expensive due to sequential evaluations. To enable DM training on\nlimited computational resources while retaining their quality and flexibility,\nwe apply them in the latent space of powerful pretrained autoencoders. In\ncontrast to previous work, training diffusion models on such a representation\nallows for the first time to reach a near-optimal point between complexity\nreduction and detail preservation, greatly boosting visual fidelity. By\nintroducing cross-attention layers into the model architecture, we turn\ndiffusion models into powerful and flexible generators for general conditioning\ninputs such as text or bounding boxes and high-resolution synthesis becomes\npossible in a convolutional manner. Our latent diffusion models (LDMs) achieve\na new state of the art for image inpainting and highly competitive performance\non various tasks, including unconditional image generation, semantic scene\nsynthesis, and super-resolution, while significantly reducing computational\nrequirements compared to pixel-based DMs. Code is available at\nhttps://github.com/CompVis/latent-diffusion .", "collection_id": "5acf1aa1b5964fcd5fc78801b380c3b3a13bb8920b31ac7f025f3248a03fc355"}, "question": "What is the main advantage of applying diffusion models in the latent space of pretrained autoencoders?\n\n", "answer": "The main advantage of applying diffusion models in the latent space of pretrained autoencoders is that it allows for a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity while significantly reducing computational requirements compared to pixel-based diffusion models."}
{"document": {"content": "For artificial intelligence to be beneficial to humans the behaviour of AI\nagents needs to be aligned with what humans want. In this paper we discuss some\nbehavioural issues for language agents, arising from accidental\nmisspecification by the system designer. We highlight some ways that\nmisspecification can occur and discuss some behavioural issues that could arise\nfrom misspecification, including deceptive or manipulative language, and review\nsome approaches for avoiding these issues.", "collection_id": "50fa1acb965800d58c683f41c3d9aedd93eaf421f9d7ed4c6217a5badc88d27d"}, "question": "What are some potential issues that can arise from accidental misspecification in language agents?\n", "answer": "Potential issues that can arise from accidental misspecification in language agents include deceptive or manipulative language. This occurs when the system designer unintentionally creates an AI agent that behaves in ways that are not aligned with human values or goals, leading to undesirable outcomes."}
{"document": {"content": "We propose a new test to measure a text model's multitask accuracy. The test\ncovers 57 tasks including elementary mathematics, US history, computer science,\nlaw, and more. To attain high accuracy on this test, models must possess\nextensive world knowledge and problem solving ability. We find that while most\nrecent models have near random-chance accuracy, the very largest GPT-3 model\nimproves over random chance by almost 20 percentage points on average. However,\non every one of the 57 tasks, the best models still need substantial\nimprovements before they can reach expert-level accuracy. Models also have\nlopsided performance and frequently do not know when they are wrong. Worse,\nthey still have near-random accuracy on some socially important subjects such\nas morality and law. By comprehensively evaluating the breadth and depth of a\nmodel's academic and professional understanding, our test can be used to\nanalyze models across many tasks and to identify important shortcomings.", "collection_id": "36a32aaec6996535afc8048d1cba063f0f6470292229cf50420ebdd4a66d92c9"}, "question": "How many tasks does the new test cover to measure a text model's multitask accuracy?\n", "answer": "The new test covers 57 tasks to measure a text model's multitask accuracy, including elementary mathematics, US history, computer science, law, and more."}
{"document": {"content": "Humans can reason compositionally when presented with new tasks. Previous\nresearch shows that appropriate prompting techniques enable large language\nmodels (LLMs) to solve artificial compositional generalization tasks such as\nSCAN. In this work, we identify additional challenges in more realistic\nsemantic parsing tasks with larger vocabulary and refine these prompting\ntechniques to address them. Our best method is based on least-to-most\nprompting: it decomposes the problem using prompting-based syntactic parsing,\nthen uses this decomposition to select appropriate exemplars and to\nsequentially generate the semantic parse. This method allows us to set a new\nstate of the art for CFQ while requiring only 1% of the training data used by\ntraditional approaches. Due to the general nature of our approach, we expect\nsimilar efforts will lead to new results in other tasks and domains, especially\nfor knowledge-intensive applications.", "collection_id": "839d53c4328cc76aacf78a01261fa5c123a63576d60b5145d896b8096053113c"}, "question": "What prompting technique is most effective for compositional generalization tasks in semantic parsing?\n\n", "answer": "The most effective prompting technique for compositional generalization tasks in semantic parsing is the \"least-to-most prompting\" method. This method involves decomposing the problem using prompting-based syntactic parsing, then using this decomposition to select appropriate exemplars and to sequentially generate the semantic parse. This approach has been shown to achieve state-of-the-art results in tasks such as CFQ while requiring significantly less training data than traditional approaches."}
{"document": {"content": "Deep learning's recent history has been one of achievement: from triumphing\nover humans in the game of Go to world-leading performance in image\nclassification, voice recognition, translation, and other tasks. But this\nprogress has come with a voracious appetite for computing power. This article\ncatalogs the extent of this dependency, showing that progress across a wide\nvariety of applications is strongly reliant on increases in computing power.\nExtrapolating forward this reliance reveals that progress along current lines\nis rapidly becoming economically, technically, and environmentally\nunsustainable. Thus, continued progress in these applications will require\ndramatically more computationally-efficient methods, which will either have to\ncome from changes to deep learning or from moving to other machine learning\nmethods.", "collection_id": "66a9a5055cf3f004624ad8c708f9ca1980bbf72789a9470f5ea02330deae8a93"}, "question": "Why is continued progress in deep learning applications becoming unsustainable?\n\n", "answer": "Continued progress in deep learning applications is becoming unsustainable due to its strong reliance on increases in computing power, which is rapidly becoming economically, technically, and environmentally unfeasible. This means that advancements in these applications will require more computationally-efficient methods to be developed, either through changes to deep learning itself or by adopting alternative machine learning approaches."}
{"document": {"content": "This paper addresses the scalability challenge of architecture search by\nformulating the task in a differentiable manner. Unlike conventional approaches\nof applying evolution or reinforcement learning over a discrete and\nnon-differentiable search space, our method is based on the continuous\nrelaxation of the architecture representation, allowing efficient search of the\narchitecture using gradient descent. Extensive experiments on CIFAR-10,\nImageNet, Penn Treebank and WikiText-2 show that our algorithm excels in\ndiscovering high-performance convolutional architectures for image\nclassification and recurrent architectures for language modeling, while being\norders of magnitude faster than state-of-the-art non-differentiable techniques.\nOur implementation has been made publicly available to facilitate further\nresearch on efficient architecture search algorithms.", "collection_id": "fc79a24882313e8b84fa5a23270ca0e68a0585160ca3cd5eeab6798982b17ef4"}, "question": "What datasets were used for experiments to test the efficiency of the architecture search algorithm?\n", "answer": "The algorithm was tested on four datasets: CIFAR-10 and ImageNet for image classification, and Penn Treebank and WikiText-2 for language modeling."}
{"document": {"content": "It is known that the learning rate is the most important hyper-parameter to\ntune for training deep neural networks. This paper describes a new method for\nsetting the learning rate, named cyclical learning rates, which practically\neliminates the need to experimentally find the best values and schedule for the\nglobal learning rates. Instead of monotonically decreasing the learning rate,\nthis method lets the learning rate cyclically vary between reasonable boundary\nvalues. Training with cyclical learning rates instead of fixed values achieves\nimproved classification accuracy without a need to tune and often in fewer\niterations. This paper also describes a simple way to estimate \"reasonable\nbounds\" -- linearly increasing the learning rate of the network for a few\nepochs. In addition, cyclical learning rates are demonstrated on the CIFAR-10\nand CIFAR-100 datasets with ResNets, Stochastic Depth networks, and DenseNets,\nand the ImageNet dataset with the AlexNet and GoogLeNet architectures. These\nare practical tools for everyone who trains neural networks.", "collection_id": "a013607cc3d3678a88059fc4d7b89a979f9f07a85a864a8b99c6cc269b583ecb"}, "question": "What is the most important hyper-parameter to tune for training deep neural networks?\n", "answer": "The learning rate is the most important hyper-parameter to tune for training deep neural networks."}
{"document": {"content": "Language model pretraining has led to significant performance gains but\ncareful comparison between different approaches is challenging. Training is\ncomputationally expensive, often done on private datasets of different sizes,\nand, as we will show, hyperparameter choices have significant impact on the\nfinal results. We present a replication study of BERT pretraining (Devlin et\nal., 2019) that carefully measures the impact of many key hyperparameters and\ntraining data size. We find that BERT was significantly undertrained, and can\nmatch or exceed the performance of every model published after it. Our best\nmodel achieves state-of-the-art results on GLUE, RACE and SQuAD. These results\nhighlight the importance of previously overlooked design choices, and raise\nquestions about the source of recently reported improvements. We release our\nmodels and code.", "collection_id": "f80c7c3aa15ff4b4c74e44c23979b2527e2a345120e03ebb13cedf18520f3c16"}, "question": "What results did the best model achieve in the replication study of BERT pretraining?\n", "answer": "The best model achieved state-of-the-art results on GLUE, RACE, and SQuAD in the replication study of BERT pretraining."}
{"document": {"content": "We present Habitat, a platform for research in embodied artificial\nintelligence (AI). Habitat enables training embodied agents (virtual robots) in\nhighly efficient photorealistic 3D simulation. Specifically, Habitat consists\nof: (i) Habitat-Sim: a flexible, high-performance 3D simulator with\nconfigurable agents, sensors, and generic 3D dataset handling. Habitat-Sim is\nfast -- when rendering a scene from Matterport3D, it achieves several thousand\nframes per second (fps) running single-threaded, and can reach over 10,000 fps\nmulti-process on a single GPU. (ii) Habitat-API: a modular high-level library\nfor end-to-end development of embodied AI algorithms -- defining tasks (e.g.,\nnavigation, instruction following, question answering), configuring, training,\nand benchmarking embodied agents.\n  These large-scale engineering contributions enable us to answer scientific\nquestions requiring experiments that were till now impracticable or 'merely'\nimpractical. Specifically, in the context of point-goal navigation: (1) we\nrevisit the comparison between learning and SLAM approaches from two recent\nworks and find evidence for the opposite conclusion -- that learning\noutperforms SLAM if scaled to an order of magnitude more experience than\nprevious investigations, and (2) we conduct the first cross-dataset\ngeneralization experiments {train, test} x {Matterport3D, Gibson} for multiple\nsensors {blind, RGB, RGBD, D} and find that only agents with depth (D) sensors\ngeneralize across datasets. We hope that our open-source platform and these\nfindings will advance research in embodied AI.", "collection_id": "4e39c4632cfe8122388e4aeee7e9b51a70c32ef8e822bc3aef732cb487258d71"}, "question": "What are the components of Habitat, a platform for research in embodied artificial intelligence?\n\n", "answer": "Habitat consists of two main components: (i) Habitat-Sim, a flexible, high-performance 3D simulator with configurable agents, sensors, and generic 3D dataset handling, and (ii) Habitat-API, a modular high-level library for end-to-end development of embodied AI algorithms, including defining tasks, configuring, training, and benchmarking embodied agents."}
{"document": {"content": "The probing methodology allows one to obtain a partial representation of\nlinguistic phenomena stored in the inner layers of the neural network, using\nexternal classifiers and statistical analysis. Pre-trained transformer-based\nlanguage models are widely used both for natural language understanding (NLU)\nand natural language generation (NLG) tasks making them most commonly used for\ndownstream applications. However, little analysis was carried out, whether the\nmodels were pre-trained enough or contained knowledge correlated with\nlinguistic theory. We are presenting the chronological probing study of\ntransformer English models such as MultiBERT and T5. We sequentially compare\nthe information about the language learned by the models in the process of\ntraining on corpora. The results show that 1) linguistic information is\nacquired in the early stages of training 2) both language models demonstrate\ncapabilities to capture various features from various levels of language,\nincluding morphology, syntax, and even discourse, while they also can\ninconsistently fail on tasks that are perceived as easy. We also introduce the\nopen-source framework for chronological probing research, compatible with other\ntransformer-based models.\nhttps://github.com/EkaterinaVoloshina/chronological_probing", "collection_id": "4a7f325636af87a380c05f135cf476b3e910bd60dd2ab6099680fc651fa7a792"}, "question": "What types of linguistic features can transformer-based language models such as MultiBERT and T5 capture?\n\n", "answer": "Transformer-based language models, such as MultiBERT and T5, demonstrate capabilities to capture various features from different levels of language, including morphology, syntax, and even discourse. This suggests that these models can learn and represent a wide range of linguistic phenomena, from the smallest units of language (morphology) to the structure of sentences (syntax) and the relationships between sentences (discourse)."}
{"document": {"content": "Chain-of-thought prompting combined with pre-trained large language models\nhas achieved encouraging results on complex reasoning tasks. In this paper, we\npropose a new decoding strategy, self-consistency, to replace the naive greedy\ndecoding used in chain-of-thought prompting. It first samples a diverse set of\nreasoning paths instead of only taking the greedy one, and then selects the\nmost consistent answer by marginalizing out the sampled reasoning paths.\nSelf-consistency leverages the intuition that a complex reasoning problem\ntypically admits multiple different ways of thinking leading to its unique\ncorrect answer. Our extensive empirical evaluation shows that self-consistency\nboosts the performance of chain-of-thought prompting with a striking margin on\na range of popular arithmetic and commonsense reasoning benchmarks, including\nGSM8K (+17.9%), SVAMP (+11.0%), AQuA (+12.2%), StrategyQA (+6.4%) and\nARC-challenge (+3.9%).", "collection_id": "7e75e1fea4e5ec0a764b307567715c6519260ae46966eb621dc4c7a8f5206a74"}, "question": "What is the percentage improvement of self-consistency on the GSM8K benchmark for chain-of-thought prompting?\n", "answer": "Self-consistency boosts the performance of chain-of-thought prompting by 17.9% on the GSM8K benchmark."}
{"document": {"content": "Generalization and robustness are both key desiderata for designing machine\nlearning methods. Adversarial training can enhance robustness, but past work\noften finds it hurts generalization. In natural language processing (NLP),\npre-training large neural language models such as BERT have demonstrated\nimpressive gain in generalization for a variety of tasks, with further\nimprovement from adversarial fine-tuning. However, these models are still\nvulnerable to adversarial attacks. In this paper, we show that adversarial\npre-training can improve both generalization and robustness. We propose a\ngeneral algorithm ALUM (Adversarial training for large neural LangUage Models),\nwhich regularizes the training objective by applying perturbations in the\nembedding space that maximizes the adversarial loss. We present the first\ncomprehensive study of adversarial training in all stages, including\npre-training from scratch, continual pre-training on a well-trained model, and\ntask-specific fine-tuning. ALUM obtains substantial gains over BERT on a wide\nrange of NLP tasks, in both regular and adversarial scenarios. Even for models\nthat have been well trained on extremely large text corpora, such as RoBERTa,\nALUM can still produce significant gains from continual pre-training, whereas\nconventional non-adversarial methods can not. ALUM can be further combined with\ntask-specific fine-tuning to attain additional gains. The ALUM code is publicly\navailable at https://github.com/namisan/mt-dnn.", "collection_id": "233ebf4131bc510a0a426ab5c84a164a5314e4f374dda2fab837475d0e69bd0d"}, "question": "What is ALUM in the context of natural language processing?\n\n", "answer": "ALUM stands for Adversarial training for large neural LangUage Models. It is a general algorithm that aims to improve both generalization and robustness in natural language processing tasks by regularizing the training objective through applying perturbations in the embedding space that maximizes the adversarial loss."}
{"document": {"content": "Training state-of-the-art, deep neural networks is computationally expensive.\nOne way to reduce the training time is to normalize the activities of the\nneurons. A recently introduced technique called batch normalization uses the\ndistribution of the summed input to a neuron over a mini-batch of training\ncases to compute a mean and variance which are then used to normalize the\nsummed input to that neuron on each training case. This significantly reduces\nthe training time in feed-forward neural networks. However, the effect of batch\nnormalization is dependent on the mini-batch size and it is not obvious how to\napply it to recurrent neural networks. In this paper, we transpose batch\nnormalization into layer normalization by computing the mean and variance used\nfor normalization from all of the summed inputs to the neurons in a layer on a\nsingle training case. Like batch normalization, we also give each neuron its\nown adaptive bias and gain which are applied after the normalization but before\nthe non-linearity. Unlike batch normalization, layer normalization performs\nexactly the same computation at training and test times. It is also\nstraightforward to apply to recurrent neural networks by computing the\nnormalization statistics separately at each time step. Layer normalization is\nvery effective at stabilizing the hidden state dynamics in recurrent networks.\nEmpirically, we show that layer normalization can substantially reduce the\ntraining time compared with previously published techniques.", "collection_id": "752a4b5a46776275bcd74ba775e2826064b776558882fcb80be2c8e65a287cad"}, "question": "What is layer normalization in deep neural networks?\n", "answer": "Layer normalization is a technique used to normalize the activities of neurons in deep neural networks. It works by computing the mean and variance of the summed inputs to all neurons in a layer on a single training case, and then using these values to normalize the summed input to each neuron. This technique is similar to batch normalization, but unlike batch normalization, layer normalization performs the same computation at both training and test times, and is straightforward to apply to recurrent neural networks. Additionally, layer normalization gives each neuron its own adaptive bias and gain, which are applied after normalization but before non-linearity. This technique is effective in stabilizing the hidden state dynamics in recurrent networks and can substantially reduce the training time compared to other techniques."}
{"document": {"content": "Solving math word problems is a challenging task that requires accurate\nnatural language understanding to bridge natural language texts and math\nexpressions. Motivated by the intuition about how human generates the equations\ngiven the problem texts, this paper presents a neural approach to automatically\nsolve math word problems by operating symbols according to their semantic\nmeanings in texts. This paper views the process of generating equation as a\nbridge between the semantic world and the symbolic world, where the proposed\nneural math solver is based on an encoder-decoder framework. In the proposed\nmodel, the encoder is designed to understand the semantics of problems, and the\ndecoder focuses on tracking semantic meanings of the generated symbols and then\ndeciding which symbol to generate next. The preliminary experiments are\nconducted in a dataset Math23K, and our model significantly outperforms both\nthe state-of-the-art single model and the best non-retrieval-based model over\nabout 10% accuracy, demonstrating the effectiveness of bridging the symbolic\nand semantic worlds from math word problems.", "collection_id": "89b8b7c839330f8b7f10b786e830096e922e18e670f4bb5efb16508c9cfbdf5b"}, "question": "What dataset was used for preliminary experiments in the neural math solver study?\n\n", "answer": "The dataset used for preliminary experiments in the neural math solver study was Math23K. This dataset was utilized to test the effectiveness of the proposed model, which is based on an encoder-decoder framework. The model demonstrated a significant improvement in accuracy, outperforming both the state-of-the-art single model and the best non-retrieval-based model by about 10%."}
{"document": {"content": "We propose a new family of policy gradient methods for reinforcement\nlearning, which alternate between sampling data through interaction with the\nenvironment, and optimizing a \"surrogate\" objective function using stochastic\ngradient ascent. Whereas standard policy gradient methods perform one gradient\nupdate per data sample, we propose a novel objective function that enables\nmultiple epochs of minibatch updates. The new methods, which we call proximal\npolicy optimization (PPO), have some of the benefits of trust region policy\noptimization (TRPO), but they are much simpler to implement, more general, and\nhave better sample complexity (empirically). Our experiments test PPO on a\ncollection of benchmark tasks, including simulated robotic locomotion and Atari\ngame playing, and we show that PPO outperforms other online policy gradient\nmethods, and overall strikes a favorable balance between sample complexity,\nsimplicity, and wall-time.", "collection_id": "fd17a5b9714ba1c379acbcc82877f79c997a04d54fd14e721ce574e5219232da"}, "question": "What is Proximal Policy Optimization (PPO) used for?\n", "answer": "Proximal Policy Optimization (PPO) is a family of policy gradient methods used for reinforcement learning, specifically designed to alternate between sampling data through interaction with the environment and optimizing a surrogate objective function using stochastic gradient ascent. It is used to improve performance in various tasks such as simulated robotic locomotion and Atari game playing."}
{"document": {"content": "Pretrained general-purpose language models can achieve state-of-the-art\naccuracies in various natural language processing domains by adapting to\ndownstream tasks via zero-shot, few-shot and fine-tuning techniques. Because of\ntheir success, the size of these models has increased rapidly, requiring\nhigh-performance hardware, software, and algorithmic techniques to enable\ntraining such large models. As the result of a joint effort between Microsoft\nand NVIDIA, we present details on the training of the largest monolithic\ntransformer based language model, Megatron-Turing NLG 530B (MT-NLG), with 530\nbillion parameters. In this paper, we first focus on the infrastructure as well\nas the 3D parallelism methodology used to train this model using DeepSpeed and\nMegatron. Next, we detail the training process, the design of our training\ncorpus, and our data curation techniques, which we believe is a key ingredient\nto the success of the model. Finally, we discuss various evaluation results, as\nwell as other interesting observations and new properties exhibited by MT-NLG.\nWe demonstrate that MT-NLG achieves superior zero-, one-, and few-shot learning\naccuracies on several NLP benchmarks and establishes new state-of-the-art\nresults. We believe that our contributions will help further the development of\nlarge-scale training infrastructures, large-scale language models, and natural\nlanguage generations.", "collection_id": "e79f7f7ce6ac6750e2ff0e1ca4666fbb8f5e2f4f378dbce6280cf527ed0ac842"}, "question": "What is the number of parameters in the Megatron-Turing NLG 530B language model?\n", "answer": "The Megatron-Turing NLG 530B (MT-NLG) language model has 530 billion parameters."}
{"document": {"content": "The last few years have seen an explosion of academic and popular interest in\nalgorithmic fairness. Despite this interest and the volume and velocity of work\nthat has been produced recently, the fundamental science of fairness in machine\nlearning is still in a nascent state. In March 2018, we convened a group of\nexperts as part of a CCC visioning workshop to assess the state of the field,\nand distill the most promising research directions going forward. This report\nsummarizes the findings of that workshop. Along the way, it surveys recent\ntheoretical work in the field and points towards promising directions for\nresearch.", "collection_id": "6359e938fd27ba50f6fd5b992c71530f90616997b38b85080e58bcf3a51ae7a0"}, "question": "When was the CCC visioning workshop on algorithmic fairness held?\n", "answer": "The CCC visioning workshop on algorithmic fairness was held in March 2018."}
{"document": {"content": "Recent progress in natural language generation has raised dual-use concerns.\nWhile applications like summarization and translation are positive, the\nunderlying technology also might enable adversaries to generate neural fake\nnews: targeted propaganda that closely mimics the style of real news.\n  Modern computer security relies on careful threat modeling: identifying\npotential threats and vulnerabilities from an adversary's point of view, and\nexploring potential mitigations to these threats. Likewise, developing robust\ndefenses against neural fake news requires us first to carefully investigate\nand characterize the risks of these models. We thus present a model for\ncontrollable text generation called Grover. Given a headline like `Link Found\nBetween Vaccines and Autism,' Grover can generate the rest of the article;\nhumans find these generations to be more trustworthy than human-written\ndisinformation.\n  Developing robust verification techniques against generators like Grover is\ncritical. We find that best current discriminators can classify neural fake\nnews from real, human-written, news with 73% accuracy, assuming access to a\nmoderate level of training data. Counterintuitively, the best defense against\nGrover turns out to be Grover itself, with 92% accuracy, demonstrating the\nimportance of public release of strong generators. We investigate these results\nfurther, showing that exposure bias -- and sampling strategies that alleviate\nits effects -- both leave artifacts that similar discriminators can pick up on.\nWe conclude by discussing ethical issues regarding the technology, and plan to\nrelease Grover publicly, helping pave the way for better detection of neural\nfake news.", "collection_id": "45f0dea3f3b10c7965bad71fca335bfa9e89d5e18574d79117205110f10efd86"}, "question": "What is the accuracy of the best current discriminators in classifying neural fake news from real news?\n", "answer": "The best current discriminators can classify neural fake news from real, human-written news with 73% accuracy, assuming access to a moderate level of training data. However, using a strong generator like Grover itself as a discriminator can increase the accuracy to 92%."}
{"document": {"content": "Recent advances in the capacity of large language models to generate\nhuman-like text have resulted in their increased adoption in user-facing\nsettings. In parallel, these improvements have prompted a heated discourse\naround the risks of societal harms they introduce, whether inadvertent or\nmalicious. Several studies have explored these harms and called for their\nmitigation via development of safer, fairer models. Going beyond enumerating\nthe risks of harms, this work provides a survey of practical methods for\naddressing potential threats and societal harms from language generation\nmodels. We draw on several prior works' taxonomies of language model risks to\npresent a structured overview of strategies for detecting and ameliorating\ndifferent kinds of risks/harms of language generators. Bridging diverse strands\nof research, this survey aims to serve as a practical guide for both LM\nresearchers and practitioners, with explanations of different mitigation\nstrategies' motivations, their limitations, and open problems for future\nresearch.", "collection_id": "7367e9a40956b77cb1b24e3e9fbac983d5be0d7934ec13446b2bc7c59a806178"}, "question": "What is the main goal of recent studies on large language models?\n", "answer": "The main goal of recent studies on large language models is to mitigate the societal harms they introduce, whether inadvertent or malicious, by developing safer and fairer models."}
{"document": {"content": "Machine learning models are trained to find patterns in data. NLP models can\ninadvertently learn socially undesirable patterns when training on gender\nbiased text. In this work, we propose a general framework that decomposes\ngender bias in text along several pragmatic and semantic dimensions: bias from\nthe gender of the person being spoken about, bias from the gender of the person\nbeing spoken to, and bias from the gender of the speaker. Using this\nfine-grained framework, we automatically annotate eight large scale datasets\nwith gender information. In addition, we collect a novel, crowdsourced\nevaluation benchmark of utterance-level gender rewrites. Distinguishing between\ngender bias along multiple dimensions is important, as it enables us to train\nfiner-grained gender bias classifiers. We show our classifiers prove valuable\nfor a variety of important applications, such as controlling for gender bias in\ngenerative models, detecting gender bias in arbitrary text, and shed light on\noffensive language in terms of genderedness.", "collection_id": "869aa876f247f019ad31482db73aaa0838a7913456c7dd487e2946d9f3f91315"}, "question": "What are the dimensions of gender bias in text that the proposed framework decomposes along?\n\n", "answer": "The proposed framework decomposes gender bias in text along three pragmatic and semantic dimensions: \n1. Bias from the gender of the person being spoken about \n2. Bias from the gender of the person being spoken to \n3. Bias from the gender of the speaker."}
{"document": {"content": "Successful and effective communication between humans and AI relies on a\nshared experience of the world. By training solely on written text, current\nlanguage models (LMs) miss the grounded experience of humans in the real-world\n-- their failure to relate language to the physical world causes knowledge to\nbe misrepresented and obvious mistakes in their reasoning. We present Mind's\nEye, a paradigm to ground language model reasoning in the physical world. Given\na physical reasoning question, we use a computational physics engine\n(DeepMind's MuJoCo) to simulate the possible outcomes, and then use the\nsimulation results as part of the input, which enables language models to\nperform reasoning. Experiments on 39 tasks in a physics alignment benchmark\ndemonstrate that Mind's Eye can improve reasoning ability by a large margin\n(27.9% zero-shot, and 46.0% few-shot absolute accuracy improvement on average).\nSmaller language models armed with Mind's Eye can obtain similar performance to\nmodels that are 100x larger. Finally, we confirm the robustness of Mind's Eye\nthrough ablation studies.", "collection_id": "5a0d1f12cd9967ae4e407f9388675e374c458e348960ff34562f8b36e37f5d79"}, "question": "What is the average absolute accuracy improvement of Mind's Eye in few-shot physics alignment benchmark tasks?\n\n", "answer": "The average absolute accuracy improvement of Mind's Eye in few-shot physics alignment benchmark tasks is 46.0%. This improvement is achieved by grounding language model reasoning in the physical world through the use of a computational physics engine to simulate possible outcomes, enabling language models to perform more accurate reasoning."}
{"document": {"content": "Text generative models (TGMs) excel in producing text that matches the style\nof human language reasonably well. Such TGMs can be misused by adversaries,\ne.g., by automatically generating fake news and fake product reviews that can\nlook authentic and fool humans. Detectors that can distinguish text generated\nby TGM from human written text play a vital role in mitigating such misuse of\nTGMs. Recently, there has been a flurry of works from both natural language\nprocessing (NLP) and machine learning (ML) communities to build accurate\ndetectors for English. Despite the importance of this problem, there is\ncurrently no work that surveys this fast-growing literature and introduces\nnewcomers to important research challenges. In this work, we fill this void by\nproviding a critical survey and review of this literature to facilitate a\ncomprehensive understanding of this problem. We conduct an in-depth error\nanalysis of the state-of-the-art detector and discuss research directions to\nguide future work in this exciting area.", "collection_id": "d06b6957cedb79f971f69267bafac3d8a615040b0213f0ed26734599960b12f9"}, "question": "What is the main purpose of detectors in the context of text generative models?\n", "answer": "The main purpose of detectors in the context of text generative models is to distinguish text generated by these models from human-written text, thereby mitigating the potential misuse of text generative models, such as generating fake news and product reviews that can deceive humans."}
{"document": {"content": "As language models become more powerful, training and evaluation are\nincreasingly bottlenecked by the data and metrics used for a particular task.\nFor example, summarization models are often trained to predict human reference\nsummaries and evaluated using ROUGE, but both of these metrics are rough\nproxies for what we really care about -- summary quality. In this work, we show\nthat it is possible to significantly improve summary quality by training a\nmodel to optimize for human preferences. We collect a large, high-quality\ndataset of human comparisons between summaries, train a model to predict the\nhuman-preferred summary, and use that model as a reward function to fine-tune a\nsummarization policy using reinforcement learning. We apply our method to a\nversion of the TL;DR dataset of Reddit posts and find that our models\nsignificantly outperform both human reference summaries and much larger models\nfine-tuned with supervised learning alone. Our models also transfer to CNN/DM\nnews articles, producing summaries nearly as good as the human reference\nwithout any news-specific fine-tuning. We conduct extensive analyses to\nunderstand our human feedback dataset and fine-tuned models We establish that\nour reward model generalizes to new datasets, and that optimizing our reward\nmodel results in better summaries than optimizing ROUGE according to humans. We\nhope the evidence from our paper motivates machine learning researchers to pay\ncloser attention to how their training loss affects the model behavior they\nactually want.", "collection_id": "3dd0778523b94e0fb32ece93af932a09a5eec50928337c0e93761184a8e2e603"}, "question": "How can summary quality be improved in language models?\n", "answer": "Summary quality in language models can be improved by training a model to optimize for human preferences. This is achieved by collecting a large, high-quality dataset of human comparisons between summaries, training a model to predict the human-preferred summary, and using that model as a reward function to fine-tune a summarization policy using reinforcement learning. This approach has been shown to significantly outperform both human reference summaries and larger models fine-tuned with supervised learning alone."}
{"document": {"content": "A key challenge for automatic hate-speech detection on social media is the\nseparation of hate speech from other instances of offensive language. Lexical\ndetection methods tend to have low precision because they classify all messages\ncontaining particular terms as hate speech and previous work using supervised\nlearning has failed to distinguish between the two categories. We used a\ncrowd-sourced hate speech lexicon to collect tweets containing hate speech\nkeywords. We use crowd-sourcing to label a sample of these tweets into three\ncategories: those containing hate speech, only offensive language, and those\nwith neither. We train a multi-class classifier to distinguish between these\ndifferent categories. Close analysis of the predictions and the errors shows\nwhen we can reliably separate hate speech from other offensive language and\nwhen this differentiation is more difficult. We find that racist and homophobic\ntweets are more likely to be classified as hate speech but that sexist tweets\nare generally classified as offensive. Tweets without explicit hate keywords\nare also more difficult to classify.", "collection_id": "ed9372a02837349deb7f06fa49aa701670f97e0c30c4537ea46a5bea94af3d8d"}, "question": "What are the challenges of using lexical detection methods for hate speech detection on social media?\n\n", "answer": "Lexical detection methods for hate speech detection on social media tend to have low precision because they classify all messages containing particular terms as hate speech, failing to distinguish between hate speech and other instances of offensive language."}
{"document": {"content": "Large language models (LMs) such as GPT-3 have the surprising ability to do\nin-context learning, where the model learns to do a downstream task simply by\nconditioning on a prompt consisting of input-output examples. The LM learns\nfrom these examples without being explicitly pretrained to learn. Thus, it is\nunclear what enables in-context learning. In this paper, we study how\nin-context learning can emerge when pretraining documents have long-range\ncoherence. Here, the LM must infer a latent document-level concept to generate\ncoherent next tokens during pretraining. At test time, in-context learning\noccurs when the LM also infers a shared latent concept between examples in a\nprompt. We prove when this occurs despite a distribution mismatch between\nprompts and pretraining data in a setting where the pretraining distribution is\na mixture of HMMs. In contrast to messy large-scale datasets used to train LMs\ncapable of in-context learning, we generate a small-scale synthetic dataset\n(GINC) where Transformers and LSTMs both exhibit in-context learning. Beyond\nthe theory, experiments on GINC exhibit large-scale real-world phenomena\nincluding improved in-context performance with model scaling (despite the same\npretraining loss), sensitivity to example order, and instances where zero-shot\nis better than few-shot in-context learning.", "collection_id": "ac11ff841b1991c0fc1093792e6f094c0d5ce997eae66b7229df57ccf7245af3"}, "question": "What enables large language models to do in-context learning despite not being explicitly pretrained for it?\n\n", "answer": "Large language models can do in-context learning when the model infers a shared latent concept between examples in a prompt, which is often learned during pretraining when the model must generate coherent next tokens in documents with long-range coherence."}
{"document": {"content": "We investigate the ability of language models to perform compositional\nreasoning tasks where the overall solution depends on correctly composing the\nanswers to sub-problems. We measure how often models can correctly answer all\nsub-problems but not generate the overall solution, a ratio we call the\ncompositionality gap. We evaluate this ratio by asking multi-hop questions with\nanswers that require composing multiple facts unlikely to have been observed\ntogether during pretraining. In the GPT-3 family of models, as model size\nincreases we show that the single-hop question answering performance improves\nfaster than the multi-hop performance does, therefore the compositionality gap\ndoes not decrease. This surprising result suggests that while more powerful\nmodels memorize and recall more factual knowledge, they show no corresponding\nimprovement in their ability to perform this kind of compositional reasoning.\n  We then demonstrate how elicitive prompting (such as chain of thought)\nnarrows the compositionality gap by reasoning explicitly instead of implicitly.\nWe present a new method, self-ask, that further improves on chain of thought.\nIn our method, the model explicitly asks itself (and then answers) follow-up\nquestions before answering the initial question. We finally show that\nself-ask's structured prompting lets us easily plug in a search engine to\nanswer the follow-up questions, which additionally improves accuracy.", "collection_id": "9a82d680923eac29bb7e7e8cc744ef6c97ec6101f45b64c66d4a292c5ba5e4b3"}, "question": "What is the compositionality gap in language models?\n", "answer": "The compositionality gap in language models refers to the ratio of how often models can correctly answer all sub-problems but not generate the overall solution in compositional reasoning tasks. This gap measures the difference between a model's ability to answer individual sub-problems and its ability to combine those answers to arrive at a correct overall solution."}
{"document": {"content": "While large-scale language models (LMs) are able to imitate the distribution\nof natural language well enough to generate realistic text, it is difficult to\ncontrol which regions of the distribution they generate. This is especially\nproblematic because datasets used for training large LMs usually contain\nsignificant toxicity, hate, bias, and negativity. We propose GeDi as an\nefficient method for using smaller LMs as generative discriminators to guide\ngeneration from large LMs to make them safer and more controllable. GeDi guides\ngeneration at each step by computing classification probabilities for all\npossible next tokens via Bayes rule by normalizing over two class-conditional\ndistributions; one conditioned on the desired attribute, or control code, and\nanother conditioned on the undesired attribute, or anti control code. We find\nthat GeDi gives stronger controllability than the state of the art method while\nalso achieving generation speeds more than 30 times faster. Additionally,\ntraining GeDi on only four topics allows us to controllably generate new topics\nzero-shot from just a keyword, unlocking a new capability that previous\ncontrollable generation methods do not have. Lastly, we show that GeDi can make\nGPT-2 (1.5B parameters) significantly less toxic without sacrificing linguistic\nquality, making it by far the most practical existing method for detoxifying\nlarge language models while maintaining a fast generation speed.", "collection_id": "205b1f5b415327bf504c657e108b195b78460574eb207ff52a5da3dc9ff98691"}, "question": "What is GeDi and what is its primary function in language models?\n", "answer": "GeDi is an efficient method that uses smaller language models as generative discriminators to guide the generation of text from large language models, aiming to make them safer and more controllable by reducing toxicity, hate, bias, and negativity in the generated text."}
{"document": {"content": "Language models (LMs) are becoming the foundation for almost all major\nlanguage technologies, but their capabilities, limitations, and risks are not\nwell understood. We present Holistic Evaluation of Language Models (HELM) to\nimprove the transparency of language models. First, we taxonomize the vast\nspace of potential scenarios (i.e. use cases) and metrics (i.e. desiderata)\nthat are of interest for LMs. Then we select a broad subset based on coverage\nand feasibility, noting what's missing or underrepresented (e.g. question\nanswering for neglected English dialects, metrics for trustworthiness). Second,\nwe adopt a multi-metric approach: We measure 7 metrics (accuracy, calibration,\nrobustness, fairness, bias, toxicity, and efficiency) for each of 16 core\nscenarios when possible (87.5% of the time). This ensures metrics beyond\naccuracy don't fall to the wayside, and that trade-offs are clearly exposed. We\nalso perform 7 targeted evaluations, based on 26 targeted scenarios, to analyze\nspecific aspects (e.g. reasoning, disinformation). Third, we conduct a\nlarge-scale evaluation of 30 prominent language models (spanning open,\nlimited-access, and closed models) on all 42 scenarios, 21 of which were not\npreviously used in mainstream LM evaluation. Prior to HELM, models on average\nwere evaluated on just 17.9% of the core HELM scenarios, with some prominent\nmodels not sharing a single scenario in common. We improve this to 96.0%: now\nall 30 models have been densely benchmarked on the same core scenarios and\nmetrics under standardized conditions. Our evaluation surfaces 25 top-level\nfindings. For full transparency, we release all raw model prompts and\ncompletions publicly for further analysis, as well as a general modular\ntoolkit. We intend for HELM to be a living benchmark for the community,\ncontinuously updated with new scenarios, metrics, and models.", "collection_id": "17da06845e6750c3944799ef6a3a5519f60a5480c575fc58d76e75ce9874c171"}, "question": "What is the purpose of the Holistic Evaluation of Language Models (HELM)?\n", "answer": "The purpose of the Holistic Evaluation of Language Models (HELM) is to improve the transparency of language models by providing a comprehensive evaluation framework that assesses their capabilities, limitations, and risks across a wide range of scenarios and metrics."}
{"document": {"content": "Stable Diffusion revolutionised image creation from descriptive text. GPT-2,\nGPT-3(.5) and GPT-4 demonstrated astonishing performance across a variety of\nlanguage tasks. ChatGPT introduced such language models to the general public.\nIt is now clear that large language models (LLMs) are here to stay, and will\nbring about drastic change in the whole ecosystem of online text and images. In\nthis paper we consider what the future might hold. What will happen to GPT-{n}\nonce LLMs contribute much of the language found online? We find that use of\nmodel-generated content in training causes irreversible defects in the\nresulting models, where tails of the original content distribution disappear.\nWe refer to this effect as Model Collapse and show that it can occur in\nVariational Autoencoders, Gaussian Mixture Models and LLMs. We build\ntheoretical intuition behind the phenomenon and portray its ubiquity amongst\nall learned generative models. We demonstrate that it has to be taken seriously\nif we are to sustain the benefits of training from large-scale data scraped\nfrom the web. Indeed, the value of data collected about genuine human\ninteractions with systems will be increasingly valuable in the presence of\ncontent generated by LLMs in data crawled from the Internet.", "collection_id": "6e05fc56bea3a3e73ca7d6aec27ff0423fe64040306618fe11e04c08b9be82b4"}, "question": "What is Model Collapse in the context of large language models?\n", "answer": "Model Collapse is a phenomenon where the use of model-generated content in training causes irreversible defects in the resulting models, resulting in the disappearance of tails of the original content distribution. This effect can occur in various types of generative models, including Variational Autoencoders, Gaussian Mixture Models, and large language models (LLMs)."}
{"document": {"content": "In recent years, supervised learning with convolutional networks (CNNs) has\nseen huge adoption in computer vision applications. Comparatively, unsupervised\nlearning with CNNs has received less attention. In this work we hope to help\nbridge the gap between the success of CNNs for supervised learning and\nunsupervised learning. We introduce a class of CNNs called deep convolutional\ngenerative adversarial networks (DCGANs), that have certain architectural\nconstraints, and demonstrate that they are a strong candidate for unsupervised\nlearning. Training on various image datasets, we show convincing evidence that\nour deep convolutional adversarial pair learns a hierarchy of representations\nfrom object parts to scenes in both the generator and discriminator.\nAdditionally, we use the learned features for novel tasks - demonstrating their\napplicability as general image representations.", "collection_id": "b93a51e2484b66671ae0f1a6769ea9d7afb9a0e353f5c314f64b135e9beb31dc"}, "question": "What type of neural networks are deep convolutional generative adversarial networks (DCGANs)?\n", "answer": "Deep convolutional generative adversarial networks (DCGANs) are a class of convolutional neural networks (CNNs) that are specifically designed for unsupervised learning, with certain architectural constraints that make them suitable for tasks such as learning general image representations."}
{"document": {"content": "Mitigating bias in machine learning systems requires refining our\nunderstanding of bias propagation pathways: from societal structures to\nlarge-scale data to trained models to impact on society. In this work, we focus\non one aspect of the problem, namely bias amplification: the tendency of models\nto amplify the biases present in the data they are trained on. A metric for\nmeasuring bias amplification was introduced in the seminal work by Zhao et al.\n(2017); however, as we demonstrate, this metric suffers from a number of\nshortcomings including conflating different types of bias amplification and\nfailing to account for varying base rates of protected attributes. We introduce\nand analyze a new, decoupled metric for measuring bias amplification,\n$\\text{BiasAmp}_{\\rightarrow}$ (Directional Bias Amplification). We thoroughly\nanalyze and discuss both the technical assumptions and normative implications\nof this metric. We provide suggestions about its measurement by cautioning\nagainst predicting sensitive attributes, encouraging the use of confidence\nintervals due to fluctuations in the fairness of models across runs, and\ndiscussing the limitations of what this metric captures. Throughout this paper,\nwe work to provide an interrogative look at the technical measurement of bias\namplification, guided by our normative ideas of what we want it to encompass.\nCode is located at https://github.com/princetonvisualai/directional-bias-amp", "collection_id": "1ffe0da2034a3f5ee0718b8631f1cc18923dfb34a5b9e5ca4bdfeb61bfc52941"}, "question": "What is the name of the new metric for measuring bias amplification introduced in the work?\n\n", "answer": "The new metric for measuring bias amplification is called Directional Bias Amplification, denoted as $\\text{BiasAmp}_{\\rightarrow}$. This metric aims to address the shortcomings of the previously introduced metric by Zhao et al. (2017) and provides a more nuanced understanding of bias amplification in machine learning systems."}
{"document": {"content": "By conditioning on natural language instructions, large language models\n(LLMs) have displayed impressive capabilities as general-purpose computers.\nHowever, task performance depends significantly on the quality of the prompt\nused to steer the model, and most effective prompts have been handcrafted by\nhumans. Inspired by classical program synthesis and the human approach to\nprompt engineering, we propose Automatic Prompt Engineer (APE) for automatic\ninstruction generation and selection. In our method, we treat the instruction\nas the \"program,\" optimized by searching over a pool of instruction candidates\nproposed by an LLM in order to maximize a chosen score function. To evaluate\nthe quality of the selected instruction, we evaluate the zero-shot performance\nof another LLM following the selected instruction. Experiments on 24 NLP tasks\nshow that our automatically generated instructions outperform the prior LLM\nbaseline by a large margin and achieve better or comparable performance to the\ninstructions generated by human annotators on 19/24 tasks. We conduct extensive\nqualitative and quantitative analyses to explore the performance of APE. We\nshow that APE-engineered prompts can be applied to steer models toward\ntruthfulness and/or informativeness, as well as to improve few-shot learning\nperformance by simply prepending them to standard in-context learning prompts.\nPlease check out our webpage at\nhttps://sites.google.com/view/automatic-prompt-engineer.", "collection_id": "0eb06cbb1c41913c90a674c12fea939597dad870ac651d6fc94ea33e4984a4dd"}, "question": "What is Automatic Prompt Engineer (APE) used for in natural language processing?\n", "answer": "Automatic Prompt Engineer (APE) is used for automatic instruction generation and selection in natural language processing. It treats the instruction as the \"program\" and optimizes it by searching over a pool of instruction candidates proposed by a large language model (LLM) to maximize a chosen score function. This allows APE to generate high-quality instructions that can steer models toward specific goals, such as truthfulness, informativeness, or improved few-shot learning performance."}
{"document": {"content": "Inductive transfer learning has greatly impacted computer vision, but\nexisting approaches in NLP still require task-specific modifications and\ntraining from scratch. We propose Universal Language Model Fine-tuning\n(ULMFiT), an effective transfer learning method that can be applied to any task\nin NLP, and introduce techniques that are key for fine-tuning a language model.\nOur method significantly outperforms the state-of-the-art on six text\nclassification tasks, reducing the error by 18-24% on the majority of datasets.\nFurthermore, with only 100 labeled examples, it matches the performance of\ntraining from scratch on 100x more data. We open-source our pretrained models\nand code.", "collection_id": "8951155c96071df09b8e5d6c19758dbc52387f9643066fb1f9ebb421fa9ae28e"}, "question": "What is ULMFiT in natural language processing?\n", "answer": "Universal Language Model Fine-tuning (ULMFiT) is an effective transfer learning method in natural language processing (NLP) that can be applied to any task. It allows for fine-tuning a language model on a specific task, achieving state-of-the-art results on several text classification tasks."}
{"document": {"content": "This paper explores the environmental impact of the super-linear growth\ntrends for AI from a holistic perspective, spanning Data, Algorithms, and\nSystem Hardware. We characterize the carbon footprint of AI computing by\nexamining the model development cycle across industry-scale machine learning\nuse cases and, at the same time, considering the life cycle of system hardware.\nTaking a step further, we capture the operational and manufacturing carbon\nfootprint of AI computing and present an end-to-end analysis for what and how\nhardware-software design and at-scale optimization can help reduce the overall\ncarbon footprint of AI. Based on the industry experience and lessons learned,\nwe share the key challenges and chart out important development directions\nacross the many dimensions of AI. We hope the key messages and insights\npresented in this paper can inspire the community to advance the field of AI in\nan environmentally-responsible manner.", "collection_id": "7675f9be0ce27341ca653d31f236ac637fb6f210fb65f12e8a159406f63370f8"}, "question": "What aspects of AI are examined to characterize its carbon footprint?\n", "answer": "The carbon footprint of AI computing is characterized by examining the model development cycle across industry-scale machine learning use cases and considering the life cycle of system hardware, including both operational and manufacturing carbon footprint."}
{"document": {"content": "The aim of this paper is to facilitate nuanced discussion around research\nnorms and practices to mitigate the harmful impacts of advances in machine\nlearning (ML). We focus particularly on the use of ML to create \"synthetic\nmedia\" (e.g. to generate or manipulate audio, video, images, and text), and the\nquestion of what publication and release processes around such research might\nlook like, though many of the considerations discussed will apply to ML\nresearch more broadly. We are not arguing for any specific approach on when or\nhow research should be distributed, but instead try to lay out some useful\ntools, analogies, and options for thinking about these issues.\n  We begin with some background on the idea that ML research might be misused\nin harmful ways, and why advances in synthetic media, in particular, are\nraising concerns. We then outline in more detail some of the different paths to\nharm from ML research, before reviewing research risk mitigation strategies in\nother fields and identifying components that seem most worth emulating in the\nML and synthetic media research communities. Next, we outline some important\ndimensions of disagreement on these issues which risk polarizing conversations.\n  Finally, we conclude with recommendations, suggesting that the machine\nlearning community might benefit from: working with subject matter experts to\nincrease understanding of the risk landscape and possible mitigation\nstrategies; building a community and norms around understanding the impacts of\nML research, e.g. through regular workshops at major conferences; and\nestablishing institutions and systems to support release practices that would\notherwise be onerous and error-prone.", "collection_id": "4d53679b2420348dc64b6b963c11e97143773df817b0c2ef7629f76019603689"}, "question": "What are some strategies to mitigate the risks associated with advances in machine learning research?\n\n", "answer": "To mitigate the risks associated with advances in machine learning research, several strategies can be employed. These include working with subject matter experts to increase understanding of the risk landscape and possible mitigation strategies, building a community and norms around understanding the impacts of machine learning research, and establishing institutions and systems to support release practices that would otherwise be onerous and error-prone. Additionally, reviewing research risk mitigation strategies in other fields and identifying components that seem most worth emulating in the machine learning and synthetic media research communities can also be beneficial."}
{"document": {"content": "Pretrained language models often do not perform tasks in ways that are in\nline with our preferences, e.g., generating offensive text or factually\nincorrect summaries. Recent work approaches the above issue by learning from a\nsimple form of human evaluation: comparisons between pairs of model-generated\ntask outputs. Comparison feedback conveys limited information about human\npreferences per human evaluation. Here, we propose to learn from natural\nlanguage feedback, which conveys more information per human evaluation. We\nlearn from language feedback on model outputs using a three-step learning\nalgorithm. First, we condition the language model on the initial output and\nfeedback to generate many refinements. Second, we choose the refinement with\nthe highest similarity to the feedback. Third, we finetune a language model to\nmaximize the likelihood of the chosen refinement given the input. In synthetic\nexperiments, we first evaluate whether language models accurately incorporate\nfeedback to produce refinements, finding that only large language models (175B\nparameters) do so. Using only 100 samples of human-written feedback, our\nlearning algorithm finetunes a GPT-3 model to roughly human-level summarization\nability.", "collection_id": "f69dc1e3fd7feb8528a42f3d0096aae0b3f48765c10380bcf9235074912a91a1"}, "question": "How many parameters do large language models that accurately incorporate feedback have?\n", "answer": "Large language models that accurately incorporate feedback have 175B parameters."}
{"document": {"content": "We highlight an important frontier in algorithmic fairness: disparity in the\nquality of natural language processing algorithms when applied to language from\nauthors of different social groups. For example, current systems sometimes\nanalyze the language of females and minorities more poorly than they do of\nwhites and males. We conduct an empirical analysis of racial disparity in\nlanguage identification for tweets written in African-American English, and\ndiscuss implications of disparity in NLP.", "collection_id": "88c99a4cbeb067fb70398fc35e02cffab02283afec31606f6497a7e349e992b3"}, "question": "What is a significant issue with current natural language processing algorithms?\n", "answer": "A significant issue with current natural language processing algorithms is that they sometimes analyze the language of females and minorities more poorly than they do of whites and males, resulting in disparity in the quality of the algorithms when applied to language from authors of different social groups."}
{"document": {"content": "To support software developers in finding and fixing software bugs, several\nautomated program repair techniques have been introduced. Given a test suite,\nstandard methods usually either synthesize a repair, or navigate a search space\nof software edits to find test-suite passing variants. Recent program repair\nmethods are based on deep learning approaches. One of these novel methods,\nwhich is not primarily intended for automated program repair, but is still\nsuitable for it, is ChatGPT. The bug fixing performance of ChatGPT, however, is\nso far unclear. Therefore, in this paper we evaluate ChatGPT on the standard\nbug fixing benchmark set, QuixBugs, and compare the performance with the\nresults of several other approaches reported in the literature. We find that\nChatGPT's bug fixing performance is competitive to the common deep learning\napproaches CoCoNut and Codex and notably better than the results reported for\nthe standard program repair approaches. In contrast to previous approaches,\nChatGPT offers a dialogue system through which further information, e.g., the\nexpected output for a certain input or an observed error message, can be\nentered. By providing such hints to ChatGPT, its success rate can be further\nincreased, fixing 31 out of 40 bugs, outperforming state-of-the-art.", "collection_id": "16a1d42c4a91b8561061f17737c9c31c6927f783574fa7fe6f4f79af36200120"}, "question": "How many bugs can ChatGPT fix when provided with additional hints?\n", "answer": "When provided with additional hints, such as the expected output for a certain input or an observed error message, ChatGPT can fix 31 out of 40 bugs, outperforming state-of-the-art results in automated program repair."}
{"document": {"content": "Many intellectual endeavors require mathematical problem solving, but this\nskill remains beyond the capabilities of computers. To measure this ability in\nmachine learning models, we introduce MATH, a new dataset of 12,500 challenging\ncompetition mathematics problems. Each problem in MATH has a full step-by-step\nsolution which can be used to teach models to generate answer derivations and\nexplanations. To facilitate future research and increase accuracy on MATH, we\nalso contribute a large auxiliary pretraining dataset which helps teach models\nthe fundamentals of mathematics. Even though we are able to increase accuracy\non MATH, our results show that accuracy remains relatively low, even with\nenormous Transformer models. Moreover, we find that simply increasing budgets\nand model parameter counts will be impractical for achieving strong\nmathematical reasoning if scaling trends continue. While scaling Transformers\nis automatically solving most other text-based tasks, scaling is not currently\nsolving MATH. To have more traction on mathematical problem solving we will\nlikely need new algorithmic advancements from the broader research community.", "collection_id": "df1d7889cc20394b05e1c7dd5fece822479f737072124c133f0da9094f2165ac"}, "question": "How many challenging competition mathematics problems are included in the MATH dataset?\n", "answer": "The MATH dataset includes 12,500 challenging competition mathematics problems, each with a full step-by-step solution to help teach models to generate answer derivations and explanations."}
{"document": {"content": "Despite the success of large language models (LLMs) in various natural\nlanguage processing (NLP) tasks, the stored knowledge in these models may\ninevitably be incomplete, out-of-date, or incorrect. This motivates the need to\nutilize external knowledge to assist LLMs. Unfortunately, current methods for\nincorporating external knowledge often require additional training or\nfine-tuning, which can be costly and may not be feasible for LLMs. To address\nthis issue, we propose a novel post-processing approach, rethinking with\nretrieval (RR), which retrieves relevant external knowledge based on the\ndecomposed reasoning steps obtained from the chain-of-thought (CoT) prompting.\nThis lightweight approach does not require additional training or fine-tuning\nand is not limited by the input length of LLMs. We evaluate the effectiveness\nof RR through extensive experiments with GPT-3 on three complex reasoning\ntasks: commonsense reasoning, temporal reasoning, and tabular reasoning. Our\nresults show that RR can produce more faithful explanations and improve the\nperformance of LLMs.", "collection_id": "2e5f108c719ba55872e01a443f932ef461c1cbde9b0815f73617f6dd298f8dac"}, "question": "What are some limitations of the knowledge stored in large language models?\n", "answer": "The stored knowledge in large language models may inevitably be incomplete, out-of-date, or incorrect, which motivates the need to utilize external knowledge to assist these models."}
{"document": {"content": "Technology for language generation has advanced rapidly, spurred by\nadvancements in pre-training large models on massive amounts of data and the\nneed for intelligent agents to communicate in a natural manner. While\ntechniques can effectively generate fluent text, they can also produce\nundesirable societal biases that can have a disproportionately negative impact\non marginalized populations. Language generation presents unique challenges for\nbiases in terms of direct user interaction and the structure of decoding\ntechniques. To better understand these challenges, we present a survey on\nsocietal biases in language generation, focusing on how data and techniques\ncontribute to biases and progress towards reducing biases. Motivated by a lack\nof studies on biases from decoding techniques, we also conduct experiments to\nquantify the effects of these techniques. By further discussing general trends\nand open challenges, we call to attention promising directions for research and\nthe importance of fairness and inclusivity considerations for language\ngeneration applications.", "collection_id": "7ef168d4bdd2ccb3a53ff2f157e35f57362220346ff198c7823ddd2fc241c8da"}, "question": "What are the main challenges associated with societal biases in language generation?\n\n", "answer": "The main challenges associated with societal biases in language generation are related to direct user interaction and the structure of decoding techniques. These challenges arise from the fact that language generation models can produce undesirable societal biases that have a disproportionately negative impact on marginalized populations. Additionally, the lack of studies on biases from decoding techniques is a significant challenge that needs to be addressed."}
{"document": {"content": "Semi-supervised algorithms aim to learn prediction functions from a small set\nof labeled observations and a large set of unlabeled observations. Because this\nframework is relevant in many applications, they have received a lot of\ninterest in both academia and industry. Among the existing techniques,\nself-training methods have undoubtedly attracted greater attention in recent\nyears. These models are designed to find the decision boundary on low density\nregions without making additional assumptions about the data distribution, and\nuse the unsigned output score of a learned classifier, or its margin, as an\nindicator of confidence. The working principle of self-training algorithms is\nto learn a classifier iteratively by assigning pseudo-labels to the set of\nunlabeled training samples with a margin greater than a certain threshold. The\npseudo-labeled examples are then used to enrich the labeled training data and\nto train a new classifier in conjunction with the labeled training set. In this\npaper, we present self-training methods for binary and multi-class\nclassification; as well as their variants and two related approaches, namely\nconsistency-based approaches and transductive learning. We examine the impact\nof significant self-training features on various methods, using different\ngeneral and image classification benchmarks, and we discuss our ideas for\nfuture research in self-training. To the best of our knowledge, this is the\nfirst thorough and complete survey on this subject.", "collection_id": "5bb81b5ec09d7cc54fec1ae319054958f3a027ca33cc2daa0cbc622c71a9912d"}, "question": "What is the working principle of self-training algorithms?\n", "answer": "The working principle of self-training algorithms is to learn a classifier iteratively by assigning pseudo-labels to the set of unlabeled training samples with a margin greater than a certain threshold. These pseudo-labeled examples are then used to enrich the labeled training data and to train a new classifier in conjunction with the labeled training set."}
{"document": {"content": "We propose a benchmark to measure whether a language model is truthful in\ngenerating answers to questions. The benchmark comprises 817 questions that\nspan 38 categories, including health, law, finance and politics. We crafted\nquestions that some humans would answer falsely due to a false belief or\nmisconception. To perform well, models must avoid generating false answers\nlearned from imitating human texts. We tested GPT-3, GPT-Neo/J, GPT-2 and a\nT5-based model. The best model was truthful on 58% of questions, while human\nperformance was 94%. Models generated many false answers that mimic popular\nmisconceptions and have the potential to deceive humans. The largest models\nwere generally the least truthful. This contrasts with other NLP tasks, where\nperformance improves with model size. However, this result is expected if false\nanswers are learned from the training distribution. We suggest that scaling up\nmodels alone is less promising for improving truthfulness than fine-tuning\nusing training objectives other than imitation of text from the web.", "collection_id": "f0f23b2be60d470de2e26b99fb80fcac2b878efce894a133c4766936640232d2"}, "question": "What percentage of questions did the best language model answer truthfully in a benchmark test of truthfulness?\n\n", "answer": "The best language model was truthful on 58% of questions in a benchmark test of truthfulness, which is significantly lower than human performance at 94%. This suggests that even the best language models struggle to provide accurate and truthful answers, and may even generate false answers that mimic popular misconceptions."}
{"document": {"content": "Transformers are slow and memory-hungry on long sequences, since the time and\nmemory complexity of self-attention are quadratic in sequence length.\nApproximate attention methods have attempted to address this problem by trading\noff model quality to reduce the compute complexity, but often do not achieve\nwall-clock speedup. We argue that a missing principle is making attention\nalgorithms IO-aware -- accounting for reads and writes between levels of GPU\nmemory. We propose FlashAttention, an IO-aware exact attention algorithm that\nuses tiling to reduce the number of memory reads/writes between GPU high\nbandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of\nFlashAttention, showing that it requires fewer HBM accesses than standard\nattention, and is optimal for a range of SRAM sizes. We also extend\nFlashAttention to block-sparse attention, yielding an approximate attention\nalgorithm that is faster than any existing approximate attention method.\nFlashAttention trains Transformers faster than existing baselines: 15%\nend-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the\nMLPerf 1.1 training speed record, 3$\\times$ speedup on GPT-2 (seq. length 1K),\nand 2.4$\\times$ speedup on long-range arena (seq. length 1K-4K). FlashAttention\nand block-sparse FlashAttention enable longer context in Transformers, yielding\nhigher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on\nlong-document classification) and entirely new capabilities: the first\nTransformers to achieve better-than-chance performance on the Path-X challenge\n(seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1%\naccuracy).", "collection_id": "2b6894cffd9fa4eb100d79b05b6fcfb194877d5ea66ac77879ba269d3cbe021c"}, "question": "What is the end-to-end wall-clock speedup achieved by FlashAttention on BERT-large compared to the MLPerf 1.1 training speed record?\n\n", "answer": "FlashAttention achieves a 15% end-to-end wall-clock speedup on BERT-large with a sequence length of 512 compared to the MLPerf 1.1 training speed record."}
{"document": {"content": "In this paper, we study the problem of learning vision-based dynamic\nmanipulation skills using a scalable reinforcement learning approach. We study\nthis problem in the context of grasping, a longstanding challenge in robotic\nmanipulation. In contrast to static learning behaviors that choose a grasp\npoint and then execute the desired grasp, our method enables closed-loop\nvision-based control, whereby the robot continuously updates its grasp strategy\nbased on the most recent observations to optimize long-horizon grasp success.\nTo that end, we introduce QT-Opt, a scalable self-supervised vision-based\nreinforcement learning framework that can leverage over 580k real-world grasp\nattempts to train a deep neural network Q-function with over 1.2M parameters to\nperform closed-loop, real-world grasping that generalizes to 96% grasp success\non unseen objects. Aside from attaining a very high success rate, our method\nexhibits behaviors that are quite distinct from more standard grasping systems:\nusing only RGB vision-based perception from an over-the-shoulder camera, our\nmethod automatically learns regrasping strategies, probes objects to find the\nmost effective grasps, learns to reposition objects and perform other\nnon-prehensile pre-grasp manipulations, and responds dynamically to\ndisturbances and perturbations.", "collection_id": "88c24c3945ca1bc512ff098cb6c52079247c7b1241384fd44aa09c72390c85de"}, "question": "What is QT-Opt in the context of robotic manipulation?\n", "answer": "QT-Opt is a scalable self-supervised vision-based reinforcement learning framework that enables closed-loop vision-based control for robots to learn dynamic manipulation skills, such as grasping. It leverages a large dataset of real-world grasp attempts to train a deep neural network Q-function, allowing robots to perform closed-loop, real-world grasping that generalizes to a high success rate on unseen objects."}
{"document": {"content": "Machine translation (MT) technology has facilitated our daily tasks by\nproviding accessible shortcuts for gathering, elaborating and communicating\ninformation. However, it can suffer from biases that harm users and society at\nlarge. As a relatively new field of inquiry, gender bias in MT still lacks\ninternal cohesion, which advocates for a unified framework to ease future\nresearch. To this end, we: i) critically review current conceptualizations of\nbias in light of theoretical insights from related disciplines, ii) summarize\nprevious analyses aimed at assessing gender bias in MT, iii) discuss the\nmitigating strategies proposed so far, and iv) point toward potential\ndirections for future work.", "collection_id": "a1d8069ee24ec3a2f75959ca272652c0c657993ae632939f62c7eb90b83a5548"}, "question": "What are the main goals of research on gender bias in machine translation?\n", "answer": "The main goals of research on gender bias in machine translation include critically reviewing current conceptualizations of bias, summarizing previous analyses aimed at assessing gender bias, discussing mitigating strategies, and pointing toward potential directions for future work to establish a unified framework for future research."}
{"document": {"content": "Neural network training relies on our ability to find \"good\" minimizers of\nhighly non-convex loss functions. It is well-known that certain network\narchitecture designs (e.g., skip connections) produce loss functions that train\neasier, and well-chosen training parameters (batch size, learning rate,\noptimizer) produce minimizers that generalize better. However, the reasons for\nthese differences, and their effects on the underlying loss landscape, are not\nwell understood. In this paper, we explore the structure of neural loss\nfunctions, and the effect of loss landscapes on generalization, using a range\nof visualization methods. First, we introduce a simple \"filter normalization\"\nmethod that helps us visualize loss function curvature and make meaningful\nside-by-side comparisons between loss functions. Then, using a variety of\nvisualizations, we explore how network architecture affects the loss landscape,\nand how training parameters affect the shape of minimizers.", "collection_id": "5bad9fc52635a9aa6c42dc847baee831dc3b000b3facb64c24014cca709fb87c"}, "question": "What methods are used to explore the structure of neural loss functions and their effects on generalization?\n\n", "answer": "A range of visualization methods are used to explore the structure of neural loss functions and their effects on generalization. Specifically, a \"filter normalization\" method is introduced to help visualize loss function curvature and make meaningful comparisons between loss functions. This method, combined with various visualizations, allows researchers to examine how network architecture affects the loss landscape and how training parameters impact the shape of minimizers."}
{"document": {"content": "Model parallelism has become a necessity for training modern large-scale deep\nlanguage models. In this work, we identify a new and orthogonal dimension from\nexisting model parallel approaches: it is possible to perform pipeline\nparallelism within a single training sequence for Transformer-based language\nmodels thanks to its autoregressive property. This enables a more fine-grained\npipeline compared with previous work. With this key idea, we design TeraPipe, a\nhigh-performance token-level pipeline parallel algorithm for synchronous\nmodel-parallel training of Transformer-based language models. We develop a\nnovel dynamic programming-based algorithm to calculate the optimal pipelining\nexecution scheme given a specific model and cluster configuration. We show that\nTeraPipe can speed up the training by 5.0x for the largest GPT-3 model with 175\nbillion parameters on an AWS cluster with 48 p3.16xlarge instances compared\nwith state-of-the-art model-parallel methods. The code for reproduction can be\nfound at https://github.com/zhuohan123/terapipe", "collection_id": "2d014668d9abca94e675562d1c8785c2cba48f07a3617bf0db5a463b633041d3"}, "question": "How much faster is TeraPipe compared to state-of-the-art model-parallel methods for training the largest GPT-3 model?\n", "answer": "TeraPipe can speed up the training of the largest GPT-3 model with 175 billion parameters by 5.0x compared to state-of-the-art model-parallel methods on an AWS cluster with 48 p3.16xlarge instances."}
{"document": {"content": "Ideally Open-Domain Question Answering models should exhibit a number of\ncompetencies, ranging from simply memorizing questions seen at training time,\nto answering novel question formulations with answers seen during training, to\ngeneralizing to completely novel questions with novel answers. However, single\naggregated test set scores do not show the full picture of what capabilities\nmodels truly have. In this work, we perform a detailed study of the test sets\nof three popular open-domain benchmark datasets with respect to these\ncompetencies. We find that 60-70% of test-time answers are also present\nsomewhere in the training sets. We also find that 30% of test-set questions\nhave a near-duplicate paraphrase in their corresponding training sets. Using\nthese findings, we evaluate a variety of popular open-domain models to obtain\ngreater insight into what extent they can actually generalize, and what drives\ntheir overall performance. We find that all models perform dramatically worse\non questions that cannot be memorized from training sets, with a mean absolute\nperformance difference of 63% between repeated and non-repeated data. Finally\nwe show that simple nearest-neighbor models out-perform a BART closed-book QA\nmodel, further highlighting the role that training set memorization plays in\nthese benchmarks", "collection_id": "01e6fb18d94038f285f6465605a2de083c16c54b5f24f9ecebef77e5da1e96fe"}, "question": "What percentage of test-set questions have a near-duplicate paraphrase in their corresponding training sets?\n", "answer": "30% of test-set questions have a near-duplicate paraphrase in their corresponding training sets. This suggests that a significant portion of questions tested in open-domain benchmark datasets may be answerable through memorization rather than true generalization capabilities."}
{"document": {"content": "Language models (LMs) exhibit remarkable abilities to solve new tasks from\njust a few examples or textual instructions, especially at scale. They also,\nparadoxically, struggle with basic functionality, such as arithmetic or factual\nlookup, where much simpler and smaller models excel. In this paper, we show\nthat LMs can teach themselves to use external tools via simple APIs and achieve\nthe best of both worlds. We introduce Toolformer, a model trained to decide\nwhich APIs to call, when to call them, what arguments to pass, and how to best\nincorporate the results into future token prediction. This is done in a\nself-supervised way, requiring nothing more than a handful of demonstrations\nfor each API. We incorporate a range of tools, including a calculator, a Q\\&A\nsystem, two different search engines, a translation system, and a calendar.\nToolformer achieves substantially improved zero-shot performance across a\nvariety of downstream tasks, often competitive with much larger models, without\nsacrificing its core language modeling abilities.", "collection_id": "26327c6b9c4122855f0cf3c1a8e22f949c32c278ab2f3b43ddb253e75f047c20"}, "question": "What is Toolformer in the context of language models?\n", "answer": "Toolformer is a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction, allowing language models to use external tools and achieve improved performance across various tasks."}
{"document": {"content": "Contextual word embeddings such as BERT have achieved state of the art\nperformance in numerous NLP tasks. Since they are optimized to capture the\nstatistical properties of training data, they tend to pick up on and amplify\nsocial stereotypes present in the data as well. In this study, we (1)~propose a\ntemplate-based method to quantify bias in BERT; (2)~show that this method\nobtains more consistent results in capturing social biases than the traditional\ncosine based method; and (3)~conduct a case study, evaluating gender bias in a\ndownstream task of Gender Pronoun Resolution. Although our case study focuses\non gender bias, the proposed technique is generalizable to unveiling other\nbiases, including in multiclass settings, such as racial and religious biases.", "collection_id": "43db9011c2d3dbf163afe9e10c0df4f5af37984e221b8edd59e7df20060cb308"}, "question": "What type of biases can the proposed template-based method for quantifying bias in BERT unveil?\n", "answer": "The proposed technique is generalizable to unveiling various biases, including gender bias, racial bias, and religious bias, and can be applied in both binary and multiclass settings."}
{"document": {"content": "We present a large, tunable neural conversational response generation model,\nDialoGPT (dialogue generative pre-trained transformer). Trained on 147M\nconversation-like exchanges extracted from Reddit comment chains over a period\nspanning from 2005 through 2017, DialoGPT extends the Hugging Face PyTorch\ntransformer to attain a performance close to human both in terms of automatic\nand human evaluation in single-turn dialogue settings. We show that\nconversational systems that leverage DialoGPT generate more relevant,\ncontentful and context-consistent responses than strong baseline systems. The\npre-trained model and training pipeline are publicly released to facilitate\nresearch into neural response generation and the development of more\nintelligent open-domain dialogue systems.", "collection_id": "22a498fd5b2e0d67ecfc525c4942e0aa7460a274f220e67f0c619452e00ead07"}, "question": "What dataset was used to train the DialoGPT model?\n", "answer": "The DialoGPT model was trained on 147M conversation-like exchanges extracted from Reddit comment chains over a period spanning from 2005 through 2017."}
{"document": {"content": "Algorithms are widely applied to detect hate speech and abusive language in\nsocial media. We investigated whether the human-annotated data used to train\nthese algorithms are biased. We utilized a publicly available annotated Twitter\ndataset (Founta et al. 2018) and classified the racial, gender, and party\nidentification dimensions of 99,996 tweets. The results showed that African\nAmerican tweets were up to 3.7 times more likely to be labeled as abusive, and\nAfrican American male tweets were up to 77% more likely to be labeled as\nhateful compared to the others. These patterns were statistically significant\nand robust even when party identification was added as a control variable. This\nstudy provides the first systematic evidence on intersectional bias in datasets\nof hate speech and abusive language.", "collection_id": "81d8407fc83076c19726116f539c94ce7b47c2f63ca9767c8b1dfaf4d4174738"}, "question": "Are African American tweets more likely to be labeled as abusive compared to other tweets?\n", "answer": "Yes, according to a study on a publicly available annotated Twitter dataset, African American tweets were found to be up to 3.7 times more likely to be labeled as abusive compared to other tweets. This pattern was statistically significant and remained even when party identification was added as a control variable."}
{"document": {"content": "There has been a recent resurgence in the area of explainable artificial\nintelligence as researchers and practitioners seek to make their algorithms\nmore understandable. Much of this research is focused on explicitly explaining\ndecisions or actions to a human observer, and it should not be controversial to\nsay that looking at how humans explain to each other can serve as a useful\nstarting point for explanation in artificial intelligence. However, it is fair\nto say that most work in explainable artificial intelligence uses only the\nresearchers' intuition of what constitutes a `good' explanation. There exists\nvast and valuable bodies of research in philosophy, psychology, and cognitive\nscience of how people define, generate, select, evaluate, and present\nexplanations, which argues that people employ certain cognitive biases and\nsocial expectations towards the explanation process. This paper argues that the\nfield of explainable artificial intelligence should build on this existing\nresearch, and reviews relevant papers from philosophy, cognitive\npsychology/science, and social psychology, which study these topics. It draws\nout some important findings, and discusses ways that these can be infused with\nwork on explainable artificial intelligence.", "collection_id": "1feea7f5c09cd198df576ce56a9b92247a72fca0df9263cb98853ea169ac730c"}, "question": "What fields of research study how people define, generate, select, evaluate, and present explanations?\n\n", "answer": "There are several fields of research that study how people define, generate, select, evaluate, and present explanations, including philosophy, psychology, and cognitive science. These fields have identified that people employ certain cognitive biases and social expectations towards the explanation process, which can inform the development of explainable artificial intelligence."}
{"document": {"content": "Mahalanobis distance (MD) is a simple and popular post-processing method for\ndetecting out-of-distribution (OOD) inputs in neural networks. We analyze its\nfailure modes for near-OOD detection and propose a simple fix called relative\nMahalanobis distance (RMD) which improves performance and is more robust to\nhyperparameter choice. On a wide selection of challenging vision, language, and\nbiology OOD benchmarks (CIFAR-100 vs CIFAR-10, CLINC OOD intent detection,\nGenomics OOD), we show that RMD meaningfully improves upon MD performance (by\nup to 15% AUROC on genomics OOD).", "collection_id": "b8c06939003270fe95fe758283cd7db9d2211c7cbcf12dfcdfa4120856fff943"}, "question": "What is the maximum percentage improvement of AUROC that Relative Mahalanobis Distance achieves over Mahalanobis Distance in OOD detection?\n", "answer": "Relative Mahalanobis Distance (RMD) achieves a maximum improvement of up to 15% AUROC over Mahalanobis Distance (MD) in out-of-distribution (OOD) detection, specifically on genomics OOD benchmarks."}
{"document": {"content": "Generative models for open domain question answering have proven to be\ncompetitive, without resorting to external knowledge. While promising, this\napproach requires to use models with billions of parameters, which are\nexpensive to train and query. In this paper, we investigate how much these\nmodels can benefit from retrieving text passages, potentially containing\nevidence. We obtain state-of-the-art results on the Natural Questions and\nTriviaQA open benchmarks. Interestingly, we observe that the performance of\nthis method significantly improves when increasing the number of retrieved\npassages. This is evidence that generative models are good at aggregating and\ncombining evidence from multiple passages.", "collection_id": "29dd4fead1c6f1c1f1729bdaae8cf7707fd4f2a7daf799f1dff24d958bcbf6fe"}, "question": "What are the benefits of retrieving text passages for generative models in open domain question answering?\n\n", "answer": "The benefits of retrieving text passages for generative models in open domain question answering include achieving state-of-the-art results on benchmarks such as Natural Questions and TriviaQA. Additionally, increasing the number of retrieved passages significantly improves the performance of the method, indicating that generative models are effective at aggregating and combining evidence from multiple passages. This approach allows models to access external knowledge without requiring a large number of parameters, making them less expensive to train and query."}
{"document": {"content": "We study the design decisions of publicly available instruction tuning\nmethods, and break down the development of Flan 2022 (Chung et al., 2022).\nThrough careful ablation studies on the Flan Collection of tasks and methods,\nwe tease apart the effect of design decisions which enable Flan-T5 to\noutperform prior work by 3-17%+ across evaluation settings. We find task\nbalancing and enrichment techniques are overlooked but critical to effective\ninstruction tuning, and in particular, training with mixed prompt settings\n(zero-shot, few-shot, and chain-of-thought) actually yields stronger (2%+)\nperformance in all settings. In further experiments, we show Flan-T5 requires\nless finetuning to converge higher and faster than T5 on single downstream\ntasks, motivating instruction-tuned models as more computationally-efficient\nstarting checkpoints for new tasks. Finally, to accelerate research on\ninstruction tuning, we make the Flan 2022 collection of datasets, templates,\nand methods publicly available at\nhttps://github.com/google-research/FLAN/tree/main/flan/v2.", "collection_id": "760ea88190342916b926bf48e14d34a429ebfe7e66f45b8de1698d7a8b5eec00"}, "question": "What techniques are found to be overlooked but critical to effective instruction tuning?\n", "answer": "Task balancing and enrichment techniques are found to be overlooked but critical to effective instruction tuning."}
{"document": {"content": "Building on Petroni et al. (2019), we propose two new probing tasks analyzing\nfactual knowledge stored in Pretrained Language Models (PLMs). (1) Negation. We\nfind that PLMs do not distinguish between negated (\"Birds cannot [MASK]\") and\nnon-negated (\"Birds can [MASK]\") cloze questions. (2) Mispriming. Inspired by\npriming methods in human psychology, we add \"misprimes\" to cloze questions\n(\"Talk? Birds can [MASK]\"). We find that PLMs are easily distracted by\nmisprimes. These results suggest that PLMs still have a long way to go to\nadequately learn human-like factual knowledge.", "collection_id": "02ef8fe5452d00714db3b4e16b2a029d64693552eecaebb5be81109a456fe933"}, "question": "What are some limitations of Pretrained Language Models in terms of factual knowledge?\n\n", "answer": "Pretrained Language Models (PLMs) have been found to have limitations in terms of factual knowledge, specifically in two areas: (1) Negation - PLMs do not distinguish between negated and non-negated cloze questions, and (2) Mispriming - PLMs are easily distracted by \"misprimes\" added to cloze questions, suggesting that they still have a long way to go to adequately learn human-like factual knowledge."}
{"document": {"content": "This survey reviews works in which language models (LMs) are augmented with\nreasoning skills and the ability to use tools. The former is defined as\ndecomposing a potentially complex task into simpler subtasks while the latter\nconsists in calling external modules such as a code interpreter. LMs can\nleverage these augmentations separately or in combination via heuristics, or\nlearn to do so from demonstrations. While adhering to a standard missing tokens\nprediction objective, such augmented LMs can use various, possibly\nnon-parametric external modules to expand their context processing ability,\nthus departing from the pure language modeling paradigm. We therefore refer to\nthem as Augmented Language Models (ALMs). The missing token objective allows\nALMs to learn to reason, use tools, and even act, while still performing\nstandard natural language tasks and even outperforming most regular LMs on\nseveral benchmarks. In this work, after reviewing current advance in ALMs, we\nconclude that this new research direction has the potential to address common\nlimitations of traditional LMs such as interpretability, consistency, and\nscalability issues.", "collection_id": "70620d62bcbfe7462b1edde7fa95b81e6d04e33b34d3c61deef15c27063000c0"}, "question": "What is an Augmented Language Model (ALM) in the context of artificial intelligence?\n", "answer": "An Augmented Language Model (ALM) is a type of language model that has been enhanced with reasoning skills and the ability to use external tools, such as code interpreters, to expand its context processing ability. This allows ALMs to learn to reason, use tools, and perform actions while still carrying out standard natural language tasks, often outperforming traditional language models on various benchmarks."}
{"document": {"content": "Recent studies have alarmed that many online hate speeches are implicit. With\nits subtle nature, the explainability of the detection of such hateful speech\nhas been a challenging problem. In this work, we examine whether ChatGPT can be\nused for providing natural language explanations (NLEs) for implicit hateful\nspeech detection. We design our prompt to elicit concise ChatGPT-generated NLEs\nand conduct user studies to evaluate their qualities by comparison with\nhuman-written NLEs. We discuss the potential and limitations of ChatGPT in the\ncontext of implicit hateful speech research.", "collection_id": "718b1501a67856adc5ceb6a07df729a28e72375b4656a9196632f30c60719a3a"}, "question": "What is a challenging problem in the detection of implicit hateful speech?\n", "answer": "The explainability of the detection of implicit hateful speech is a challenging problem due to its subtle nature."}
{"document": {"content": "GPT-3 can perform numerous tasks when provided a natural language prompt that\ncontains a few training examples. We show that this type of few-shot learning\ncan be unstable: the choice of prompt format, training examples, and even the\norder of the training examples can cause accuracy to vary from near chance to\nnear state-of-the-art. We demonstrate that this instability arises from the\nbias of language models towards predicting certain answers, e.g., those that\nare placed near the end of the prompt or are common in the pre-training data.\nTo mitigate this, we first estimate the model's bias towards each answer by\nasking for its prediction when given the training prompt and a content-free\ntest input such as \"N/A\". We then fit calibration parameters that cause the\nprediction for this input to be uniform across answers. On a diverse set of\ntasks, this contextual calibration procedure substantially improves GPT-3 and\nGPT-2's average accuracy (up to 30.0% absolute) and reduces variance across\ndifferent choices of the prompt.", "collection_id": "3f1d00868ae8003418b9aee8d70645e73b28de2061847f2cbb2e6489553557d3"}, "question": "How much can contextual calibration procedure improve the average accuracy of GPT-3 and GPT-2 models?\n", "answer": "The contextual calibration procedure can substantially improve GPT-3 and GPT-2's average accuracy by up to 30.0% absolute, and also reduce variance across different choices of the prompt."}
{"document": {"content": "In an increasing number of domains it has been demonstrated that deep\nlearning models can be trained using relatively large batch sizes without\nsacrificing data efficiency. However the limits of this massive data\nparallelism seem to differ from domain to domain, ranging from batches of tens\nof thousands in ImageNet to batches of millions in RL agents that play the game\nDota 2. To our knowledge there is limited conceptual understanding of why these\nlimits to batch size differ or how we might choose the correct batch size in a\nnew domain. In this paper, we demonstrate that a simple and easy-to-measure\nstatistic called the gradient noise scale predicts the largest useful batch\nsize across many domains and applications, including a number of supervised\nlearning datasets (MNIST, SVHN, CIFAR-10, ImageNet, Billion Word),\nreinforcement learning domains (Atari and Dota), and even generative model\ntraining (autoencoders on SVHN). We find that the noise scale increases as the\nloss decreases over a training run and depends on the model size primarily\nthrough improved model performance. Our empirically-motivated theory also\ndescribes the tradeoff between compute-efficiency and time-efficiency, and\nprovides a rough model of the benefits of adaptive batch-size training.", "collection_id": "05db703938271ea5ed845a31956a2392b1da5b172578a3f2363044c05c81d455"}, "question": "What are some domains where deep learning models can be trained using large batch sizes without sacrificing data efficiency?\n", "answer": "Domains where deep learning models can be trained using large batch sizes without sacrificing data efficiency include, but are not limited to, ImageNet (with batches of tens of thousands), and RL agents that play the game Dota 2 (with batches of millions), as well as various supervised learning datasets (MNIST, SVHN, CIFAR-10, Billion Word), reinforcement learning domains (Atari), and generative model training (autoencoders on SVHN)."}
{"document": {"content": "Dialogue systems in the form of chatbots and personal assistants are being\nincreasingly integrated into people's lives. Modern dialogue systems may\nconsider adopting anthropomorphic personas, mimicking societal demographic\ngroups to appear more approachable and trustworthy to users. However, the\nadoption of a persona can result in the adoption of biases. In this paper, we\npresent the first large-scale study on persona biases in dialogue systems and\nconduct analyses on personas of different social classes, sexual orientations,\nraces, and genders. We define persona biases as harmful differences in\nresponses (e.g., varying levels of offensiveness, agreement with harmful\nstatements) generated from adopting different demographic personas.\nFurthermore, we introduce an open-source framework, UnitPersonaBias, to explore\nand aggregate persona biases in dialogue systems. By analyzing the Blender and\nDialoGPT dialogue systems, we observe that adopting personas can actually\ndecrease harmful responses, compared to not using any personas. Additionally,\nwe find that persona choices can affect the degree of harms in generated\nresponses and thus should be systematically evaluated before deployment. We\nalso analyze how personas can result in different amounts of harm towards\nspecific demographics.", "collection_id": "24ca58154695e9edcf4641c14923c1c131cc046928b9e651d957ef783bbc2e32"}, "question": "What is the potential effect of adopting personas in dialogue systems on the generation of harmful responses?\n\n", "answer": "Adopting personas in dialogue systems can actually decrease the generation of harmful responses compared to not using any personas. However, the choice of persona can also affect the degree of harm in the generated responses, emphasizing the need for systematic evaluation before deployment."}
{"document": {"content": "Contrastive models like CLIP have been shown to learn robust representations\nof images that capture both semantics and style. To leverage these\nrepresentations for image generation, we propose a two-stage model: a prior\nthat generates a CLIP image embedding given a text caption, and a decoder that\ngenerates an image conditioned on the image embedding. We show that explicitly\ngenerating image representations improves image diversity with minimal loss in\nphotorealism and caption similarity. Our decoders conditioned on image\nrepresentations can also produce variations of an image that preserve both its\nsemantics and style, while varying the non-essential details absent from the\nimage representation. Moreover, the joint embedding space of CLIP enables\nlanguage-guided image manipulations in a zero-shot fashion. We use diffusion\nmodels for the decoder and experiment with both autoregressive and diffusion\nmodels for the prior, finding that the latter are computationally more\nefficient and produce higher-quality samples.", "collection_id": "467089940b89b390f118b263e50abeffb793892ebeeea3e3b39e2cce58762953"}, "question": "What type of models are used for the decoder in the proposed two-stage model for image generation?\n", "answer": "The proposed two-stage model for image generation utilizes diffusion models for the decoder. These models generate an image conditioned on a given image embedding, which is produced by a prior model based on a text caption."}
{"document": {"content": "We present a very simple algorithm for attention that requires $O(1)$ memory\nwith respect to sequence length and an extension to self-attention that\nrequires $O(\\log n)$ memory. This is in contrast with the frequently stated\nbelief that self-attention requires $O(n^2)$ memory. While the time complexity\nis still $O(n^2)$, device memory rather than compute capability is often the\nlimiting factor on modern accelerators. Thus, reducing the memory requirements\nof attention allows processing of longer sequences than might otherwise be\nfeasible. We provide a practical implementation for accelerators that requires\n$O(\\sqrt{n})$ memory, is numerically stable, and is within a few percent of the\nruntime of the standard implementation of attention. We also demonstrate how to\ndifferentiate the function while remaining memory-efficient. For sequence\nlength 16384, the memory overhead of self-attention is reduced by 59X for\ninference and by 32X for differentiation.", "collection_id": "5ce67181f3e9689bb6cbfc6f6d3af4cede9bed5f426dde1c30767c355d928d1a"}, "question": "What is the reduction in memory overhead of self-attention for sequence length 16384 during inference?\n", "answer": "The memory overhead of self-attention is reduced by 59X for inference when the sequence length is 16384."}
{"document": {"content": "Generative AI systems across modalities, ranging from text, image, audio, and\nvideo, have broad social impacts, but there exists no official standard for\nmeans of evaluating those impacts and which impacts should be evaluated. We\nmove toward a standard approach in evaluating a generative AI system for any\nmodality, in two overarching categories: what is able to be evaluated in a base\nsystem that has no predetermined application and what is able to be evaluated\nin society. We describe specific social impact categories and how to approach\nand conduct evaluations in the base technical system, then in people and\nsociety. Our framework for a base system defines seven categories of social\nimpact: bias, stereotypes, and representational harms; cultural values and\nsensitive content; disparate performance; privacy and data protection;\nfinancial costs; environmental costs; and data and content moderation labor\ncosts. Suggested methods for evaluation apply to all modalities and analyses of\nthe limitations of existing evaluations serve as a starting point for necessary\ninvestment in future evaluations. We offer five overarching categories for what\nis able to be evaluated in society, each with their own subcategories:\ntrustworthiness and autonomy; inequality, marginalization, and violence;\nconcentration of authority; labor and creativity; and ecosystem and\nenvironment. Each subcategory includes recommendations for mitigating harm. We\nare concurrently crafting an evaluation repository for the AI research\ncommunity to contribute existing evaluations along the given categories. This\nversion will be updated following a CRAFT session at ACM FAccT 2023.", "collection_id": "de9ba340246b4e9a1de3a9b4016285dfb280ff53b31f8bc1bb9d258df51c582c"}, "question": "What are the seven categories of social impact defined for a base generative AI system?\n\n", "answer": "The seven categories of social impact defined for a base generative AI system are: \n1. Bias, stereotypes, and representational harms \n2. Cultural values and sensitive content \n3. Disparate performance \n4. Privacy and data protection \n5. Financial costs \n6. Environmental costs \n7. Data and content moderation labor costs."}
{"document": {"content": "Recent works have highlighted the strength of the Transformer architecture on\nsequence tasks while, at the same time, neural architecture search (NAS) has\nbegun to outperform human-designed models. Our goal is to apply NAS to search\nfor a better alternative to the Transformer. We first construct a large search\nspace inspired by the recent advances in feed-forward sequence models and then\nrun evolutionary architecture search with warm starting by seeding our initial\npopulation with the Transformer. To directly search on the computationally\nexpensive WMT 2014 English-German translation task, we develop the Progressive\nDynamic Hurdles method, which allows us to dynamically allocate more resources\nto more promising candidate models. The architecture found in our experiments\n-- the Evolved Transformer -- demonstrates consistent improvement over the\nTransformer on four well-established language tasks: WMT 2014 English-German,\nWMT 2014 English-French, WMT 2014 English-Czech and LM1B. At a big model size,\nthe Evolved Transformer establishes a new state-of-the-art BLEU score of 29.8\non WMT'14 English-German; at smaller sizes, it achieves the same quality as the\noriginal \"big\" Transformer with 37.6% less parameters and outperforms the\nTransformer by 0.7 BLEU at a mobile-friendly model size of 7M parameters.", "collection_id": "33839f4252f3f5e9e1a9e8c3f604d172903b09e85dbd97c2d904dbe3292dacb6"}, "question": "What is the best BLEU score achieved by the Evolved Transformer on the WMT'14 English-German task?\n", "answer": "The Evolved Transformer establishes a new state-of-the-art BLEU score of 29.8 on WMT'14 English-German at a big model size."}
{"document": {"content": "Large transformer-based language models (LMs) trained on huge text corpora\nhave shown unparalleled generation capabilities. However, controlling\nattributes of the generated language (e.g. switching topic or sentiment) is\ndifficult without modifying the model architecture or fine-tuning on\nattribute-specific data and entailing the significant cost of retraining. We\npropose a simple alternative: the Plug and Play Language Model (PPLM) for\ncontrollable language generation, which combines a pretrained LM with one or\nmore simple attribute classifiers that guide text generation without any\nfurther training of the LM. In the canonical scenario we present, the attribute\nmodels are simple classifiers consisting of a user-specified bag of words or a\nsingle learned layer with 100,000 times fewer parameters than the LM. Sampling\nentails a forward and backward pass in which gradients from the attribute model\npush the LM's hidden activations and thus guide the generation. Model samples\ndemonstrate control over a range of topics and sentiment styles, and extensive\nautomated and human annotated evaluations show attribute alignment and fluency.\nPPLMs are flexible in that any combination of differentiable attribute models\nmay be used to steer text generation, which will allow for diverse and creative\napplications beyond the examples given in this paper.", "collection_id": "31ea01c7663e057b3da42ca2fd650d7c40a68e42c31b4844880669d565df6f87"}, "question": "What is the Plug and Play Language Model (PPLM) used for?\n", "answer": "The Plug and Play Language Model (PPLM) is used for controllable language generation, allowing users to control attributes of the generated language, such as topic or sentiment, without modifying the model architecture or fine-tuning on attribute-specific data."}
{"document": {"content": "This paper aims to help structure the risk landscape associated with\nlarge-scale Language Models (LMs). In order to foster advances in responsible\ninnovation, an in-depth understanding of the potential risks posed by these\nmodels is needed. A wide range of established and anticipated risks are\nanalysed in detail, drawing on multidisciplinary expertise and literature from\ncomputer science, linguistics, and social sciences.\n  We outline six specific risk areas: I. Discrimination, Exclusion and\nToxicity, II. Information Hazards, III. Misinformation Harms, V. Malicious\nUses, V. Human-Computer Interaction Harms, VI. Automation, Access, and\nEnvironmental Harms. The first area concerns the perpetuation of stereotypes,\nunfair discrimination, exclusionary norms, toxic language, and lower\nperformance by social group for LMs. The second focuses on risks from private\ndata leaks or LMs correctly inferring sensitive information. The third\naddresses risks arising from poor, false or misleading information including in\nsensitive domains, and knock-on risks such as the erosion of trust in shared\ninformation. The fourth considers risks from actors who try to use LMs to cause\nharm. The fifth focuses on risks specific to LLMs used to underpin\nconversational agents that interact with human users, including unsafe use,\nmanipulation or deception. The sixth discusses the risk of environmental harm,\njob automation, and other challenges that may have a disparate effect on\ndifferent social groups or communities.\n  In total, we review 21 risks in-depth. We discuss the points of origin of\ndifferent risks and point to potential mitigation approaches. Lastly, we\ndiscuss organisational responsibilities in implementing mitigations, and the\nrole of collaboration and participation. We highlight directions for further\nresearch, particularly on expanding the toolkit for assessing and evaluating\nthe outlined risks in LMs.", "collection_id": "04b47d3650202378915dcbd2c02611c433de4906c95f8c2f887a6e4d92eea080"}, "question": "What are the six specific risk areas associated with large-scale Language Models?\n\n", "answer": "The six specific risk areas associated with large-scale Language Models are: \n1. Discrimination, Exclusion and Toxicity, which concerns the perpetuation of stereotypes, unfair discrimination, exclusionary norms, toxic language, and lower performance by social group for LMs.\n2. Information Hazards, which focuses on risks from private data leaks or LMs correctly inferring sensitive information.\n3. Misinformation Harms, which addresses risks arising from poor, false or misleading information including in sensitive domains, and knock-on risks such as the erosion of trust in shared information.\n4. Malicious Uses, which considers risks from actors who try to use LMs to cause harm.\n5. Human-Computer Interaction Harms, which focuses on risks specific to LLMs used to underpin conversational agents that interact with human users, including unsafe use, manipulation or deception.\n6. Automation, Access, and Environmental Harms, which discusses the risk of environmental harm, job automation, and other challenges that may have a disparate effect on different social groups or communities."}
{"document": {"content": "Books are a rich source of both fine-grained information, how a character, an\nobject or a scene looks like, as well as high-level semantics, what someone is\nthinking, feeling and how these states evolve through a story. This paper aims\nto align books to their movie releases in order to provide rich descriptive\nexplanations for visual content that go semantically far beyond the captions\navailable in current datasets. To align movies and books we exploit a neural\nsentence embedding that is trained in an unsupervised way from a large corpus\nof books, as well as a video-text neural embedding for computing similarities\nbetween movie clips and sentences in the book. We propose a context-aware CNN\nto combine information from multiple sources. We demonstrate good quantitative\nperformance for movie/book alignment and show several qualitative examples that\nshowcase the diversity of tasks our model can be used for.", "collection_id": "46bf8937dd46f570ce25d585bd9bc500325de321142e545e8f558582ebab31fe"}, "question": "What is the purpose of aligning books to their movie releases in the context of artificial intelligence research?\n\n", "answer": "The purpose of aligning books to their movie releases is to provide rich descriptive explanations for visual content that go semantically far beyond the captions available in current datasets. This alignment aims to leverage the detailed information present in books, including fine-grained descriptions and high-level semantics, to enhance the understanding of visual content in movies."}
{"document": {"content": "Recent studies report that autoregressive language models can successfully\nsolve many NLP tasks via zero- and few-shot learning paradigms, which opens up\nnew possibilities for using the pre-trained language models. This paper\nintroduces two autoregressive GPT-like models with 1.3 billion and 13 billion\nparameters trained on 60 languages from 25 language families using Wikipedia\nand Colossal Clean Crawled Corpus. We reproduce the GPT-3 architecture using\nGPT-2 sources and the sparse attention mechanism; Deepspeed and Megatron\nframeworks allow us to parallelize the training and inference steps\neffectively. The resulting models show performance on par with the recently\nreleased XGLM models by Facebook, covering more languages and enhancing NLP\npossibilities for low resource languages of CIS countries and Russian small\nnations. We detail the motivation for the choices of the architecture design,\nthoroughly describe the data preparation pipeline, and train five small\nversions of the model to choose the most optimal multilingual tokenization\nstrategy. We measure the model perplexity in all covered languages and evaluate\nit on the wide spectre of multilingual tasks, including classification,\ngenerative, sequence labeling and knowledge probing. The models were evaluated\nwith the zero-shot and few-shot methods. Furthermore, we compared the\nclassification tasks with the state-of-the-art multilingual model XGLM. source\ncode and the mGPT XL model are publicly released.", "collection_id": "242a2fa2ffae3e54ee9d93da07c97bc13535ef9483dd11a9855c657da9243940"}, "question": "What frameworks were used to parallelize the training and inference steps for the autoregressive GPT-like models?\n\n", "answer": "The frameworks used to parallelize the training and inference steps for the autoregressive GPT-like models were Deepspeed and Megatron. These frameworks allowed for effective parallelization, enabling the successful training of large models with 1.3 billion and 13 billion parameters."}
{"document": {"content": "The method introduced in this paper aims at helping deep learning\npractitioners faced with an overfit problem. The idea is to replace, in a\nmulti-branch network, the standard summation of parallel branches with a\nstochastic affine combination. Applied to 3-branch residual networks,\nshake-shake regularization improves on the best single shot published results\non CIFAR-10 and CIFAR-100 by reaching test errors of 2.86% and 15.85%.\nExperiments on architectures without skip connections or Batch Normalization\nshow encouraging results and open the door to a large set of applications. Code\nis available at https://github.com/xgastaldi/shake-shake", "collection_id": "101b5a58f762749d82018bb89b11d5587893288692af44dee4c689ce86475303"}, "question": "What is the test error achieved by shake-shake regularization on CIFAR-10 and CIFAR-100 datasets?\n", "answer": "Shake-shake regularization achieves test errors of 2.86% on CIFAR-10 and 15.85% on CIFAR-100 when applied to 3-branch residual networks."}
{"document": {"content": "Humans (e.g., crowdworkers) have a remarkable ability in solving different\ntasks, by simply reading textual instructions that define them and looking at a\nfew examples. Despite the success of the conventional supervised learning on\nindividual datasets, such models often struggle with generalization across\ntasks (e.g., a question-answering system cannot solve classification tasks). A\nlong-standing challenge in AI is to build a model that learns a new task by\nunderstanding the human-readable instructions that define it. To study this, we\nintroduce NATURAL INSTRUCTIONS, a dataset of 61 distinct tasks, their\nhuman-authored instructions, and 193k task instances (input-output pairs). The\ninstructions are obtained from crowdsourcing instructions used to create\nexisting NLP datasets and mapped to a unified schema. Using this meta-dataset,\nwe measure cross-task generalization by training models on seen tasks and\nmeasuring generalization to the remaining unseen ones. We adopt generative\npre-trained language models to encode task-specific instructions along with\ninput and generate task output. Our results indicate that models benefit from\ninstructions when evaluated in terms of generalization to unseen tasks (19%\nbetter for models utilizing instructions). These models, however, are far\nbehind an estimated performance upperbound indicating significant room for more\nprogress in this direction.", "collection_id": "cbc417f5dd466b1e95d91b15967a06fc437473e180fa5285e924ed2ef41a990d"}, "question": "How many distinct tasks are included in the NATURAL INSTRUCTIONS dataset?\n", "answer": "The NATURAL INSTRUCTIONS dataset includes 61 distinct tasks, along with their human-authored instructions and 193k task instances (input-output pairs)."}
{"document": {"content": "Deep learning has improved performance on many natural language processing\n(NLP) tasks individually. However, general NLP models cannot emerge within a\nparadigm that focuses on the particularities of a single metric, dataset, and\ntask. We introduce the Natural Language Decathlon (decaNLP), a challenge that\nspans ten tasks: question answering, machine translation, summarization,\nnatural language inference, sentiment analysis, semantic role labeling,\nzero-shot relation extraction, goal-oriented dialogue, semantic parsing, and\ncommonsense pronoun resolution. We cast all tasks as question answering over a\ncontext. Furthermore, we present a new Multitask Question Answering Network\n(MQAN) jointly learns all tasks in decaNLP without any task-specific modules or\nparameters in the multitask setting. MQAN shows improvements in transfer\nlearning for machine translation and named entity recognition, domain\nadaptation for sentiment analysis and natural language inference, and zero-shot\ncapabilities for text classification. We demonstrate that the MQAN's\nmulti-pointer-generator decoder is key to this success and performance further\nimproves with an anti-curriculum training strategy. Though designed for\ndecaNLP, MQAN also achieves state of the art results on the WikiSQL semantic\nparsing task in the single-task setting. We also release code for procuring and\nprocessing data, training and evaluating models, and reproducing all\nexperiments for decaNLP.", "collection_id": "5d8fd6d8213fcd100c40521bbc98333bd9bd57889583990af7735e9ce177fa2c"}, "question": "What is the Natural Language Decathlon (decaNLP) challenge in NLP?\n", "answer": "The Natural Language Decathlon (decaNLP) is a challenge that spans ten natural language processing (NLP) tasks, including question answering, machine translation, summarization, natural language inference, sentiment analysis, semantic role labeling, zero-shot relation extraction, goal-oriented dialogue, semantic parsing, and commonsense pronoun resolution. All tasks in decaNLP are cast as question answering over a context."}
{"document": {"content": "As AI systems become more capable, we would like to enlist their help to\nsupervise other AIs. We experiment with methods for training a harmless AI\nassistant through self-improvement, without any human labels identifying\nharmful outputs. The only human oversight is provided through a list of rules\nor principles, and so we refer to the method as 'Constitutional AI'. The\nprocess involves both a supervised learning and a reinforcement learning phase.\nIn the supervised phase we sample from an initial model, then generate\nself-critiques and revisions, and then finetune the original model on revised\nresponses. In the RL phase, we sample from the finetuned model, use a model to\nevaluate which of the two samples is better, and then train a preference model\nfrom this dataset of AI preferences. We then train with RL using the preference\nmodel as the reward signal, i.e. we use 'RL from AI Feedback' (RLAIF). As a\nresult we are able to train a harmless but non-evasive AI assistant that\nengages with harmful queries by explaining its objections to them. Both the SL\nand RL methods can leverage chain-of-thought style reasoning to improve the\nhuman-judged performance and transparency of AI decision making. These methods\nmake it possible to control AI behavior more precisely and with far fewer human\nlabels.", "collection_id": "cea0980fa97b657451778997c5957c81b32ba6b63a9aa712f4b0c807cdea007a"}, "question": "What is Constitutional AI and how does it train a harmless AI assistant?\n", "answer": "Constitutional AI is a method for training a harmless AI assistant through self-improvement, without any human labels identifying harmful outputs. The process involves both a supervised learning phase and a reinforcement learning phase. In the supervised phase, the model generates self-critiques and revisions, and is then finetuned on revised responses. In the reinforcement learning phase, a preference model is trained on AI preferences and used as a reward signal to train the AI assistant. This approach enables the training of a harmless but non-evasive AI assistant that engages with harmful queries by explaining its objections to them."}
{"document": {"content": "We test the hypothesis that language models trained with reinforcement\nlearning from human feedback (RLHF) have the capability to \"morally\nself-correct\" -- to avoid producing harmful outputs -- if instructed to do so.\nWe find strong evidence in support of this hypothesis across three different\nexperiments, each of which reveal different facets of moral self-correction. We\nfind that the capability for moral self-correction emerges at 22B model\nparameters, and typically improves with increasing model size and RLHF\ntraining. We believe that at this level of scale, language models obtain two\ncapabilities that they can use for moral self-correction: (1) they can follow\ninstructions and (2) they can learn complex normative concepts of harm like\nstereotyping, bias, and discrimination. As such, they can follow instructions\nto avoid certain kinds of morally harmful outputs. We believe our results are\ncause for cautious optimism regarding the ability to train language models to\nabide by ethical principles.", "collection_id": "d31e2beacddac85b350cd67e7e605b6ed11e7a0870900f46b3538e2bfb3c8b01"}, "question": "At what model size does the capability for moral self-correction emerge in language models trained with reinforcement learning from human feedback?\n\n", "answer": "The capability for moral self-correction in language models trained with reinforcement learning from human feedback emerges at 22B model parameters. This suggests that at this level of scale, language models develop the necessary capabilities to follow instructions and learn complex normative concepts of harm, enabling them to avoid producing harmful outputs when instructed to do so."}
{"document": {"content": "With the success of large-scale pre-training and multilingual modeling in\nNatural Language Processing (NLP), recent years have seen a proliferation of\nlarge, web-mined text datasets covering hundreds of languages. We manually\naudit the quality of 205 language-specific corpora released with five major\npublic datasets (CCAligned, ParaCrawl, WikiMatrix, OSCAR, mC4). Lower-resource\ncorpora have systematic issues: At least 15 corpora have no usable text, and a\nsignificant fraction contains less than 50% sentences of acceptable quality. In\naddition, many are mislabeled or use nonstandard/ambiguous language codes. We\ndemonstrate that these issues are easy to detect even for non-proficient\nspeakers, and supplement the human audit with automatic analyses. Finally, we\nrecommend techniques to evaluate and improve multilingual corpora and discuss\npotential risks that come with low-quality data releases.", "collection_id": "82b2dcb3ea03a079ed755519797f8d7fb781831f7e171dbb50afed3f1143e16f"}, "question": "What percentage of sentences in lower-resource corpora are of acceptable quality?\n", "answer": "A significant fraction of lower-resource corpora contains less than 50% sentences of acceptable quality."}
{"document": {"content": "How should conversational agents respond to verbal abuse through the user? To\nanswer this question, we conduct a large-scale crowd-sourced evaluation of\nabuse response strategies employed by current state-of-the-art systems. Our\nresults show that some strategies, such as \"polite refusal\" score highly across\nthe board, while for other strategies demographic factors, such as age, as well\nas the severity of the preceding abuse influence the user's perception of which\nresponse is appropriate. In addition, we find that most data-driven models lag\nbehind rule-based or commercial systems in terms of their perceived\nappropriateness.", "collection_id": "350537b64e465c07724b172ecfe8f28c1db0eb54fba2e37c27ea22b61763c4c5"}, "question": "How do data-driven models compare to rule-based or commercial systems in responding to verbal abuse?\n", "answer": "Data-driven models lag behind rule-based or commercial systems in terms of their perceived appropriateness when responding to verbal abuse."}
{"document": {"content": "We present BART, a denoising autoencoder for pretraining sequence-to-sequence\nmodels. BART is trained by (1) corrupting text with an arbitrary noising\nfunction, and (2) learning a model to reconstruct the original text. It uses a\nstandard Tranformer-based neural machine translation architecture which,\ndespite its simplicity, can be seen as generalizing BERT (due to the\nbidirectional encoder), GPT (with the left-to-right decoder), and many other\nmore recent pretraining schemes. We evaluate a number of noising approaches,\nfinding the best performance by both randomly shuffling the order of the\noriginal sentences and using a novel in-filling scheme, where spans of text are\nreplaced with a single mask token. BART is particularly effective when fine\ntuned for text generation but also works well for comprehension tasks. It\nmatches the performance of RoBERTa with comparable training resources on GLUE\nand SQuAD, achieves new state-of-the-art results on a range of abstractive\ndialogue, question answering, and summarization tasks, with gains of up to 6\nROUGE. BART also provides a 1.1 BLEU increase over a back-translation system\nfor machine translation, with only target language pretraining. We also report\nablation experiments that replicate other pretraining schemes within the BART\nframework, to better measure which factors most influence end-task performance.", "collection_id": "ed3e723521f6b4db721303e069a3613a8abdedd4f102a48b02f1ebb3b1dcc71e"}, "question": "What is BART in natural language processing?\n", "answer": "BART is a denoising autoencoder for pretraining sequence-to-sequence models, trained by corrupting text with an arbitrary noising function and learning a model to reconstruct the original text. It uses a standard Transformer-based neural machine translation architecture and is effective for both text generation and comprehension tasks."}
{"document": {"content": "We take a step towards addressing the under-representation of the African\ncontinent in NLP research by creating the first large publicly available\nhigh-quality dataset for named entity recognition (NER) in ten African\nlanguages, bringing together a variety of stakeholders. We detail\ncharacteristics of the languages to help researchers understand the challenges\nthat these languages pose for NER. We analyze our datasets and conduct an\nextensive empirical evaluation of state-of-the-art methods across both\nsupervised and transfer learning settings. We release the data, code, and\nmodels in order to inspire future research on African NLP.", "collection_id": "a8ff1e98b6c9587ee59a10cd62d01f733bc6b3dcd5e4b8e3476e29fe1e5189db"}, "question": "What is the goal of creating the first large publicly available high-quality dataset for named entity recognition in ten African languages?\n\n", "answer": "The goal is to address the under-representation of the African continent in NLP research by bringing together a variety of stakeholders and providing resources to inspire future research on African NLP."}
