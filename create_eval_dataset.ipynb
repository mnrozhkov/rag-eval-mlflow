{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation of custom RAG evaluation dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import LLM client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use LLM hosted in Nebius AI-Studio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "TEMPERATURE=0.1\n",
    "TOP_P=0.95\n",
    "MAX_TOKENS=2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The capital of France is Paris.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL = \"meta-llama/Meta-Llama-3.1-405B-Instruct\"\n",
    "\n",
    "llm_client = ChatOpenAI(\n",
    "    base_url=\"https://api.studio.nebius.ai/v1/\",\n",
    "    api_key=os.environ.get(\"NB_AI_STUDIO_KEY\"),\n",
    "    model=MODEL,\n",
    "    temperature=TEMPERATURE,\n",
    "    top_p=TOP_P,\n",
    "    max_tokens=MAX_TOKENS,\n",
    ")\n",
    "\n",
    "llm_client.invoke(\"What is the capital of France\").content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use an OSS LLM hosted on Nebius AI-Studio as a judge to power `mlflow.genai` metrics. For this we need to provide the local proxy server which handles the API calls to LLM. Run the following in your terminal:\n",
    "\n",
    "```bash\n",
    "mlflow deployments start-server --config-path mlflow_config/model_config.yaml\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the 'deployement' to use as evaluator LLM and see if it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _asteroid-impact:\n",
      "\n",
      "Asteroid Impact at 0.5c\n",
      "=======================\n",
      "\n",
      "Introduction\n",
      "------------\n",
      "\n",
      "This document discusses the hypothetical scenario of an asteroid, approximately the size of a basketball (with a diameter of about 0.24 meters), encountering Earth while traveling at 0.5 times the speed of light (0.5c, approximately 150,000,000 meters per second).\n",
      "\n",
      "Impact Energy\n",
      "-------------\n",
      "\n",
      "The kinetic energy of\n"
     ]
    }
   ],
   "source": [
    "from mlflow.deployments import get_deploy_client\n",
    "\n",
    "client = get_deploy_client(\"http://localhost:5000\")\n",
    "\n",
    "data = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What would happen if an asteroid the size of a basketball encountered the Earth traveling at 0.5c? Please provide your answer in .rst format for the purposes of documentation.\",\n",
    "        }\n",
    "    ],\n",
    "    \"temperature\": 0.5,\n",
    "    \"max_tokens\": 100,\n",
    "    \"n\": 1,\n",
    "    \"frequency_penalty\": 0.2,\n",
    "    \"presence_penalty\": 0.2,\n",
    "}\n",
    "print(client.predict(endpoint=\"ai-studio-chat\", inputs=data)[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare evaluation dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate QA pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to evaluate RAG ppeline, we need to have areference dataset which will include golden QA pairs together with reference context. \n",
    "\n",
    "\n",
    "To generate QA pairs we are going to use `jamescalam/ai-arxiv-chunked` which contains chunkd of NLP-related reseearch papers. We are not going to use the chunks themselves but only paper summaries which are provided together with chunks. \n",
    "There are over 400 unique summaries and we are going to sample 200 of them to use as source context for QA pairs generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cyril-k/.cache/pypoetry/virtualenvs/mlflow-llmops-iFome7o9-py3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import random\n",
    "\n",
    "data = load_dataset(\"jamescalam/ai-arxiv-chunked\", split=\"train\")\n",
    "summaries = list(set(data[\"summary\"]))\n",
    "sampled_summaries = random.sample(summaries, 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make batches of 4 to accelerate the generation process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 4\n",
    "batches = [sampled_summaries[i * BATCH_SIZE:(i+1) * BATCH_SIZE] for i in range(len(sampled_summaries)//BATCH_SIZE)]\n",
    "len(batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to create some helper function which will process batches of summaries and generate QA pairs. It is usefull to wrap this function with `retry` in case we run into rate limit for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from tenacity import retry, wait_random_exponential, stop_after_attempt \n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "Your task is to write a standalone factoid question and an answer given a context.\n",
    "Your factoid question should be answerable with a specific, concise piece of factual information from the context.\n",
    "Your factoid question should be formulated in the same style as questions users could ask in a search engine.\n",
    "Your answer to the factoid question should be detailed and relying upon given context and be accessible for a wide variety of users.\n",
    "This means that your standalone factoid question MUST NOT mention something like \"according to the passage\" or \"context\".\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Output:::\n",
    "Factoid question: (your factoid question)\n",
    "Answer: (your answer to the factoid question)\"\"\"\n",
    "\n",
    "USER_PROMPT = \"\"\"Now here is the context.\n",
    "\n",
    "Context: {context}\\n\n",
    "Output:::\"\"\"\n",
    "\n",
    "gen_prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", SYSTEM_PROMPT),\n",
    "        (\"human\", USER_PROMPT),\n",
    "    ]\n",
    ")\n",
    "\n",
    "gen_prompt_template.batch(inputs=[{\"context\": \"context\"},{\"context\": \"context\"}])\n",
    "\n",
    "@retry(wait=wait_random_exponential(multiplier=4, max=120), stop=stop_after_attempt(3))\n",
    "def process_batch_messages(llm, batch):\n",
    "    return llm.batch(gen_prompt_template.batch(inputs=[{\"context\": d} for d in batch]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some other helper functions which may be useful later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "\n",
    "def hash_string(input: str) -> str:\n",
    "    h = hashlib.new('sha256')\n",
    "    h.update(input.encode())\n",
    "    \n",
    "    return h.hexdigest()\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "def get_timestamp() -> str:\n",
    "    timestamp = datetime.now()\n",
    "    return timestamp.strftime(\"%Y-%m-%d_%H-%M-%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the code which processes the batches and parses the output into QA pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/11/12 09:35:26 INFO mlflow.tracking.fluent: Experiment with name 'Data generation for RAG eval 2024-11-12_09-35-26' does not exist. Creating a new experiment.\n",
      "2024/11/12 09:35:27 WARNING mlflow.utils.autologging_utils: You are using an unsupported version of langchain. If you encounter errors during autologging, try upgrading / downgrading langchain to a supported version, or try upgrading MLflow.\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]/usr/lib/python3.10/json/encoder.py:257: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  return _iterencode(o, 0)\n",
      " 92%|█████████▏| 46/50 [04:56<00:26,  6.70s/it]2024/11/12 09:40:29 WARNING mlflow.tracing.export.mlflow: Failed to log trace to MLflow backend: BAD_REQUEST: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)\n",
      "(psycopg2.errors.InternalError_) unexpected data beyond EOF in block 152 of relation base/16385/16609\n",
      "HINT:  This has been seen to occur with buggy kernels; consider updating your system.\n",
      "\n",
      "[SQL: INSERT INTO trace_request_metadata (key, value, request_id) VALUES (%(key)s, %(value)s, %(request_id)s)]\n",
      "[parameters: {'key': 'mlflow.traceInputs', 'value': '[[{\"content\": \"\\\\nYour task is to write a standalone factoid question and an answer given a context.\\\\nYour factoid question should be answerable with a specific, concise piece of factual information from the context.\\\\nYour factoid question should b...', 'request_id': '9c323c9973f0465bac4374ed00a1b52a'}]\n",
      "(Background on this error at: https://sqlalche.me/e/20/2j85)\n",
      "2024/11/12 09:40:30 WARNING mlflow.tracing.export.mlflow: Failed to log trace to MLflow backend: BAD_REQUEST: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)\n",
      "(psycopg2.errors.InternalError_) unexpected data beyond EOF in block 152 of relation base/16385/16609\n",
      "HINT:  This has been seen to occur with buggy kernels; consider updating your system.\n",
      "\n",
      "[SQL: INSERT INTO trace_request_metadata (key, value, request_id) VALUES (%(key)s, %(value)s, %(request_id)s)]\n",
      "[parameters: {'key': 'mlflow.traceInputs', 'value': '[[{\"content\": \"\\\\nYour task is to write a standalone factoid question and an answer given a context.\\\\nYour factoid question should be answerable with a specific, concise piece of factual information from the context.\\\\nYour factoid question should b...', 'request_id': '61e0ff1b7ea744e3a046b5fadf8ccded'}]\n",
      "(Background on this error at: https://sqlalche.me/e/20/2j85)\n",
      "2024/11/12 09:40:31 WARNING mlflow.tracing.export.mlflow: Failed to log trace to MLflow backend: BAD_REQUEST: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)\n",
      "(psycopg2.errors.InternalError_) unexpected data beyond EOF in block 152 of relation base/16385/16609\n",
      "HINT:  This has been seen to occur with buggy kernels; consider updating your system.\n",
      "\n",
      "[SQL: INSERT INTO trace_request_metadata (key, value, request_id) VALUES (%(key)s, %(value)s, %(request_id)s)]\n",
      "[parameters: {'key': 'mlflow.traceInputs', 'value': '[[{\"content\": \"\\\\nYour task is to write a standalone factoid question and an answer given a context.\\\\nYour factoid question should be answerable with a specific, concise piece of factual information from the context.\\\\nYour factoid question should b...', 'request_id': '53d0f571c7bc45fcbee5799e54cd91e2'}]\n",
      "(Background on this error at: https://sqlalche.me/e/20/2j85)\n",
      " 94%|█████████▍| 47/50 [05:03<00:20,  6.70s/it]2024/11/12 09:40:32 WARNING mlflow.tracing.export.mlflow: Failed to log trace to MLflow backend: BAD_REQUEST: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)\n",
      "(psycopg2.errors.InternalError_) unexpected data beyond EOF in block 152 of relation base/16385/16609\n",
      "HINT:  This has been seen to occur with buggy kernels; consider updating your system.\n",
      "\n",
      "[SQL: INSERT INTO trace_request_metadata (key, value, request_id) VALUES (%(key)s, %(value)s, %(request_id)s)]\n",
      "[parameters: {'key': 'mlflow.traceInputs', 'value': '{\"context\": \"This paper aims to help structure the risk landscape associated with\\\\nlarge-scale Language Models (LMs). In order to foster advances in responsible\\\\ninnovation, an in-depth understanding of the potential risks posed by these\\\\nmodels i...', 'request_id': '9a00a21e189f4a17939591158c5b8af1'}]\n",
      "(Background on this error at: https://sqlalche.me/e/20/2j85)\n",
      "2024/11/12 09:40:32 WARNING mlflow.tracing.export.mlflow: Failed to log trace to MLflow backend: BAD_REQUEST: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)\n",
      "(psycopg2.errors.InternalError_) unexpected data beyond EOF in block 152 of relation base/16385/16609\n",
      "HINT:  This has been seen to occur with buggy kernels; consider updating your system.\n",
      "\n",
      "[SQL: INSERT INTO trace_request_metadata (key, value, request_id) VALUES (%(key)s, %(value)s, %(request_id)s)]\n",
      "[parameters: {'key': 'mlflow.traceInputs', 'value': '{\"context\": \"The method introduced in this paper aims at helping deep learning\\\\npractitioners faced with an overfit problem. The idea is to replace, in a\\\\nmulti-branch network, the standard summation of parallel branches with a\\\\nstochastic affine ...', 'request_id': '41eff2cc3aaf4a12a7cfd4d419ee2218'}]\n",
      "(Background on this error at: https://sqlalche.me/e/20/2j85)\n",
      "2024/11/12 09:40:32 WARNING mlflow.tracing.export.mlflow: Failed to log trace to MLflow backend: BAD_REQUEST: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)\n",
      "(psycopg2.errors.InternalError_) unexpected data beyond EOF in block 152 of relation base/16385/16609\n",
      "HINT:  This has been seen to occur with buggy kernels; consider updating your system.\n",
      "\n",
      "[SQL: INSERT INTO trace_request_metadata (key, value, request_id) VALUES (%(key)s, %(value)s, %(request_id)s)]\n",
      "[parameters: {'key': 'mlflow.traceInputs', 'value': '{\"context\": \"Books are a rich source of both fine-grained information, how a character, an\\\\nobject or a scene looks like, as well as high-level semantics, what someone is\\\\nthinking, feeling and how these states evolve through a story. This paper a...', 'request_id': 'a22e588ae03d43c389d1c95b2b4ecbef'}]\n",
      "(Background on this error at: https://sqlalche.me/e/20/2j85)\n",
      "2024/11/12 09:40:32 WARNING mlflow.tracing.export.mlflow: Failed to log trace to MLflow backend: BAD_REQUEST: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)\n",
      "(psycopg2.errors.InternalError_) unexpected data beyond EOF in block 152 of relation base/16385/16609\n",
      "HINT:  This has been seen to occur with buggy kernels; consider updating your system.\n",
      "\n",
      "[SQL: INSERT INTO trace_request_metadata (key, value, request_id) VALUES (%(key)s, %(value)s, %(request_id)s)]\n",
      "[parameters: {'key': 'mlflow.traceInputs', 'value': '{\"context\": \"Recent studies report that autoregressive language models can successfully\\\\nsolve many NLP tasks via zero- and few-shot learning paradigms, which opens up\\\\nnew possibilities for using the pre-trained language models. This paper\\\\nintro...', 'request_id': '1802261211614e4cb97ea4409d050623'}]\n",
      "(Background on this error at: https://sqlalche.me/e/20/2j85)\n",
      "2024/11/12 09:40:36 WARNING mlflow.tracing.export.mlflow: Failed to log trace to MLflow backend: BAD_REQUEST: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)\n",
      "(psycopg2.errors.InternalError_) unexpected data beyond EOF in block 152 of relation base/16385/16609\n",
      "HINT:  This has been seen to occur with buggy kernels; consider updating your system.\n",
      "\n",
      "[SQL: INSERT INTO trace_request_metadata (key, value, request_id) VALUES (%(key)s, %(value)s, %(request_id)s)]\n",
      "[parameters: {'key': 'mlflow.traceInputs', 'value': '[[{\"content\": \"\\\\nYour task is to write a standalone factoid question and an answer given a context.\\\\nYour factoid question should be answerable with a specific, concise piece of factual information from the context.\\\\nYour factoid question should b...', 'request_id': '6092b9c7e0b24e48ad53d634b8299985'}]\n",
      "(Background on this error at: https://sqlalche.me/e/20/2j85)\n",
      "2024/11/12 09:40:37 WARNING mlflow.tracing.export.mlflow: Failed to log trace to MLflow backend: BAD_REQUEST: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)\n",
      "(psycopg2.errors.InternalError_) unexpected data beyond EOF in block 152 of relation base/16385/16609\n",
      "HINT:  This has been seen to occur with buggy kernels; consider updating your system.\n",
      "\n",
      "[SQL: INSERT INTO trace_request_metadata (key, value, request_id) VALUES (%(key)s, %(value)s, %(request_id)s)]\n",
      "[parameters: {'key': 'mlflow.traceInputs', 'value': '[[{\"content\": \"\\\\nYour task is to write a standalone factoid question and an answer given a context.\\\\nYour factoid question should be answerable with a specific, concise piece of factual information from the context.\\\\nYour factoid question should b...', 'request_id': 'fc72f319bfc94d95b8aa4fb84a55e8e9'}]\n",
      "(Background on this error at: https://sqlalche.me/e/20/2j85)\n",
      "2024/11/12 09:40:37 WARNING mlflow.tracing.export.mlflow: Failed to log trace to MLflow backend: BAD_REQUEST: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)\n",
      "(psycopg2.errors.InternalError_) unexpected data beyond EOF in block 152 of relation base/16385/16609\n",
      "HINT:  This has been seen to occur with buggy kernels; consider updating your system.\n",
      "\n",
      "[SQL: INSERT INTO trace_request_metadata (key, value, request_id) VALUES (%(key)s, %(value)s, %(request_id)s)]\n",
      "[parameters: {'key': 'mlflow.traceInputs', 'value': '[[{\"content\": \"\\\\nYour task is to write a standalone factoid question and an answer given a context.\\\\nYour factoid question should be answerable with a specific, concise piece of factual information from the context.\\\\nYour factoid question should b...', 'request_id': 'aeca24605328438d827c4d04906cc2a8'}]\n",
      "(Background on this error at: https://sqlalche.me/e/20/2j85)\n",
      "2024/11/12 09:40:41 WARNING mlflow.tracing.export.mlflow: Failed to log trace to MLflow backend: BAD_REQUEST: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)\n",
      "(psycopg2.errors.InternalError_) unexpected data beyond EOF in block 152 of relation base/16385/16609\n",
      "HINT:  This has been seen to occur with buggy kernels; consider updating your system.\n",
      "\n",
      "[SQL: INSERT INTO trace_request_metadata (key, value, request_id) VALUES (%(key)s, %(value)s, %(request_id)s)]\n",
      "[parameters: {'key': 'mlflow.traceInputs', 'value': '[[{\"content\": \"\\\\nYour task is to write a standalone factoid question and an answer given a context.\\\\nYour factoid question should be answerable with a specific, concise piece of factual information from the context.\\\\nYour factoid question should b...', 'request_id': 'c01b4b6a40c1414ab8dbcd7cef0a68a5'}]\n",
      "(Background on this error at: https://sqlalche.me/e/20/2j85)\n",
      "100%|██████████| 50/50 [05:27<00:00,  6.54s/it]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import mlflow\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "formatted_timestamp = get_timestamp()\n",
    "mlflow.set_experiment(f\"Data generation for RAG eval {formatted_timestamp}\")\n",
    "mlflow.langchain.autolog()\n",
    "\n",
    "file_path = f'syntetic_data/generated_items_{formatted_timestamp}.jsonl'\n",
    "\n",
    "outputs = []\n",
    "for batch in tqdm(batches[:]):\n",
    "    responses = process_batch_messages(llm_client, batch)\n",
    "    for response, doc in zip(responses, batch):\n",
    "        output_QA_couple = response.content\n",
    "        try:\n",
    "            question = output_QA_couple.split(\"Factoid question: \")[-1].split(\"Answer: \")[0]\n",
    "            answer = output_QA_couple.split(\"Answer: \")[-1]\n",
    "            item =  {\n",
    "                \"document\": {\n",
    "                    \"content\": doc,\n",
    "                    \"collection_id\": str(hash_string(doc)),\n",
    "                },\n",
    "                \"question\": question,\n",
    "                \"answer\": answer,\n",
    "            }\n",
    "            outputs.append(item)\n",
    "            json_line = json.dumps(item)\n",
    "            with open(file_path, 'a') as file:\n",
    "                file.write(json_line + '\\n')\n",
    "        except Exception as e:\n",
    "            print(e.__str__())\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Evaluate generated eval dataset with `mlflow.genai` metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use our `mlflow` deployement as LLM-as-a-judge to power evaluation metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.deployments import set_deployments_target\n",
    "\n",
    "set_deployments_target(\"http://localhost:5000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use a built-in `mlflow` metric for relevance. We only need to provide the name of deployement specified in `model_config.yaml`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevance_metric = mlflow.metrics.genai.relevance(\n",
    "    model = \"endpoints:/ai-studio-chat\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also need 2 additional metrics to evaluate for groundedness and autonomy. For this, we may use generic `genai` metric constructor `make_genai_metric()` provided by `mlflow`, which takes metric description and corresponding grading prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "groundedness_metric = mlflow.metrics.genai.make_genai_metric(\n",
    "    name=\"groundedness\",\n",
    "    definition=(\n",
    "        \"Groundedness refers to how well the question is formulated to stay within the provided context, ensuring clarity \"\n",
    "        \"and relevance to the task at hand. A grounded question should directly address the instruction, avoid ambiguity, \"\n",
    "        \"and be clearly rooted in the context provided.\"\n",
    "    ),\n",
    "    grading_prompt=(\n",
    "        \"Groundedness: Evaluate if the question is formulated clearly, without ambiguity, and remains grounded in the provided Context. \"\n",
    "        \"Below are the details for different scores: \"\n",
    "        \"- Score 1: The question is vague or unclear and does not engage with the provided Context, making it impossible to discern \"\n",
    "        \"how it relates to the question or instruction.\"\n",
    "        \"- Score 2: The question partially engages with the Context but includes significant ambiguities or unclear portions, often \"\n",
    "        \"straying from the context or not fully addressing the question.\"\n",
    "        \"- Score 3: The question generally addresses the question using the provided Context but has occasional ambiguities or is \"\n",
    "        \"unclear in certain aspects, making parts of the question less grounded.\"\n",
    "        \"- Score 4: The question is mostly clear and unambiguous, providing a grounded question based on the Context. However, there \"\n",
    "        \"are minor instances where clarity could be improved or where the grounding in the context is weaker.\"\n",
    "        \"- Score 5: The question is entirely clear, unambiguous, and fully grounded in the provided Context. It aligns with context \"\n",
    "        \"precisely with no unnecessary or unclear content.\"\n",
    "    ),\n",
    "    model=\"endpoints:/ai-studio-chat\",\n",
    "    parameters={\"temperature\": 0.0},\n",
    "    aggregations=[\"mean\", \"variance\"],\n",
    "    greater_is_better=True,\n",
    ")\n",
    "\n",
    "standalone_metric = mlflow.metrics.genai.make_genai_metric(\n",
    "    name=\"standalone\",\n",
    "    definition=(\n",
    "        \"Standalone refers to the degree to which the question can be understood and answered independently of any external \"\n",
    "        \"context or information. A fully standalone question should be self-contained, meaning it does not rely on additional \"\n",
    "        \"documents, previous interactions, or external scenarios to be complete or comprehensible.\"\n",
    "    ),\n",
    "    grading_prompt=(\n",
    "        \"Standalone: Evaluate if the question can be understood and answered without looking at the Context in the Instruction. \"\n",
    "        \"Below are the details for different scores: \"\n",
    "        \"- Score 1: The question heavily depends on external context or previous information to be understood. It refers to specific \"\n",
    "        \"content (e.g., 'in the context' or 'in the document') and is incomplete on its own.\"\n",
    "        \"- Score 2: The question is mostly dependent on external information. While parts of the question may be clear, it still \"\n",
    "        \"requires knowledge of additional context or documents to be fully understood.\"\n",
    "        \"- Score 3: The question is partially understandable on its own but still relies on some implicit context or background knowledge \"\n",
    "        \"to be fully clear. It is incomplete without certain pieces of information.\"\n",
    "        \"- Score 4: The question is largely standalone and makes sense without needing much additional context. It may refer to specific \"\n",
    "        \"technical details but can generally be understood independently.\"\n",
    "        \"- Score 5: The question is entirely self-contained and makes complete sense on its own. Even if technical terms or acronyms are used, \"\n",
    "        \"a user with relevant expertise or access to documentation would understand it without needing additional context.\"\n",
    "    ),\n",
    "    model=\"endpoints:/ai-studio-chat\",\n",
    "    parameters={\"temperature\": 0.0},\n",
    "    aggregations=[\"mean\", \"variance\"],\n",
    "    greater_is_better=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate generated QA dataset, we will use `mlflow.evaluate()` with a static dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/11/12 09:40:56 INFO mlflow.models.evaluation.default_evaluator: Testing metrics on first row...\n",
      "/home/cyril-k/.cache/pypoetry/virtualenvs/mlflow-llmops-iFome7o9-py3.10/lib/python3.10/site-packages/pydantic/_internal/_config.py:341: UserWarning: Valid config keys have changed in V2:\n",
      "* 'schema_extra' has been renamed to 'json_schema_extra'\n",
      "  warnings.warn(message, UserWarning)\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.01s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.13s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.06s/it]\n",
      "100%|██████████| 200/200 [00:39<00:00,  5.10it/s]\n",
      "100%|██████████| 200/200 [00:47<00:00,  4.24it/s]\n",
      "100%|██████████| 200/200 [00:50<00:00,  4.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated evaluation results: \n",
      "{'groundedness/v1/mean': 5.0, 'groundedness/v1/variance': 0.0, 'relevance/v1/mean': 4.905, 'relevance/v1/variance': 0.085975, 'relevance/v1/p90': 5.0, 'standalone/v1/mean': 4.215, 'standalone/v1/variance': 0.16877499999999998}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/11/12 09:43:19 INFO mlflow.tracking._tracking_service.client: 🏃 View run secretive-ray-53 at: https://tracking.mlflow-e00rhqs1bwevnqy5wj.backbone-e00ffdgj3ybad7mxrx.msp.eu-north1.nebius.cloud/#/experiments/5/runs/12a4e7b4760047f99164848e7582f64d.\n",
      "2024/11/12 09:43:19 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: https://tracking.mlflow-e00rhqs1bwevnqy5wj.backbone-e00ffdgj3ybad7mxrx.msp.eu-north1.nebius.cloud/#/experiments/5.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# simplify the QA generation instruction prompt for evaluation\n",
    "instruction_prompt = \"\"\"\n",
    "Your task is to write a standalone factoid question and an answer given a context.\n",
    "Your factoid question should be answerable with a specific, concise piece of factual information from the context.\n",
    "Your factoid question should be formulated in the same style as questions users could ask in a search engine.\n",
    "\n",
    "Context: {context}\"\"\"\n",
    "\n",
    "eval_data = pd.DataFrame(\n",
    "    {   \n",
    "        \"inputs\": [instruction_prompt.format(context=' '.join(o[\"document\"][\"content\"])) for o in outputs],\n",
    "        \"predictions\": [f\"\"\"Question: {o[\"question\"]}\\n Answer: {o[\"answer\"]}\"\"\" for o in outputs],\n",
    "        \"question\": [o[\"question\"] for o in outputs],\n",
    "        \"answer\": [o[\"answer\"] for o in outputs],\n",
    "        \"context\": [o[\"document\"][\"content\"] for o in outputs],\n",
    "        \"document\": [o[\"document\"] for o in outputs],\n",
    "    }\n",
    ")\n",
    "\n",
    "with mlflow.start_run() as run:\n",
    "    results = mlflow.evaluate(\n",
    "        data=eval_data,\n",
    "        predictions=\"predictions\",\n",
    "        extra_metrics=[\n",
    "            groundedness_metric, \n",
    "            relevance_metric, \n",
    "            standalone_metric\n",
    "        ],\n",
    "    )\n",
    "    \n",
    "    print(f\"Aggregated evaluation results: \\n{results.metrics}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at evaluation results table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|██████████| 1/1 [00:00<00:00,  3.26it/s]\n",
      "Downloading artifacts: 100%|██████████| 1/1 [00:00<00:00,  3.63it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inputs</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>context</th>\n",
       "      <th>document</th>\n",
       "      <th>outputs</th>\n",
       "      <th>groundedness/v1/score</th>\n",
       "      <th>groundedness/v1/justification</th>\n",
       "      <th>relevance/v1/score</th>\n",
       "      <th>relevance/v1/justification</th>\n",
       "      <th>standalone/v1/score</th>\n",
       "      <th>standalone/v1/justification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\nYour task is to write a standalone factoid q...</td>\n",
       "      <td>What is the main goal of interpretable machine...</td>\n",
       "      <td>The main goal of interpretable machine learnin...</td>\n",
       "      <td>As machine learning systems become ubiquitous,...</td>\n",
       "      <td>{'content': 'As machine learning systems becom...</td>\n",
       "      <td>Question: What is the main goal of interpretab...</td>\n",
       "      <td>5</td>\n",
       "      <td>The question is entirely clear, unambiguous, a...</td>\n",
       "      <td>4</td>\n",
       "      <td>The output answers the question and is consist...</td>\n",
       "      <td>4</td>\n",
       "      <td>The question \"What is the main goal of interpr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\nYour task is to write a standalone factoid q...</td>\n",
       "      <td>What F1 score was achieved on the web portion ...</td>\n",
       "      <td>The proposed solution achieved a score of 71.3...</td>\n",
       "      <td>We consider the problem of adapting neural par...</td>\n",
       "      <td>{'content': 'We consider the problem of adapti...</td>\n",
       "      <td>Question: What F1 score was achieved on the we...</td>\n",
       "      <td>5</td>\n",
       "      <td>The question is entirely clear, unambiguous, a...</td>\n",
       "      <td>5</td>\n",
       "      <td>The output comprehensively answers the questio...</td>\n",
       "      <td>4</td>\n",
       "      <td>The question is largely standalone and makes s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\nYour task is to write a standalone factoid q...</td>\n",
       "      <td>What is the time and memory complexity reducti...</td>\n",
       "      <td>The sparse factorizations of the attention mat...</td>\n",
       "      <td>Transformers are powerful sequence models, but...</td>\n",
       "      <td>{'content': 'Transformers are powerful sequenc...</td>\n",
       "      <td>Question: What is the time and memory complexi...</td>\n",
       "      <td>5</td>\n",
       "      <td>The question is entirely clear, unambiguous, a...</td>\n",
       "      <td>5</td>\n",
       "      <td>The output comprehensively answers the questio...</td>\n",
       "      <td>4</td>\n",
       "      <td>The question is largely standalone and makes s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\nYour task is to write a standalone factoid q...</td>\n",
       "      <td>What is the human performance accuracy on the ...</td>\n",
       "      <td>The human performance accuracy on the Commonse...</td>\n",
       "      <td>When answering a question, people often draw u...</td>\n",
       "      <td>{'content': 'When answering a question, people...</td>\n",
       "      <td>Question: What is the human performance accura...</td>\n",
       "      <td>5</td>\n",
       "      <td>The question is entirely clear, unambiguous, a...</td>\n",
       "      <td>5</td>\n",
       "      <td>The output answers the question comprehensivel...</td>\n",
       "      <td>5</td>\n",
       "      <td>The question \"What is the human performance ac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\nYour task is to write a standalone factoid q...</td>\n",
       "      <td>What is a universal adversarial perturbation (...</td>\n",
       "      <td>A universal adversarial perturbation (UAP) is ...</td>\n",
       "      <td>The intriguing phenomenon of adversarial examp...</td>\n",
       "      <td>{'content': 'The intriguing phenomenon of adve...</td>\n",
       "      <td>Question: What is a universal adversarial pert...</td>\n",
       "      <td>5</td>\n",
       "      <td>The question is entirely clear, unambiguous, a...</td>\n",
       "      <td>5</td>\n",
       "      <td>The output comprehensively answers the questio...</td>\n",
       "      <td>4</td>\n",
       "      <td>The question is largely standalone and makes s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>\\nYour task is to write a standalone factoid q...</td>\n",
       "      <td>At what model size does the capability for mor...</td>\n",
       "      <td>The capability for moral self-correction in la...</td>\n",
       "      <td>We test the hypothesis that language models tr...</td>\n",
       "      <td>{'content': 'We test the hypothesis that langu...</td>\n",
       "      <td>Question: At what model size does the capabili...</td>\n",
       "      <td>5</td>\n",
       "      <td>The question is entirely clear, unambiguous, a...</td>\n",
       "      <td>5</td>\n",
       "      <td>The output comprehensively answers the questio...</td>\n",
       "      <td>4</td>\n",
       "      <td>The question is largely standalone and makes s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>\\nYour task is to write a standalone factoid q...</td>\n",
       "      <td>What percentage of sentences in lower-resource...</td>\n",
       "      <td>A significant fraction of lower-resource corpo...</td>\n",
       "      <td>With the success of large-scale pre-training a...</td>\n",
       "      <td>{'content': 'With the success of large-scale p...</td>\n",
       "      <td>Question: What percentage of sentences in lowe...</td>\n",
       "      <td>5</td>\n",
       "      <td>The question is entirely clear, unambiguous, a...</td>\n",
       "      <td>4</td>\n",
       "      <td>The output directly answers the question about...</td>\n",
       "      <td>4</td>\n",
       "      <td>The question is largely standalone and makes s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>\\nYour task is to write a standalone factoid q...</td>\n",
       "      <td>How do data-driven models compare to rule-base...</td>\n",
       "      <td>Data-driven models lag behind rule-based or co...</td>\n",
       "      <td>How should conversational agents respond to ve...</td>\n",
       "      <td>{'content': 'How should conversational agents ...</td>\n",
       "      <td>Question: How do data-driven models compare to...</td>\n",
       "      <td>5</td>\n",
       "      <td>The question is entirely clear, unambiguous, a...</td>\n",
       "      <td>5</td>\n",
       "      <td>The output comprehensively answers the questio...</td>\n",
       "      <td>4</td>\n",
       "      <td>The question is largely standalone and makes s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>\\nYour task is to write a standalone factoid q...</td>\n",
       "      <td>What is BART in natural language processing?\\n</td>\n",
       "      <td>BART is a denoising autoencoder for pretrainin...</td>\n",
       "      <td>We present BART, a denoising autoencoder for p...</td>\n",
       "      <td>{'content': 'We present BART, a denoising auto...</td>\n",
       "      <td>Question: What is BART in natural language pro...</td>\n",
       "      <td>5</td>\n",
       "      <td>The question is entirely clear, unambiguous, a...</td>\n",
       "      <td>5</td>\n",
       "      <td>The output comprehensively answers the questio...</td>\n",
       "      <td>5</td>\n",
       "      <td>The question \"What is BART in natural language...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>\\nYour task is to write a standalone factoid q...</td>\n",
       "      <td>What is the goal of creating the first large p...</td>\n",
       "      <td>The goal is to address the under-representatio...</td>\n",
       "      <td>We take a step towards addressing the under-re...</td>\n",
       "      <td>{'content': 'We take a step towards addressing...</td>\n",
       "      <td>Question: What is the goal of creating the fir...</td>\n",
       "      <td>5</td>\n",
       "      <td>The question is entirely clear, unambiguous, a...</td>\n",
       "      <td>5</td>\n",
       "      <td>The output comprehensively answers the questio...</td>\n",
       "      <td>4</td>\n",
       "      <td>The question is largely standalone and makes s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                inputs  \\\n",
       "0    \\nYour task is to write a standalone factoid q...   \n",
       "1    \\nYour task is to write a standalone factoid q...   \n",
       "2    \\nYour task is to write a standalone factoid q...   \n",
       "3    \\nYour task is to write a standalone factoid q...   \n",
       "4    \\nYour task is to write a standalone factoid q...   \n",
       "..                                                 ...   \n",
       "195  \\nYour task is to write a standalone factoid q...   \n",
       "196  \\nYour task is to write a standalone factoid q...   \n",
       "197  \\nYour task is to write a standalone factoid q...   \n",
       "198  \\nYour task is to write a standalone factoid q...   \n",
       "199  \\nYour task is to write a standalone factoid q...   \n",
       "\n",
       "                                              question  \\\n",
       "0    What is the main goal of interpretable machine...   \n",
       "1    What F1 score was achieved on the web portion ...   \n",
       "2    What is the time and memory complexity reducti...   \n",
       "3    What is the human performance accuracy on the ...   \n",
       "4    What is a universal adversarial perturbation (...   \n",
       "..                                                 ...   \n",
       "195  At what model size does the capability for mor...   \n",
       "196  What percentage of sentences in lower-resource...   \n",
       "197  How do data-driven models compare to rule-base...   \n",
       "198     What is BART in natural language processing?\\n   \n",
       "199  What is the goal of creating the first large p...   \n",
       "\n",
       "                                                answer  \\\n",
       "0    The main goal of interpretable machine learnin...   \n",
       "1    The proposed solution achieved a score of 71.3...   \n",
       "2    The sparse factorizations of the attention mat...   \n",
       "3    The human performance accuracy on the Commonse...   \n",
       "4    A universal adversarial perturbation (UAP) is ...   \n",
       "..                                                 ...   \n",
       "195  The capability for moral self-correction in la...   \n",
       "196  A significant fraction of lower-resource corpo...   \n",
       "197  Data-driven models lag behind rule-based or co...   \n",
       "198  BART is a denoising autoencoder for pretrainin...   \n",
       "199  The goal is to address the under-representatio...   \n",
       "\n",
       "                                               context  \\\n",
       "0    As machine learning systems become ubiquitous,...   \n",
       "1    We consider the problem of adapting neural par...   \n",
       "2    Transformers are powerful sequence models, but...   \n",
       "3    When answering a question, people often draw u...   \n",
       "4    The intriguing phenomenon of adversarial examp...   \n",
       "..                                                 ...   \n",
       "195  We test the hypothesis that language models tr...   \n",
       "196  With the success of large-scale pre-training a...   \n",
       "197  How should conversational agents respond to ve...   \n",
       "198  We present BART, a denoising autoencoder for p...   \n",
       "199  We take a step towards addressing the under-re...   \n",
       "\n",
       "                                              document  \\\n",
       "0    {'content': 'As machine learning systems becom...   \n",
       "1    {'content': 'We consider the problem of adapti...   \n",
       "2    {'content': 'Transformers are powerful sequenc...   \n",
       "3    {'content': 'When answering a question, people...   \n",
       "4    {'content': 'The intriguing phenomenon of adve...   \n",
       "..                                                 ...   \n",
       "195  {'content': 'We test the hypothesis that langu...   \n",
       "196  {'content': 'With the success of large-scale p...   \n",
       "197  {'content': 'How should conversational agents ...   \n",
       "198  {'content': 'We present BART, a denoising auto...   \n",
       "199  {'content': 'We take a step towards addressing...   \n",
       "\n",
       "                                               outputs  groundedness/v1/score  \\\n",
       "0    Question: What is the main goal of interpretab...                      5   \n",
       "1    Question: What F1 score was achieved on the we...                      5   \n",
       "2    Question: What is the time and memory complexi...                      5   \n",
       "3    Question: What is the human performance accura...                      5   \n",
       "4    Question: What is a universal adversarial pert...                      5   \n",
       "..                                                 ...                    ...   \n",
       "195  Question: At what model size does the capabili...                      5   \n",
       "196  Question: What percentage of sentences in lowe...                      5   \n",
       "197  Question: How do data-driven models compare to...                      5   \n",
       "198  Question: What is BART in natural language pro...                      5   \n",
       "199  Question: What is the goal of creating the fir...                      5   \n",
       "\n",
       "                         groundedness/v1/justification  relevance/v1/score  \\\n",
       "0    The question is entirely clear, unambiguous, a...                   4   \n",
       "1    The question is entirely clear, unambiguous, a...                   5   \n",
       "2    The question is entirely clear, unambiguous, a...                   5   \n",
       "3    The question is entirely clear, unambiguous, a...                   5   \n",
       "4    The question is entirely clear, unambiguous, a...                   5   \n",
       "..                                                 ...                 ...   \n",
       "195  The question is entirely clear, unambiguous, a...                   5   \n",
       "196  The question is entirely clear, unambiguous, a...                   4   \n",
       "197  The question is entirely clear, unambiguous, a...                   5   \n",
       "198  The question is entirely clear, unambiguous, a...                   5   \n",
       "199  The question is entirely clear, unambiguous, a...                   5   \n",
       "\n",
       "                            relevance/v1/justification  standalone/v1/score  \\\n",
       "0    The output answers the question and is consist...                    4   \n",
       "1    The output comprehensively answers the questio...                    4   \n",
       "2    The output comprehensively answers the questio...                    4   \n",
       "3    The output answers the question comprehensivel...                    5   \n",
       "4    The output comprehensively answers the questio...                    4   \n",
       "..                                                 ...                  ...   \n",
       "195  The output comprehensively answers the questio...                    4   \n",
       "196  The output directly answers the question about...                    4   \n",
       "197  The output comprehensively answers the questio...                    4   \n",
       "198  The output comprehensively answers the questio...                    5   \n",
       "199  The output comprehensively answers the questio...                    4   \n",
       "\n",
       "                           standalone/v1/justification  \n",
       "0    The question \"What is the main goal of interpr...  \n",
       "1    The question is largely standalone and makes s...  \n",
       "2    The question is largely standalone and makes s...  \n",
       "3    The question \"What is the human performance ac...  \n",
       "4    The question is largely standalone and makes s...  \n",
       "..                                                 ...  \n",
       "195  The question is largely standalone and makes s...  \n",
       "196  The question is largely standalone and makes s...  \n",
       "197  The question is largely standalone and makes s...  \n",
       "198  The question \"What is BART in natural language...  \n",
       "199  The question is largely standalone and makes s...  \n",
       "\n",
       "[200 rows x 12 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_results_table = results.tables[\"eval_results_table\"]\n",
    "eval_results_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code which evaluates QA pairs updates the original items with evaluation scores and corresponding justification:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the evaluation results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation dataset before filtering:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>groundedness/v1/score</th>\n",
       "      <th>relevance/v1/score</th>\n",
       "      <th>standalone/v1/score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the main goal of interpretable machine learning systems?\\n</td>\n",
       "      <td>The main goal of interpretable machine learning systems is to provide explanations for their outputs, which can be used to qualitatively assess other criteria such as safety or non-discrimination.</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What F1 score was achieved on the web portion of TriviaQA using the proposed solution for adapting neural paragraph-level question answering models?\\n\\n</td>\n",
       "      <td>The proposed solution achieved a score of 71.3 F1 on the web portion of TriviaQA, significantly improving upon the previous best system's score of 56.7 F1.</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the time and memory complexity reduction achieved by sparse factorizations of the attention matrix in Transformers?\\n</td>\n",
       "      <td>The sparse factorizations of the attention matrix reduce the time and memory complexity from quadratic growth with the sequence length to $O(n \\sqrt{n})$.</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the human performance accuracy on the CommonsenseQA dataset?\\n</td>\n",
       "      <td>The human performance accuracy on the CommonsenseQA dataset is 89%.</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is a universal adversarial perturbation (UAP) in machine learning?\\n</td>\n",
       "      <td>A universal adversarial perturbation (UAP) is a single perturbation that can fool a target deep neural network (DNN) for most images, meaning it is a single alteration that can cause the DNN to misclassify a wide range of images.</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>At what model size does the capability for moral self-correction emerge in language models trained with reinforcement learning from human feedback?\\n\\n</td>\n",
       "      <td>The capability for moral self-correction in language models trained with reinforcement learning from human feedback emerges at 22B model parameters. This suggests that at this level of scale, language models develop the necessary capabilities to follow instructions and learn complex normative concepts of harm, enabling them to avoid producing harmful outputs when instructed to do so.</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>What percentage of sentences in lower-resource corpora are of acceptable quality?\\n</td>\n",
       "      <td>A significant fraction of lower-resource corpora contains less than 50% sentences of acceptable quality.</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>How do data-driven models compare to rule-based or commercial systems in responding to verbal abuse?\\n</td>\n",
       "      <td>Data-driven models lag behind rule-based or commercial systems in terms of their perceived appropriateness when responding to verbal abuse.</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>What is BART in natural language processing?\\n</td>\n",
       "      <td>BART is a denoising autoencoder for pretraining sequence-to-sequence models, trained by corrupting text with an arbitrary noising function and learning a model to reconstruct the original text. It uses a standard Transformer-based neural machine translation architecture and is effective for both text generation and comprehension tasks.</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>What is the goal of creating the first large publicly available high-quality dataset for named entity recognition in ten African languages?\\n\\n</td>\n",
       "      <td>The goal is to address the under-representation of the African continent in NLP research by bringing together a variety of stakeholders and providing resources to inspire future research on African NLP.</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                     question  \\\n",
       "0                                                                                          What is the main goal of interpretable machine learning systems?\\n   \n",
       "1    What F1 score was achieved on the web portion of TriviaQA using the proposed solution for adapting neural paragraph-level question answering models?\\n\\n   \n",
       "2                               What is the time and memory complexity reduction achieved by sparse factorizations of the attention matrix in Transformers?\\n   \n",
       "3                                                                                      What is the human performance accuracy on the CommonsenseQA dataset?\\n   \n",
       "4                                                                                   What is a universal adversarial perturbation (UAP) in machine learning?\\n   \n",
       "..                                                                                                                                                        ...   \n",
       "195   At what model size does the capability for moral self-correction emerge in language models trained with reinforcement learning from human feedback?\\n\\n   \n",
       "196                                                                       What percentage of sentences in lower-resource corpora are of acceptable quality?\\n   \n",
       "197                                                    How do data-driven models compare to rule-based or commercial systems in responding to verbal abuse?\\n   \n",
       "198                                                                                                            What is BART in natural language processing?\\n   \n",
       "199           What is the goal of creating the first large publicly available high-quality dataset for named entity recognition in ten African languages?\\n\\n   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                 answer  \\\n",
       "0                                                                                                                                                                                                  The main goal of interpretable machine learning systems is to provide explanations for their outputs, which can be used to qualitatively assess other criteria such as safety or non-discrimination.   \n",
       "1                                                                                                                                                                                                                                           The proposed solution achieved a score of 71.3 F1 on the web portion of TriviaQA, significantly improving upon the previous best system's score of 56.7 F1.   \n",
       "2                                                                                                                                                                                                                                            The sparse factorizations of the attention matrix reduce the time and memory complexity from quadratic growth with the sequence length to $O(n \\sqrt{n})$.   \n",
       "3                                                                                                                                                                                                                                                                                                                                   The human performance accuracy on the CommonsenseQA dataset is 89%.   \n",
       "4                                                                                                                                                                 A universal adversarial perturbation (UAP) is a single perturbation that can fool a target deep neural network (DNN) for most images, meaning it is a single alteration that can cause the DNN to misclassify a wide range of images.   \n",
       "..                                                                                                                                                                                                                                                                                                                                                                                                  ...   \n",
       "195  The capability for moral self-correction in language models trained with reinforcement learning from human feedback emerges at 22B model parameters. This suggests that at this level of scale, language models develop the necessary capabilities to follow instructions and learn complex normative concepts of harm, enabling them to avoid producing harmful outputs when instructed to do so.   \n",
       "196                                                                                                                                                                                                                                                                                            A significant fraction of lower-resource corpora contains less than 50% sentences of acceptable quality.   \n",
       "197                                                                                                                                                                                                                                                         Data-driven models lag behind rule-based or commercial systems in terms of their perceived appropriateness when responding to verbal abuse.   \n",
       "198                                                   BART is a denoising autoencoder for pretraining sequence-to-sequence models, trained by corrupting text with an arbitrary noising function and learning a model to reconstruct the original text. It uses a standard Transformer-based neural machine translation architecture and is effective for both text generation and comprehension tasks.   \n",
       "199                                                                                                                                                                                          The goal is to address the under-representation of the African continent in NLP research by bringing together a variety of stakeholders and providing resources to inspire future research on African NLP.   \n",
       "\n",
       "     groundedness/v1/score  relevance/v1/score  standalone/v1/score  \n",
       "0                        5                   4                    4  \n",
       "1                        5                   5                    4  \n",
       "2                        5                   5                    4  \n",
       "3                        5                   5                    5  \n",
       "4                        5                   5                    4  \n",
       "..                     ...                 ...                  ...  \n",
       "195                      5                   5                    4  \n",
       "196                      5                   4                    4  \n",
       "197                      5                   5                    4  \n",
       "198                      5                   5                    5  \n",
       "199                      5                   5                    4  \n",
       "\n",
       "[200 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================\n",
      "Final evaluation dataset:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>groundedness/v1/score</th>\n",
       "      <th>relevance/v1/score</th>\n",
       "      <th>standalone/v1/score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the human performance accuracy on the CommonsenseQA dataset?\\n</td>\n",
       "      <td>The human performance accuracy on the CommonsenseQA dataset is 89%.</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>What is the name of the dataset created to assess the effectiveness of controllable text generation algorithms at preventing toxic language generation?\\n\\n</td>\n",
       "      <td>The dataset is called RealToxicityPrompts, which consists of 100K naturally occurring, sentence-level prompts derived from a large corpus of English web text, paired with toxicity scores from a widely-used toxicity classifier.</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>How many languages are there in the world?\\n</td>\n",
       "      <td>There are over 7000 languages in the world, but only a small number of them are represented in language technologies and applications.</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>How many teams submitted system description papers in OffensEval 2020?\\n</td>\n",
       "      <td>In OffensEval 2020, a total of 70 teams submitted system description papers.</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>What is lifelong learning in humans and animals?\\n</td>\n",
       "      <td>Lifelong learning is the ability to continually acquire, fine-tune, and transfer knowledge and skills throughout one's lifespan, mediated by a rich set of neurocognitive mechanisms that contribute to the development and specialization of sensorimotor skills, as well as long-term memory consolidation and retrieval.</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>What is the approximate number of question-answer-evidence triples in the TriviaQA dataset?\\n</td>\n",
       "      <td>The TriviaQA dataset contains over 650,000 question-answer-evidence triples.</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>What is the top-1 accuracy achieved by NASNet on ImageNet?\\n</td>\n",
       "      <td>NASNet achieves a state-of-the-art accuracy of 82.7% top-1 on ImageNet, which is 1.2% better than the best human-invented architectures.</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>How many tasks does the Beyond the Imitation Game benchmark (BIG-bench) currently consist of?\\n\\n</td>\n",
       "      <td>The Beyond the Imitation Game benchmark (BIG-bench) currently consists of 204 tasks, which were contributed by 450 authors across 132 institutions. These tasks cover a diverse range of topics, including linguistics, childhood development, math, common-sense reasoning, biology, physics, social bias, software development, and more.</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>What year was the Alexa Prize launched to tackle the problem of achieving natural, sustained, coherent and engaging open-domain dialogs?\\n\\n</td>\n",
       "      <td>The Alexa Prize was launched in 2016 to tackle the problem of achieving natural, sustained, coherent and engaging open-domain dialogs.</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>What is ExMix in natural language processing?\\n</td>\n",
       "      <td>ExMix, short for Extreme Mixture, is a massive collection of 107 supervised natural language processing (NLP) tasks across diverse domains and task-families. It was created to study the effect of scaling up the number of tasks during pre-training in NLP.</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>What is the website for accessing SuperGLUE?\\n</td>\n",
       "      <td>The website for accessing SuperGLUE is super.gluebenchmark.com.</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>What is the performance gap between humans and machines on the NewsQA dataset?\\n</td>\n",
       "      <td>The performance gap between humans and machines on the NewsQA dataset is 0.198 in F1 score, indicating that there is still room for improvement through future research.</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>What is the name of the new decoding method that improves the consistency of a transformer-based abstractive summarizer by constraining beam search to avoid hallucinations?\\n\\n</td>\n",
       "      <td>The new decoding method is called PINOCCHIO. It detects likely model hallucinations based on various measures of attribution to the source text and backtracks to find more consistent output, opting to produce no summary at all when no consistent generation can be found.</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>How many parameters does the LIMA language model have?\\n</td>\n",
       "      <td>The LIMA language model has 65 billion parameters.</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>What is the top-1 accuracy achieved by EfficientNet-B7 on ImageNet?\\n</td>\n",
       "      <td>EfficientNet-B7 achieves a state-of-the-art 84.3% top-1 accuracy on ImageNet.</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>What accuracy was achieved on the Stanford Natural Language Inference Dataset in the presented study?\\n</td>\n",
       "      <td>The study achieved an accuracy of 88.6% on the Stanford Natural Language Inference Dataset, setting a new state-of-the-art result.</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>How many languages does the MLQA benchmark contain QA instances in?\\n</td>\n",
       "      <td>The MLQA benchmark contains QA instances in 7 languages, which are English, Arabic, German, Spanish, Hindi, Vietnamese, and Simplified Chinese.</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>What is the name of the proposed non-autoregressive decoder based on the discrete diffusion model for text-to-sound generation?\\n\\n</td>\n",
       "      <td>The proposed non-autoregressive decoder based on the discrete diffusion model is named Diffsound. It predicts all mel-spectrogram tokens in one step and refines the predicted tokens in subsequent steps to achieve the best results.</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>When is the stock of high-quality language data expected to be exhausted?\\n</td>\n",
       "      <td>The stock of high-quality language data is expected to be exhausted soon, likely before 2026.</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>What percentage improvement did the proposed method achieve over the state-of-the-art in the Math23k dataset?\\n</td>\n",
       "      <td>The proposed method achieved a 7% improvement over the state-of-the-art in the Math23k dataset, increasing the performance from 78.4% to 85.4%.</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>What ranking did AlphaCode achieve in simulated evaluations on recent programming competitions on the Codeforces platform?\\n</td>\n",
       "      <td>AlphaCode achieved an average ranking of top 54.3% in competitions with more than 5,000 participants on the Codeforces platform. This ranking showcases the system's ability to create novel solutions to complex programming problems that require deeper reasoning.</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>What is OPT-IML Bench and what is its purpose in natural language processing research?\\n\\n</td>\n",
       "      <td>OPT-IML Bench is a large benchmark for Instruction Meta-Learning (IML) consisting of 2000 NLP tasks consolidated into task categories from 8 existing benchmarks. Its purpose is to provide a comprehensive evaluation framework to measure the performance of large pre-trained language models fine-tuned on a collection of tasks described via instructions, and to characterize the effect of instruction-tuning decisions on downstream task performance when scaling both model and benchmark sizes.</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>What percentage of toxic examples in ToxiGen are labeled as hate speech by human annotators?\\n</td>\n",
       "      <td>94.5% of toxic examples in ToxiGen are labeled as hate speech by human annotators, indicating that the dataset effectively captures hate speech and can be used to improve toxicity classifiers.</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>What accuracy rate did GPT-4 attain on the SAT Math test?\\n</td>\n",
       "      <td>GPT-4 attained a 95% accuracy rate on the SAT Math test, demonstrating its extraordinary performance in tackling human-level tasks. This achievement showcases the capabilities of contemporary foundation models in handling mathematical problems, as assessed by the AGIEval benchmark.</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>What is the size of the Pile English text corpus?\\n</td>\n",
       "      <td>The Pile English text corpus is 825 GiB in size, constructed from 22 diverse high-quality subsets, and is targeted at training large-scale language models.</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>What is the range of parameters for the Llama 2 large language models?\\n</td>\n",
       "      <td>The Llama 2 large language models range in scale from 7 billion to 70 billion parameters.</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>What is a major issue that standard artificial neural networks face in machine learning?\\n\\n</td>\n",
       "      <td>Standard artificial neural networks suffer from the well-known issue of catastrophic forgetting, which makes continual or lifelong learning difficult for machine learning. This issue occurs when a neural network forgets previously learned information after being trained on new data, resulting in a significant decrease in performance on the original task.</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>How many tasks does the new test cover to measure a text model's multitask accuracy?\\n</td>\n",
       "      <td>The new test covers 57 tasks to measure a text model's multitask accuracy, including elementary mathematics, US history, computer science, law, and more.</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>What is the most important hyper-parameter to tune for training deep neural networks?\\n</td>\n",
       "      <td>The learning rate is the most important hyper-parameter to tune for training deep neural networks.</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>What is layer normalization in deep neural networks?\\n</td>\n",
       "      <td>Layer normalization is a technique used to normalize the activities of neurons in deep neural networks. It works by computing the mean and variance of the summed inputs to all neurons in a layer on a single training case, and then using these values to normalize the summed input to each neuron. This technique is similar to batch normalization, but unlike batch normalization, layer normalization performs the same computation at both training and test times, and is straightforward to apply to recurrent neural networks. Additionally, layer normalization gives each neuron its own adaptive bias and gain, which are applied after normalization but before non-linearity. This technique is effective in stabilizing the hidden state dynamics in recurrent networks and can substantially reduce the training time compared to other techniques.</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>What is the number of parameters in the Megatron-Turing NLG 530B language model?\\n</td>\n",
       "      <td>The Megatron-Turing NLG 530B (MT-NLG) language model has 530 billion parameters.</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>When was the CCC visioning workshop on algorithmic fairness held?\\n</td>\n",
       "      <td>The CCC visioning workshop on algorithmic fairness was held in March 2018.</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>What is the purpose of the Holistic Evaluation of Language Models (HELM)?\\n</td>\n",
       "      <td>The purpose of the Holistic Evaluation of Language Models (HELM) is to improve the transparency of language models by providing a comprehensive evaluation framework that assesses their capabilities, limitations, and risks across a wide range of scenarios and metrics.</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>What is ULMFiT in natural language processing?\\n</td>\n",
       "      <td>Universal Language Model Fine-tuning (ULMFiT) is an effective transfer learning method in natural language processing (NLP) that can be applied to any task. It allows for fine-tuning a language model on a specific task, achieving state-of-the-art results on several text classification tasks.</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>How many challenging competition mathematics problems are included in the MATH dataset?\\n</td>\n",
       "      <td>The MATH dataset includes 12,500 challenging competition mathematics problems, each with a full step-by-step solution to help teach models to generate answer derivations and explanations.</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>What percentage of questions did the best language model answer truthfully in a benchmark test of truthfulness?\\n\\n</td>\n",
       "      <td>The best language model was truthful on 58% of questions in a benchmark test of truthfulness, which is significantly lower than human performance at 94%. This suggests that even the best language models struggle to provide accurate and truthful answers, and may even generate false answers that mimic popular misconceptions.</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>How much faster is TeraPipe compared to state-of-the-art model-parallel methods for training the largest GPT-3 model?\\n</td>\n",
       "      <td>TeraPipe can speed up the training of the largest GPT-3 model with 175 billion parameters by 5.0x compared to state-of-the-art model-parallel methods on an AWS cluster with 48 p3.16xlarge instances.</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>What dataset was used to train the DialoGPT model?\\n</td>\n",
       "      <td>The DialoGPT model was trained on 147M conversation-like exchanges extracted from Reddit comment chains over a period spanning from 2005 through 2017.</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>Are African American tweets more likely to be labeled as abusive compared to other tweets?\\n</td>\n",
       "      <td>Yes, according to a study on a publicly available annotated Twitter dataset, African American tweets were found to be up to 3.7 times more likely to be labeled as abusive compared to other tweets. This pattern was statistically significant and remained even when party identification was added as a control variable.</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>What are the six specific risk areas associated with large-scale Language Models?\\n\\n</td>\n",
       "      <td>The six specific risk areas associated with large-scale Language Models are: \\n1. Discrimination, Exclusion and Toxicity, which concerns the perpetuation of stereotypes, unfair discrimination, exclusionary norms, toxic language, and lower performance by social group for LMs.\\n2. Information Hazards, which focuses on risks from private data leaks or LMs correctly inferring sensitive information.\\n3. Misinformation Harms, which addresses risks arising from poor, false or misleading information including in sensitive domains, and knock-on risks such as the erosion of trust in shared information.\\n4. Malicious Uses, which considers risks from actors who try to use LMs to cause harm.\\n5. Human-Computer Interaction Harms, which focuses on risks specific to LLMs used to underpin conversational agents that interact with human users, including unsafe use, manipulation or deception.\\n6. Automation, Access, and Environmental Harms, which discusses the risk of environmental harm, job automation, and other challenges that may have a disparate effect on different social groups or communities.</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>How many distinct tasks are included in the NATURAL INSTRUCTIONS dataset?\\n</td>\n",
       "      <td>The NATURAL INSTRUCTIONS dataset includes 61 distinct tasks, along with their human-authored instructions and 193k task instances (input-output pairs).</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>What is the Natural Language Decathlon (decaNLP) challenge in NLP?\\n</td>\n",
       "      <td>The Natural Language Decathlon (decaNLP) is a challenge that spans ten natural language processing (NLP) tasks, including question answering, machine translation, summarization, natural language inference, sentiment analysis, semantic role labeling, zero-shot relation extraction, goal-oriented dialogue, semantic parsing, and commonsense pronoun resolution. All tasks in decaNLP are cast as question answering over a context.</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>What is BART in natural language processing?\\n</td>\n",
       "      <td>BART is a denoising autoencoder for pretraining sequence-to-sequence models, trained by corrupting text with an arbitrary noising function and learning a model to reconstruct the original text. It uses a standard Transformer-based neural machine translation architecture and is effective for both text generation and comprehension tasks.</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                             question  \\\n",
       "3                                                                                                              What is the human performance accuracy on the CommonsenseQA dataset?\\n   \n",
       "8                         What is the name of the dataset created to assess the effectiveness of controllable text generation algorithms at preventing toxic language generation?\\n\\n   \n",
       "11                                                                                                                                       How many languages are there in the world?\\n   \n",
       "13                                                                                                           How many teams submitted system description papers in OffensEval 2020?\\n   \n",
       "16                                                                                                                                 What is lifelong learning in humans and animals?\\n   \n",
       "21                                                                                      What is the approximate number of question-answer-evidence triples in the TriviaQA dataset?\\n   \n",
       "25                                                                                                                       What is the top-1 accuracy achieved by NASNet on ImageNet?\\n   \n",
       "26                                                                                  How many tasks does the Beyond the Imitation Game benchmark (BIG-bench) currently consist of?\\n\\n   \n",
       "28                                       What year was the Alexa Prize launched to tackle the problem of achieving natural, sustained, coherent and engaging open-domain dialogs?\\n\\n   \n",
       "30                                                                                                                                    What is ExMix in natural language processing?\\n   \n",
       "32                                                                                                                                     What is the website for accessing SuperGLUE?\\n   \n",
       "35                                                                                                   What is the performance gap between humans and machines on the NewsQA dataset?\\n   \n",
       "40   What is the name of the new decoding method that improves the consistency of a transformer-based abstractive summarizer by constraining beam search to avoid hallucinations?\\n\\n   \n",
       "42                                                                                                                           How many parameters does the LIMA language model have?\\n   \n",
       "46                                                                                                              What is the top-1 accuracy achieved by EfficientNet-B7 on ImageNet?\\n   \n",
       "52                                                                            What accuracy was achieved on the Stanford Natural Language Inference Dataset in the presented study?\\n   \n",
       "62                                                                                                              How many languages does the MLQA benchmark contain QA instances in?\\n   \n",
       "79                                                What is the name of the proposed non-autoregressive decoder based on the discrete diffusion model for text-to-sound generation?\\n\\n   \n",
       "83                                                                                                        When is the stock of high-quality language data expected to be exhausted?\\n   \n",
       "90                                                                    What percentage improvement did the proposed method achieve over the state-of-the-art in the Math23k dataset?\\n   \n",
       "91                                                       What ranking did AlphaCode achieve in simulated evaluations on recent programming competitions on the Codeforces platform?\\n   \n",
       "95                                                                                         What is OPT-IML Bench and what is its purpose in natural language processing research?\\n\\n   \n",
       "100                                                                                    What percentage of toxic examples in ToxiGen are labeled as hate speech by human annotators?\\n   \n",
       "103                                                                                                                       What accuracy rate did GPT-4 attain on the SAT Math test?\\n   \n",
       "107                                                                                                                               What is the size of the Pile English text corpus?\\n   \n",
       "111                                                                                                          What is the range of parameters for the Llama 2 large language models?\\n   \n",
       "118                                                                                      What is a major issue that standard artificial neural networks face in machine learning?\\n\\n   \n",
       "122                                                                                            How many tasks does the new test cover to measure a text model's multitask accuracy?\\n   \n",
       "126                                                                                           What is the most important hyper-parameter to tune for training deep neural networks?\\n   \n",
       "132                                                                                                                            What is layer normalization in deep neural networks?\\n   \n",
       "135                                                                                                What is the number of parameters in the Megatron-Turing NLG 530B language model?\\n   \n",
       "136                                                                                                               When was the CCC visioning workshop on algorithmic fairness held?\\n   \n",
       "147                                                                                                       What is the purpose of the Holistic Evaluation of Language Models (HELM)?\\n   \n",
       "152                                                                                                                                  What is ULMFiT in natural language processing?\\n   \n",
       "158                                                                                         How many challenging competition mathematics problems are included in the MATH dataset?\\n   \n",
       "162                                                               What percentage of questions did the best language model answer truthfully in a benchmark test of truthfulness?\\n\\n   \n",
       "167                                                           How much faster is TeraPipe compared to state-of-the-art model-parallel methods for training the largest GPT-3 model?\\n   \n",
       "171                                                                                                                              What dataset was used to train the DialoGPT model?\\n   \n",
       "172                                                                                      Are African American tweets more likely to be labeled as abusive compared to other tweets?\\n   \n",
       "188                                                                                             What are the six specific risk areas associated with large-scale Language Models?\\n\\n   \n",
       "192                                                                                                       How many distinct tasks are included in the NATURAL INSTRUCTIONS dataset?\\n   \n",
       "193                                                                                                              What is the Natural Language Decathlon (decaNLP) challenge in NLP?\\n   \n",
       "198                                                                                                                                    What is BART in natural language processing?\\n   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      answer  \\\n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        The human performance accuracy on the CommonsenseQA dataset is 89%.   \n",
       "8                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         The dataset is called RealToxicityPrompts, which consists of 100K naturally occurring, sentence-level prompts derived from a large corpus of English web text, paired with toxicity scores from a widely-used toxicity classifier.   \n",
       "11                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    There are over 7000 languages in the world, but only a small number of them are represented in language technologies and applications.   \n",
       "13                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              In OffensEval 2020, a total of 70 teams submitted system description papers.   \n",
       "16                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Lifelong learning is the ability to continually acquire, fine-tune, and transfer knowledge and skills throughout one's lifespan, mediated by a rich set of neurocognitive mechanisms that contribute to the development and specialization of sensorimotor skills, as well as long-term memory consolidation and retrieval.   \n",
       "21                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              The TriviaQA dataset contains over 650,000 question-answer-evidence triples.   \n",
       "25                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  NASNet achieves a state-of-the-art accuracy of 82.7% top-1 on ImageNet, which is 1.2% better than the best human-invented architectures.   \n",
       "26                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               The Beyond the Imitation Game benchmark (BIG-bench) currently consists of 204 tasks, which were contributed by 450 authors across 132 institutions. These tasks cover a diverse range of topics, including linguistics, childhood development, math, common-sense reasoning, biology, physics, social bias, software development, and more.   \n",
       "28                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    The Alexa Prize was launched in 2016 to tackle the problem of achieving natural, sustained, coherent and engaging open-domain dialogs.   \n",
       "30                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            ExMix, short for Extreme Mixture, is a massive collection of 107 supervised natural language processing (NLP) tasks across diverse domains and task-families. It was created to study the effect of scaling up the number of tasks during pre-training in NLP.   \n",
       "32                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           The website for accessing SuperGLUE is super.gluebenchmark.com.   \n",
       "35                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  The performance gap between humans and machines on the NewsQA dataset is 0.198 in F1 score, indicating that there is still room for improvement through future research.   \n",
       "40                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            The new decoding method is called PINOCCHIO. It detects likely model hallucinations based on various measures of attribution to the source text and backtracks to find more consistent output, opting to produce no summary at all when no consistent generation can be found.   \n",
       "42                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        The LIMA language model has 65 billion parameters.   \n",
       "46                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             EfficientNet-B7 achieves a state-of-the-art 84.3% top-1 accuracy on ImageNet.   \n",
       "52                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        The study achieved an accuracy of 88.6% on the Stanford Natural Language Inference Dataset, setting a new state-of-the-art result.   \n",
       "62                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           The MLQA benchmark contains QA instances in 7 languages, which are English, Arabic, German, Spanish, Hindi, Vietnamese, and Simplified Chinese.   \n",
       "79                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    The proposed non-autoregressive decoder based on the discrete diffusion model is named Diffsound. It predicts all mel-spectrogram tokens in one step and refines the predicted tokens in subsequent steps to achieve the best results.   \n",
       "83                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             The stock of high-quality language data is expected to be exhausted soon, likely before 2026.   \n",
       "90                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           The proposed method achieved a 7% improvement over the state-of-the-art in the Math23k dataset, increasing the performance from 78.4% to 85.4%.   \n",
       "91                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     AlphaCode achieved an average ranking of top 54.3% in competitions with more than 5,000 participants on the Codeforces platform. This ranking showcases the system's ability to create novel solutions to complex programming problems that require deeper reasoning.   \n",
       "95                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               OPT-IML Bench is a large benchmark for Instruction Meta-Learning (IML) consisting of 2000 NLP tasks consolidated into task categories from 8 existing benchmarks. Its purpose is to provide a comprehensive evaluation framework to measure the performance of large pre-trained language models fine-tuned on a collection of tasks described via instructions, and to characterize the effect of instruction-tuning decisions on downstream task performance when scaling both model and benchmark sizes.   \n",
       "100                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         94.5% of toxic examples in ToxiGen are labeled as hate speech by human annotators, indicating that the dataset effectively captures hate speech and can be used to improve toxicity classifiers.   \n",
       "103                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               GPT-4 attained a 95% accuracy rate on the SAT Math test, demonstrating its extraordinary performance in tackling human-level tasks. This achievement showcases the capabilities of contemporary foundation models in handling mathematical problems, as assessed by the AGIEval benchmark.   \n",
       "107                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              The Pile English text corpus is 825 GiB in size, constructed from 22 diverse high-quality subsets, and is targeted at training large-scale language models.   \n",
       "111                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                The Llama 2 large language models range in scale from 7 billion to 70 billion parameters.   \n",
       "118                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Standard artificial neural networks suffer from the well-known issue of catastrophic forgetting, which makes continual or lifelong learning difficult for machine learning. This issue occurs when a neural network forgets previously learned information after being trained on new data, resulting in a significant decrease in performance on the original task.   \n",
       "122                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                The new test covers 57 tasks to measure a text model's multitask accuracy, including elementary mathematics, US history, computer science, law, and more.   \n",
       "126                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       The learning rate is the most important hyper-parameter to tune for training deep neural networks.   \n",
       "132                                                                                                                                                                                                                                                                   Layer normalization is a technique used to normalize the activities of neurons in deep neural networks. It works by computing the mean and variance of the summed inputs to all neurons in a layer on a single training case, and then using these values to normalize the summed input to each neuron. This technique is similar to batch normalization, but unlike batch normalization, layer normalization performs the same computation at both training and test times, and is straightforward to apply to recurrent neural networks. Additionally, layer normalization gives each neuron its own adaptive bias and gain, which are applied after normalization but before non-linearity. This technique is effective in stabilizing the hidden state dynamics in recurrent networks and can substantially reduce the training time compared to other techniques.   \n",
       "135                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         The Megatron-Turing NLG 530B (MT-NLG) language model has 530 billion parameters.   \n",
       "136                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               The CCC visioning workshop on algorithmic fairness was held in March 2018.   \n",
       "147                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              The purpose of the Holistic Evaluation of Language Models (HELM) is to improve the transparency of language models by providing a comprehensive evaluation framework that assesses their capabilities, limitations, and risks across a wide range of scenarios and metrics.   \n",
       "152                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Universal Language Model Fine-tuning (ULMFiT) is an effective transfer learning method in natural language processing (NLP) that can be applied to any task. It allows for fine-tuning a language model on a specific task, achieving state-of-the-art results on several text classification tasks.   \n",
       "158                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              The MATH dataset includes 12,500 challenging competition mathematics problems, each with a full step-by-step solution to help teach models to generate answer derivations and explanations.   \n",
       "162                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     The best language model was truthful on 58% of questions in a benchmark test of truthfulness, which is significantly lower than human performance at 94%. This suggests that even the best language models struggle to provide accurate and truthful answers, and may even generate false answers that mimic popular misconceptions.   \n",
       "167                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   TeraPipe can speed up the training of the largest GPT-3 model with 175 billion parameters by 5.0x compared to state-of-the-art model-parallel methods on an AWS cluster with 48 p3.16xlarge instances.   \n",
       "171                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   The DialoGPT model was trained on 147M conversation-like exchanges extracted from Reddit comment chains over a period spanning from 2005 through 2017.   \n",
       "172                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Yes, according to a study on a publicly available annotated Twitter dataset, African American tweets were found to be up to 3.7 times more likely to be labeled as abusive compared to other tweets. This pattern was statistically significant and remained even when party identification was added as a control variable.   \n",
       "188  The six specific risk areas associated with large-scale Language Models are: \\n1. Discrimination, Exclusion and Toxicity, which concerns the perpetuation of stereotypes, unfair discrimination, exclusionary norms, toxic language, and lower performance by social group for LMs.\\n2. Information Hazards, which focuses on risks from private data leaks or LMs correctly inferring sensitive information.\\n3. Misinformation Harms, which addresses risks arising from poor, false or misleading information including in sensitive domains, and knock-on risks such as the erosion of trust in shared information.\\n4. Malicious Uses, which considers risks from actors who try to use LMs to cause harm.\\n5. Human-Computer Interaction Harms, which focuses on risks specific to LLMs used to underpin conversational agents that interact with human users, including unsafe use, manipulation or deception.\\n6. Automation, Access, and Environmental Harms, which discusses the risk of environmental harm, job automation, and other challenges that may have a disparate effect on different social groups or communities.   \n",
       "192                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  The NATURAL INSTRUCTIONS dataset includes 61 distinct tasks, along with their human-authored instructions and 193k task instances (input-output pairs).   \n",
       "193                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               The Natural Language Decathlon (decaNLP) is a challenge that spans ten natural language processing (NLP) tasks, including question answering, machine translation, summarization, natural language inference, sentiment analysis, semantic role labeling, zero-shot relation extraction, goal-oriented dialogue, semantic parsing, and commonsense pronoun resolution. All tasks in decaNLP are cast as question answering over a context.   \n",
       "198                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        BART is a denoising autoencoder for pretraining sequence-to-sequence models, trained by corrupting text with an arbitrary noising function and learning a model to reconstruct the original text. It uses a standard Transformer-based neural machine translation architecture and is effective for both text generation and comprehension tasks.   \n",
       "\n",
       "     groundedness/v1/score  relevance/v1/score  standalone/v1/score  \n",
       "3                        5                   5                    5  \n",
       "8                        5                   5                    5  \n",
       "11                       5                   4                    5  \n",
       "13                       5                   5                    5  \n",
       "16                       5                   5                    5  \n",
       "21                       5                   5                    5  \n",
       "25                       5                   5                    5  \n",
       "26                       5                   5                    5  \n",
       "28                       5                   5                    5  \n",
       "30                       5                   5                    5  \n",
       "32                       5                   5                    5  \n",
       "35                       5                   5                    5  \n",
       "40                       5                   5                    5  \n",
       "42                       5                   5                    5  \n",
       "46                       5                   5                    5  \n",
       "52                       5                   5                    5  \n",
       "62                       5                   5                    5  \n",
       "79                       5                   5                    5  \n",
       "83                       5                   5                    5  \n",
       "90                       5                   5                    5  \n",
       "91                       5                   5                    5  \n",
       "95                       5                   5                    5  \n",
       "100                      5                   5                    5  \n",
       "103                      5                   5                    5  \n",
       "107                      5                   5                    5  \n",
       "111                      5                   5                    5  \n",
       "118                      5                   5                    5  \n",
       "122                      5                   5                    5  \n",
       "126                      5                   5                    5  \n",
       "132                      5                   5                    5  \n",
       "135                      5                   5                    5  \n",
       "136                      5                   5                    5  \n",
       "147                      5                   5                    5  \n",
       "152                      5                   5                    5  \n",
       "158                      5                   5                    5  \n",
       "162                      5                   5                    5  \n",
       "167                      5                   5                    5  \n",
       "171                      5                   5                    5  \n",
       "172                      5                   5                    5  \n",
       "188                      5                   5                    5  \n",
       "192                      5                   5                    5  \n",
       "193                      5                   5                    5  \n",
       "198                      5                   5                    5  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import datasets\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "print(\"Evaluation dataset before filtering:\")\n",
    "display(\n",
    "    eval_results_table[\n",
    "        [\n",
    "            \"question\",\n",
    "            \"answer\",\n",
    "            \"groundedness/v1/score\",\n",
    "            \"relevance/v1/score\",\n",
    "            \"standalone/v1/score\",\n",
    "        ]\n",
    "    ]\n",
    ")\n",
    "generated_questions = eval_results_table.loc[\n",
    "    (eval_results_table[\"groundedness/v1/score\"] >= 4)\n",
    "    & (eval_results_table[\"relevance/v1/score\"] >= 4)\n",
    "    & (eval_results_table[\"standalone/v1/score\"] >= 5)\n",
    "]\n",
    "print(\"============================================\")\n",
    "print(\"Final evaluation dataset:\")\n",
    "display(\n",
    "    generated_questions[\n",
    "        [\n",
    "            \"question\",\n",
    "            \"answer\",\n",
    "            \"groundedness/v1/score\",\n",
    "            \"relevance/v1/score\",\n",
    "            \"standalone/v1/score\",\n",
    "        ]\n",
    "    ]\n",
    ")\n",
    "\n",
    "eval_dataset = datasets.Dataset.from_pandas(generated_questions, split=\"train\", preserve_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(eval_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that we have only 1/4 of the generated QA pairs which scored 5/5 on every metric. Proceed to save the filtered dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 43/43 [00:00<00:00, 9176.97 examples/s] \n"
     ]
    }
   ],
   "source": [
    "eval_dataset.save_to_disk(\"NLP_eval_dataset_demo\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlflow-llmops-iFome7o9-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
